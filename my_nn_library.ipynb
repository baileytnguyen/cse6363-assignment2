{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Neural Network Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"Base class for all layers in the neural network.\"\"\"\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Computes the forward pass.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"Computes the backward pass.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class LinearLayer(Layer):\n",
    "    \"\"\"Fully connected linear layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.weights = np.random.randn(output_dim, input_dim) * np.sqrt(2 / input_dim)\n",
    "        self.bias = np.zeros((output_dim, 1))\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Performs forward pass: output = input * weights^T + bias\"\"\"\n",
    "        self.input_data = input_data  # Store for use in backward pass\n",
    "        return np.dot(self.weights, input_data.T).T + self.bias.T\n",
    "\n",
    "    def backward(self, grad_output, learning_rate=0.01):\n",
    "        \"\"\"Computes gradients and updates parameters.\"\"\"\n",
    "        grad_input = np.dot(grad_output, self.weights)  # dL/dX\n",
    "        grad_weights = np.dot(grad_output.T, self.input_data)  # dL/dW\n",
    "        grad_bias = np.sum(grad_output, axis=0, keepdims=True).T  # dL/db\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weights -= learning_rate * grad_weights\n",
    "        self.bias -= learning_rate * grad_bias\n",
    "        \n",
    "        return grad_input\n",
    "    \n",
    "class Sigmoid(Layer):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    def forward(self, input_data):\n",
    "        self.output = 1 / (1 + np.exp(-input_data))\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * self.output * (1 - self.output)\n",
    "    \n",
    "class Tanh(Layer):\n",
    "    \"\"\"Tanh activation function.\"\"\"\n",
    "    def forward(self, input_data):\n",
    "        self.output = np.tanh(input_data)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * (1 - self.output ** 2)\n",
    "    \n",
    "class ReLU(Layer):\n",
    "    \"\"\"ReLU activation function.\"\"\"\n",
    "    def forward(self, input_data):\n",
    "        self.input_data = input_data\n",
    "        return np.maximum(0, input_data)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * (self.input_data > 0)\n",
    "\n",
    "class BinaryCrossEntropyLoss(Layer):\n",
    "    \"\"\"Binary Cross-Entropy Loss Function.\"\"\"\n",
    "    def forward(self, predictions, targets):\n",
    "        self.predictions = np.clip(predictions, 1e-12, 1 - 1e-12)  # Avoid log(0)\n",
    "        self.targets = targets\n",
    "        return -np.mean(targets * np.log(self.predictions) + (1 - targets) * np.log(1 - self.predictions))\n",
    "    \n",
    "    def backward(self):\n",
    "        return (self.predictions - self.targets) / (self.targets.shape[0])\n",
    "\n",
    "class MeanSquaredErrorLoss(Layer):\n",
    "    \"\"\"Mean Squared Error Loss Function.\"\"\"\n",
    "    def forward(self, predictions, targets):\n",
    "        self.predictions = predictions\n",
    "        self.targets = targets\n",
    "        # Ensure both predictions and targets have the same shape\n",
    "        assert predictions.shape == targets.shape, \"Shapes of predictions and targets do not match\"\n",
    "        return np.mean((predictions - targets) ** 2)  # Element-wise squared error\n",
    "\n",
    "    def backward(self):\n",
    "        # Compute the gradient of the loss with respect to predictions\n",
    "        return 2 * (self.predictions - self.targets) / self.predictions.size\n",
    "\n",
    "class Sequential(Layer):\n",
    "    \"\"\"Sequential model to stack multiple layers.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def add(self, layer):\n",
    "        \"\"\"Adds a new layer to the model.\"\"\"\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Performs a forward pass through all layers.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            input_data = layer.forward(input_data)\n",
    "        return input_data\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate=0.01):\n",
    "        \"\"\"Performs a backward pass through all layers.\"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_output = layer.backward(grad_output, learning_rate) if isinstance(layer, LinearLayer) else layer.backward(grad_output)\n",
    "    \n",
    "    def save_weights(self, filename):\n",
    "        \"\"\"Saves model weights to a file.\"\"\"\n",
    "        weights = [layer.weights for layer in self.layers if isinstance(layer, LinearLayer)]\n",
    "        biases = [layer.bias for layer in self.layers if isinstance(layer, LinearLayer)]\n",
    "        np.savez(filename, *weights, *biases)\n",
    "    \n",
    "    def load_weights(self, filename):\n",
    "        \"\"\"Loads model weights from a file while ensuring correct indexing.\"\"\"\n",
    "        data = np.load(filename)\n",
    "        keys = sorted(data.files)  # Ensure we get them in the right order\n",
    "        num_linear_layers = sum(1 for layer in self.layers if isinstance(layer, LinearLayer))\n",
    "\n",
    "        weight_keys = keys[:num_linear_layers]\n",
    "        bias_keys = keys[num_linear_layers:]\n",
    "\n",
    "        linear_layers = [layer for layer in self.layers if isinstance(layer, LinearLayer)]\n",
    "\n",
    "        for layer, w_key, b_key in zip(linear_layers, weight_keys, bias_keys):\n",
    "            layer.weights = data[w_key]\n",
    "            layer.bias = data[b_key]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Library Against XOR Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 7.2639\n",
      "Epoch 1000, Loss: 0.3836\n",
      "Epoch 2000, Loss: 0.0517\n",
      "Epoch 3000, Loss: 0.0263\n",
      "Epoch 4000, Loss: 0.0193\n",
      "Epoch 5000, Loss: 0.0158\n",
      "Epoch 6000, Loss: 0.0136\n",
      "Epoch 7000, Loss: 0.0121\n",
      "Epoch 8000, Loss: 0.0110\n",
      "Epoch 9000, Loss: 0.0101\n",
      "Final Predictions:\n",
      "[[4.67735818e-04]\n",
      " [9.82337554e-01]\n",
      " [9.81713307e-01]\n",
      " [9.25608756e-04]]\n"
     ]
    }
   ],
   "source": [
    "# XOR Problem Setup\n",
    "np.random.seed(0)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(LinearLayer(2, 2))  # Hidden layer with 2 neurons\n",
    "model.add(Tanh())\n",
    "model.add(LinearLayer(2, 1))  # Output layer\n",
    "model.add(Tanh())\n",
    "\n",
    "# Training\n",
    "epochs = 10000\n",
    "learning_rate = 0.1\n",
    "loss_function = BinaryCrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    predictions = model.forward(X)\n",
    "    loss = loss_function.forward(predictions, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    grad_output = loss_function.backward()\n",
    "    model.backward(grad_output, learning_rate)\n",
    "    \n",
    "    # Print loss every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Save trained model weights\n",
    "model.save_weights(\"XOR_solved\")\n",
    "\n",
    "# Test the model\n",
    "predictions = model.forward(X)\n",
    "print(\"Final Predictions:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR Problem Results\n",
    "\n",
    "Looks like using the tanh activations got the right predictions, couldn't get it to work well with sigmoid activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York Data Set\n",
    "https://drive.google.com/file/d/1xEtmFpP-WwZ-GC0B2njySPoLmdOCJOYM/view\n",
    "\n",
    "## Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before Preprocessing\n",
      "id                              id2425795\n",
      "vendor_id                               1\n",
      "pickup_datetime       2016-01-08 23:55:11\n",
      "dropoff_datetime      2016-01-09 00:04:32\n",
      "passenger_count                         1\n",
      "pickup_longitude               -73.955551\n",
      "pickup_latitude                 40.773346\n",
      "dropoff_longitude               -73.97364\n",
      "dropoff_latitude                  40.7635\n",
      "store_and_fwd_flag                      N\n",
      "Name: 879655, dtype: object\n",
      "\n",
      "After Preprocessing\n",
      "id                               id2425795\n",
      "vendor_id                        -1.072367\n",
      "pickup_datetime        2016-01-08 23:55:11\n",
      "dropoff_datetime       2016-01-09 00:04:32\n",
      "passenger_count                  -0.505442\n",
      "pickup_longitude                  0.244154\n",
      "pickup_latitude                   0.681191\n",
      "dropoff_longitude                 -0.00304\n",
      "dropoff_latitude                  0.326676\n",
      "store_and_fwd_flag                       N\n",
      "pickup_month                     -1.496875\n",
      "pickup_day                        -0.86245\n",
      "pickup_hour                       1.467487\n",
      "dropoff_month                    -1.496925\n",
      "dropoff_day                      -0.747615\n",
      "dropoff_hour                     -2.097212\n",
      "pickup_day_of_week                0.486031\n",
      "dropoff_day_of_week               0.994397\n",
      "Name: 879655, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "np.random.seed(0)\n",
    "dataset = np.load(\"nyc_taxi_data.npy\", allow_pickle=True).item()\n",
    "X_train, y_train = pd.DataFrame(dataset[\"X_train\"]), pd.Series(dataset[\"y_train\"])\n",
    "X_test, y_test = pd.DataFrame(dataset[\"X_test\"]), pd.Series(dataset[\"y_test\"])\n",
    "\n",
    "# print one row of the dataset\n",
    "print(\"\\nBefore Preprocessing\")\n",
    "print(X_train.iloc[0])\n",
    "\n",
    "# create function to split pickup and dropoff datetime into month, day, hour\n",
    "def split_datetime(df):\n",
    "    df[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"])\n",
    "    df[\"dropoff_datetime\"] = pd.to_datetime(df[\"dropoff_datetime\"])\n",
    "    df[\"pickup_month\"] = df[\"pickup_datetime\"].dt.month\n",
    "    df[\"pickup_day\"] = df[\"pickup_datetime\"].dt.day\n",
    "    df[\"pickup_hour\"] = df[\"pickup_datetime\"].dt.hour\n",
    "    df[\"dropoff_month\"] = df[\"dropoff_datetime\"].dt.month\n",
    "    df[\"dropoff_day\"] = df[\"dropoff_datetime\"].dt.day\n",
    "    df[\"dropoff_hour\"] = df[\"dropoff_datetime\"].dt.hour\n",
    "    return df\n",
    "\n",
    "# create function to add day of week, 0 = Monday, 6 = Sunday\n",
    "def add_day_of_week(df):\n",
    "    df[\"pickup_day_of_week\"] = df[\"pickup_datetime\"].dt.dayofweek\n",
    "    df[\"dropoff_day_of_week\"] = df[\"dropoff_datetime\"].dt.dayofweek\n",
    "    return df\n",
    "\n",
    "# create function to normalize all numerical features to same scale\n",
    "# should only be applied to numerical features\n",
    "#\n",
    "#   Feature Type\t                Recommended Scaling\n",
    "#   Longitude/Latitude\t            Min-Max (0 to 1)\n",
    "#   Time-based (hour, day, month)\tMin-Max (0 to 1) or Sin/Cos Encoding\n",
    "def normalize_numerical_features_0to1(df):\n",
    "    numerical_features = df.select_dtypes(include=[np.number]).columns\n",
    "    for feature in numerical_features:\n",
    "        if \"longitude\" in feature or \"latitude\" in feature:\n",
    "            df[feature] = (df[feature] - df[feature].min()) / (df[feature].max() - df[feature].min())\n",
    "        elif \"hour\" in feature or \"day\" in feature or \"month\" in feature:\n",
    "            df[feature] = (df[feature] - df[feature].min()) / (df[feature].max() - df[feature].min())\n",
    "        elif \"day_of_week\" in feature:\n",
    "            df[feature] = df[feature] / 6  # Scale day of week between 0 and 1\n",
    "    return df\n",
    "\n",
    "# create function to normalize all numerical features to same scale\n",
    "# should only be applied to numerical features\n",
    "# zscore normalization\n",
    "def normalize_numerical_features_zscore(df):\n",
    "    numerical_features = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numerical_features] = (df[numerical_features] - df[numerical_features].mean()) / df[numerical_features].std()\n",
    "    return df\n",
    "\n",
    "# function to perform split date time and add day of week given dataframe and normalize features\n",
    "def preprocess_data(df):\n",
    "    df = split_datetime(df)\n",
    "    df = add_day_of_week(df)\n",
    "    df = normalize_numerical_features_zscore(df) # Change to which ever normalization if needed\n",
    "    return df\n",
    "\n",
    "# preprocess the training and test data\n",
    "X_train = preprocess_data(X_train)\n",
    "X_test = preprocess_data(X_test)\n",
    "\n",
    "# Log-transform the target variable to reduce skewness\n",
    "y_train = np.log1p(y_train.to_numpy().reshape(-1, 1))\n",
    "y_test = np.log1p(y_test.to_numpy().reshape(-1, 1))\n",
    "\n",
    "# print one row of the dataset with after text title\n",
    "print(\"\\nAfter Preprocessing\")\n",
    "print(X_train.iloc[0])\n",
    "\n",
    "# create function to only keep the features we want given df and features\n",
    "def select_features(df, features):\n",
    "    return df[features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Pick Up and Drop Off Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 47.2248, Val Loss: 46.8674\n",
      "Saving model at epoch 0 with loss 47.2248\n",
      "Epoch 1, Loss: 43.3706, Val Loss: 43.2139\n",
      "Saving model at epoch 1 with loss 43.3706\n",
      "Epoch 2, Loss: 40.7624, Val Loss: 40.6766\n",
      "Saving model at epoch 2 with loss 40.7624\n",
      "Epoch 3, Loss: 38.5732, Val Loss: 38.5248\n",
      "Saving model at epoch 3 with loss 38.5732\n",
      "Epoch 4, Loss: 36.6342, Val Loss: 36.6076\n",
      "Saving model at epoch 4 with loss 36.6342\n",
      "Epoch 5, Loss: 34.8649, Val Loss: 34.8514\n",
      "Saving model at epoch 5 with loss 34.8649\n",
      "Epoch 6, Loss: 33.2119, Val Loss: 33.2059\n",
      "Saving model at epoch 6 with loss 33.2119\n",
      "Epoch 7, Loss: 31.6476, Val Loss: 31.6453\n",
      "Saving model at epoch 7 with loss 31.6476\n",
      "Epoch 8, Loss: 30.1527, Val Loss: 30.1508\n",
      "Saving model at epoch 8 with loss 30.1527\n",
      "Epoch 9, Loss: 28.7143, Val Loss: 28.7100\n",
      "Saving model at epoch 9 with loss 28.7143\n",
      "Epoch 10, Loss: 27.3225, Val Loss: 27.3129\n",
      "Saving model at epoch 10 with loss 27.3225\n",
      "Epoch 11, Loss: 25.9691, Val Loss: 25.9512\n",
      "Saving model at epoch 11 with loss 25.9691\n",
      "Epoch 12, Loss: 24.6468, Val Loss: 24.6172\n",
      "Saving model at epoch 12 with loss 24.6468\n",
      "Epoch 13, Loss: 23.3474, Val Loss: 23.3022\n",
      "Saving model at epoch 13 with loss 23.3474\n",
      "Epoch 14, Loss: 22.0588, Val Loss: 21.9932\n",
      "Saving model at epoch 14 with loss 22.0588\n",
      "Epoch 15, Loss: 20.7471, Val Loss: 20.6547\n",
      "Saving model at epoch 15 with loss 20.7471\n",
      "Epoch 16, Loss: 19.3438, Val Loss: 19.2160\n",
      "Saving model at epoch 16 with loss 19.3438\n",
      "Epoch 17, Loss: 17.8581, Val Loss: 17.6845\n",
      "Saving model at epoch 17 with loss 17.8581\n",
      "Epoch 18, Loss: 16.2100, Val Loss: 15.9759\n",
      "Saving model at epoch 18 with loss 16.2100\n",
      "Epoch 19, Loss: 14.1606, Val Loss: 13.8359\n",
      "Saving model at epoch 19 with loss 14.1606\n",
      "Epoch 20, Loss: 11.7648, Val Loss: 11.2873\n",
      "Saving model at epoch 20 with loss 11.7648\n",
      "Epoch 21, Loss: 9.8107, Val Loss: 9.1649\n",
      "Saving model at epoch 21 with loss 9.8107\n",
      "Epoch 22, Loss: 8.4939, Val Loss: 7.7345\n",
      "Saving model at epoch 22 with loss 8.4939\n",
      "Epoch 23, Loss: 7.5819, Val Loss: 6.7670\n",
      "Saving model at epoch 23 with loss 7.5819\n",
      "Epoch 24, Loss: 6.8659, Val Loss: 6.0378\n",
      "Saving model at epoch 24 with loss 6.8659\n",
      "Epoch 25, Loss: 6.2448, Val Loss: 5.4262\n",
      "Saving model at epoch 25 with loss 6.2448\n",
      "Epoch 26, Loss: 5.6889, Val Loss: 4.8894\n",
      "Saving model at epoch 26 with loss 5.6889\n",
      "Epoch 27, Loss: 5.1899, Val Loss: 4.4129\n",
      "Saving model at epoch 27 with loss 5.1899\n",
      "Epoch 28, Loss: 4.7432, Val Loss: 3.9900\n",
      "Saving model at epoch 28 with loss 4.7432\n",
      "Epoch 29, Loss: 4.3445, Val Loss: 3.6158\n",
      "Saving model at epoch 29 with loss 4.3445\n",
      "Epoch 30, Loss: 3.9891, Val Loss: 3.2858\n",
      "Saving model at epoch 30 with loss 3.9891\n",
      "Epoch 31, Loss: 3.6728, Val Loss: 2.9957\n",
      "Saving model at epoch 31 with loss 3.6728\n",
      "Epoch 32, Loss: 3.3915, Val Loss: 2.7413\n",
      "Saving model at epoch 32 with loss 3.3915\n",
      "Epoch 33, Loss: 3.1414, Val Loss: 2.5184\n",
      "Saving model at epoch 33 with loss 3.1414\n",
      "Epoch 34, Loss: 2.9190, Val Loss: 2.3233\n",
      "Saving model at epoch 34 with loss 2.9190\n",
      "Epoch 35, Loss: 2.7212, Val Loss: 2.1527\n",
      "Saving model at epoch 35 with loss 2.7212\n",
      "Epoch 36, Loss: 2.5450, Val Loss: 2.0033\n",
      "Saving model at epoch 36 with loss 2.5450\n",
      "Epoch 37, Loss: 2.3879, Val Loss: 1.8724\n",
      "Saving model at epoch 37 with loss 2.3879\n",
      "Epoch 38, Loss: 2.2477, Val Loss: 1.7574\n",
      "Saving model at epoch 38 with loss 2.2477\n",
      "Epoch 39, Loss: 2.1222, Val Loss: 1.6562\n",
      "Saving model at epoch 39 with loss 2.1222\n",
      "Epoch 40, Loss: 2.0097, Val Loss: 1.5669\n",
      "Saving model at epoch 40 with loss 2.0097\n",
      "Epoch 41, Loss: 1.9086, Val Loss: 1.4878\n",
      "Saving model at epoch 41 with loss 1.9086\n",
      "Epoch 42, Loss: 1.8176, Val Loss: 1.4175\n",
      "Saving model at epoch 42 with loss 1.8176\n",
      "Epoch 43, Loss: 1.7351, Val Loss: 1.3548\n",
      "Saving model at epoch 43 with loss 1.7351\n",
      "Epoch 44, Loss: 1.6600, Val Loss: 1.2984\n",
      "Saving model at epoch 44 with loss 1.6600\n",
      "Epoch 45, Loss: 1.5918, Val Loss: 1.2477\n",
      "Saving model at epoch 45 with loss 1.5918\n",
      "Epoch 46, Loss: 1.5299, Val Loss: 1.2019\n",
      "Saving model at epoch 46 with loss 1.5299\n",
      "Epoch 47, Loss: 1.4735, Val Loss: 1.1605\n",
      "Saving model at epoch 47 with loss 1.4735\n",
      "Epoch 48, Loss: 1.4214, Val Loss: 1.1228\n",
      "Saving model at epoch 48 with loss 1.4214\n",
      "Epoch 49, Loss: 1.3740, Val Loss: 1.0885\n",
      "Saving model at epoch 49 with loss 1.3740\n",
      "Epoch 50, Loss: 1.3306, Val Loss: 1.0572\n",
      "Saving model at epoch 50 with loss 1.3306\n",
      "Epoch 51, Loss: 1.2905, Val Loss: 1.0286\n",
      "Saving model at epoch 51 with loss 1.2905\n",
      "Epoch 52, Loss: 1.2538, Val Loss: 1.0025\n",
      "Saving model at epoch 52 with loss 1.2538\n",
      "Epoch 53, Loss: 1.2199, Val Loss: 0.9785\n",
      "Saving model at epoch 53 with loss 1.2199\n",
      "Epoch 54, Loss: 1.1887, Val Loss: 0.9564\n",
      "Saving model at epoch 54 with loss 1.1887\n",
      "Epoch 55, Loss: 1.1598, Val Loss: 0.9362\n",
      "Saving model at epoch 55 with loss 1.1598\n",
      "Epoch 56, Loss: 1.1330, Val Loss: 0.9175\n",
      "Saving model at epoch 56 with loss 1.1330\n",
      "Epoch 57, Loss: 1.1081, Val Loss: 0.9002\n",
      "Saving model at epoch 57 with loss 1.1081\n",
      "Epoch 58, Loss: 1.0851, Val Loss: 0.8843\n",
      "Saving model at epoch 58 with loss 1.0851\n",
      "Epoch 59, Loss: 1.0636, Val Loss: 0.8696\n",
      "Saving model at epoch 59 with loss 1.0636\n",
      "Epoch 60, Loss: 1.0436, Val Loss: 0.8559\n",
      "Saving model at epoch 60 with loss 1.0436\n",
      "Epoch 61, Loss: 1.0249, Val Loss: 0.8432\n",
      "Saving model at epoch 61 with loss 1.0249\n",
      "Epoch 62, Loss: 1.0074, Val Loss: 0.8314\n",
      "Saving model at epoch 62 with loss 1.0074\n",
      "Epoch 63, Loss: 0.9911, Val Loss: 0.8204\n",
      "Saving model at epoch 63 with loss 0.9911\n",
      "Epoch 64, Loss: 0.9758, Val Loss: 0.8102\n",
      "Saving model at epoch 64 with loss 0.9758\n",
      "Epoch 65, Loss: 0.9614, Val Loss: 0.8006\n",
      "Saving model at epoch 65 with loss 0.9614\n",
      "Epoch 66, Loss: 0.9480, Val Loss: 0.7916\n",
      "Saving model at epoch 66 with loss 0.9480\n",
      "Epoch 67, Loss: 0.9353, Val Loss: 0.7833\n",
      "Saving model at epoch 67 with loss 0.9353\n",
      "Epoch 68, Loss: 0.9234, Val Loss: 0.7754\n",
      "Saving model at epoch 68 with loss 0.9234\n",
      "Epoch 69, Loss: 0.9122, Val Loss: 0.7681\n",
      "Saving model at epoch 69 with loss 0.9122\n",
      "Epoch 70, Loss: 0.9016, Val Loss: 0.7612\n",
      "Saving model at epoch 70 with loss 0.9016\n",
      "Epoch 71, Loss: 0.8916, Val Loss: 0.7547\n",
      "Saving model at epoch 71 with loss 0.8916\n",
      "Epoch 72, Loss: 0.8821, Val Loss: 0.7485\n",
      "Saving model at epoch 72 with loss 0.8821\n",
      "Epoch 73, Loss: 0.8732, Val Loss: 0.7428\n",
      "Saving model at epoch 73 with loss 0.8732\n",
      "Epoch 74, Loss: 0.8646, Val Loss: 0.7373\n",
      "Saving model at epoch 74 with loss 0.8646\n",
      "Epoch 75, Loss: 0.8565, Val Loss: 0.7322\n",
      "Saving model at epoch 75 with loss 0.8565\n",
      "Epoch 76, Loss: 0.8488, Val Loss: 0.7273\n",
      "Saving model at epoch 76 with loss 0.8488\n",
      "Epoch 77, Loss: 0.8415, Val Loss: 0.7228\n",
      "Saving model at epoch 77 with loss 0.8415\n",
      "Epoch 78, Loss: 0.8346, Val Loss: 0.7184\n",
      "Saving model at epoch 78 with loss 0.8346\n",
      "Epoch 79, Loss: 0.8280, Val Loss: 0.7143\n",
      "Saving model at epoch 79 with loss 0.8280\n",
      "Epoch 80, Loss: 0.8217, Val Loss: 0.7103\n",
      "Saving model at epoch 80 with loss 0.8217\n",
      "Epoch 81, Loss: 0.8157, Val Loss: 0.7066\n",
      "Saving model at epoch 81 with loss 0.8157\n",
      "Epoch 82, Loss: 0.8099, Val Loss: 0.7031\n",
      "Saving model at epoch 82 with loss 0.8099\n",
      "Epoch 83, Loss: 0.8044, Val Loss: 0.6997\n",
      "Saving model at epoch 83 with loss 0.8044\n",
      "Epoch 84, Loss: 0.7992, Val Loss: 0.6965\n",
      "Saving model at epoch 84 with loss 0.7992\n",
      "Epoch 85, Loss: 0.7941, Val Loss: 0.6935\n",
      "Saving model at epoch 85 with loss 0.7941\n",
      "Epoch 86, Loss: 0.7893, Val Loss: 0.6905\n",
      "Saving model at epoch 86 with loss 0.7893\n",
      "Epoch 87, Loss: 0.7847, Val Loss: 0.6878\n",
      "Saving model at epoch 87 with loss 0.7847\n",
      "Epoch 88, Loss: 0.7803, Val Loss: 0.6851\n",
      "Saving model at epoch 88 with loss 0.7803\n",
      "Epoch 89, Loss: 0.7761, Val Loss: 0.6826\n",
      "Saving model at epoch 89 with loss 0.7761\n",
      "Epoch 90, Loss: 0.7720, Val Loss: 0.6801\n",
      "Saving model at epoch 90 with loss 0.7720\n",
      "Epoch 91, Loss: 0.7681, Val Loss: 0.6778\n",
      "Saving model at epoch 91 with loss 0.7681\n",
      "Epoch 92, Loss: 0.7643, Val Loss: 0.6756\n",
      "Saving model at epoch 92 with loss 0.7643\n",
      "Epoch 93, Loss: 0.7607, Val Loss: 0.6734\n",
      "Saving model at epoch 93 with loss 0.7607\n",
      "Epoch 94, Loss: 0.7571, Val Loss: 0.6713\n",
      "Saving model at epoch 94 with loss 0.7571\n",
      "Epoch 95, Loss: 0.7538, Val Loss: 0.6693\n",
      "Saving model at epoch 95 with loss 0.7538\n",
      "Epoch 96, Loss: 0.7505, Val Loss: 0.6674\n",
      "Saving model at epoch 96 with loss 0.7505\n",
      "Epoch 97, Loss: 0.7474, Val Loss: 0.6655\n",
      "Saving model at epoch 97 with loss 0.7474\n",
      "Epoch 98, Loss: 0.7444, Val Loss: 0.6637\n",
      "Saving model at epoch 98 with loss 0.7444\n",
      "Epoch 99, Loss: 0.7415, Val Loss: 0.6620\n",
      "Saving model at epoch 99 with loss 0.7415\n",
      "Epoch 100, Loss: 0.7387, Val Loss: 0.6604\n",
      "Saving model at epoch 100 with loss 0.7387\n",
      "Epoch 101, Loss: 0.7360, Val Loss: 0.6588\n",
      "Saving model at epoch 101 with loss 0.7360\n",
      "Epoch 102, Loss: 0.7334, Val Loss: 0.6573\n",
      "Saving model at epoch 102 with loss 0.7334\n",
      "Epoch 103, Loss: 0.7309, Val Loss: 0.6558\n",
      "Saving model at epoch 103 with loss 0.7309\n",
      "Epoch 104, Loss: 0.7285, Val Loss: 0.6544\n",
      "Saving model at epoch 104 with loss 0.7285\n",
      "Epoch 105, Loss: 0.7262, Val Loss: 0.6530\n",
      "Saving model at epoch 105 with loss 0.7262\n",
      "Epoch 106, Loss: 0.7239, Val Loss: 0.6517\n",
      "Saving model at epoch 106 with loss 0.7239\n",
      "Epoch 107, Loss: 0.7218, Val Loss: 0.6504\n",
      "Saving model at epoch 107 with loss 0.7218\n",
      "Epoch 108, Loss: 0.7197, Val Loss: 0.6492\n",
      "Saving model at epoch 108 with loss 0.7197\n",
      "Epoch 109, Loss: 0.7176, Val Loss: 0.6480\n",
      "Saving model at epoch 109 with loss 0.7176\n",
      "Epoch 110, Loss: 0.7157, Val Loss: 0.6468\n",
      "Saving model at epoch 110 with loss 0.7157\n",
      "Epoch 111, Loss: 0.7138, Val Loss: 0.6457\n",
      "Saving model at epoch 111 with loss 0.7138\n",
      "Epoch 112, Loss: 0.7119, Val Loss: 0.6447\n",
      "Saving model at epoch 112 with loss 0.7119\n",
      "Epoch 113, Loss: 0.7102, Val Loss: 0.6436\n",
      "Saving model at epoch 113 with loss 0.7102\n",
      "Epoch 114, Loss: 0.7084, Val Loss: 0.6426\n",
      "Saving model at epoch 114 with loss 0.7084\n",
      "Epoch 115, Loss: 0.7068, Val Loss: 0.6416\n",
      "Saving model at epoch 115 with loss 0.7068\n",
      "Epoch 116, Loss: 0.7052, Val Loss: 0.6407\n",
      "Saving model at epoch 116 with loss 0.7052\n",
      "Epoch 117, Loss: 0.7036, Val Loss: 0.6398\n",
      "Saving model at epoch 117 with loss 0.7036\n",
      "Epoch 118, Loss: 0.7021, Val Loss: 0.6389\n",
      "Saving model at epoch 118 with loss 0.7021\n",
      "Epoch 119, Loss: 0.7006, Val Loss: 0.6380\n",
      "Saving model at epoch 119 with loss 0.7006\n",
      "Epoch 120, Loss: 0.6991, Val Loss: 0.6372\n",
      "Saving model at epoch 120 with loss 0.6991\n",
      "Epoch 121, Loss: 0.6977, Val Loss: 0.6363\n",
      "Saving model at epoch 121 with loss 0.6977\n",
      "Epoch 122, Loss: 0.6964, Val Loss: 0.6355\n",
      "Saving model at epoch 122 with loss 0.6964\n",
      "Epoch 123, Loss: 0.6950, Val Loss: 0.6347\n",
      "Saving model at epoch 123 with loss 0.6950\n",
      "Epoch 124, Loss: 0.6938, Val Loss: 0.6340\n",
      "Saving model at epoch 124 with loss 0.6938\n",
      "Epoch 125, Loss: 0.6925, Val Loss: 0.6332\n",
      "Saving model at epoch 125 with loss 0.6925\n",
      "Epoch 126, Loss: 0.6913, Val Loss: 0.6325\n",
      "Saving model at epoch 126 with loss 0.6913\n",
      "Epoch 127, Loss: 0.6901, Val Loss: 0.6318\n",
      "Saving model at epoch 127 with loss 0.6901\n",
      "Epoch 128, Loss: 0.6889, Val Loss: 0.6311\n",
      "Saving model at epoch 128 with loss 0.6889\n",
      "Epoch 129, Loss: 0.6878, Val Loss: 0.6304\n",
      "Saving model at epoch 129 with loss 0.6878\n",
      "Epoch 130, Loss: 0.6867, Val Loss: 0.6298\n",
      "Saving model at epoch 130 with loss 0.6867\n",
      "Epoch 131, Loss: 0.6856, Val Loss: 0.6291\n",
      "Saving model at epoch 131 with loss 0.6856\n",
      "Epoch 132, Loss: 0.6845, Val Loss: 0.6285\n",
      "Saving model at epoch 132 with loss 0.6845\n",
      "Epoch 133, Loss: 0.6834, Val Loss: 0.6278\n",
      "Saving model at epoch 133 with loss 0.6834\n",
      "Epoch 134, Loss: 0.6824, Val Loss: 0.6272\n",
      "Saving model at epoch 134 with loss 0.6824\n",
      "Epoch 135, Loss: 0.6814, Val Loss: 0.6266\n",
      "Saving model at epoch 135 with loss 0.6814\n",
      "Epoch 136, Loss: 0.6804, Val Loss: 0.6260\n",
      "Saving model at epoch 136 with loss 0.6804\n",
      "Epoch 137, Loss: 0.6794, Val Loss: 0.6254\n",
      "Saving model at epoch 137 with loss 0.6794\n",
      "Epoch 138, Loss: 0.6785, Val Loss: 0.6249\n",
      "Saving model at epoch 138 with loss 0.6785\n",
      "Epoch 139, Loss: 0.6775, Val Loss: 0.6243\n",
      "Saving model at epoch 139 with loss 0.6775\n",
      "Epoch 140, Loss: 0.6766, Val Loss: 0.6238\n",
      "Saving model at epoch 140 with loss 0.6766\n",
      "Epoch 141, Loss: 0.6757, Val Loss: 0.6232\n",
      "Saving model at epoch 141 with loss 0.6757\n",
      "Epoch 142, Loss: 0.6748, Val Loss: 0.6227\n",
      "Saving model at epoch 142 with loss 0.6748\n",
      "Epoch 143, Loss: 0.6740, Val Loss: 0.6222\n",
      "Saving model at epoch 143 with loss 0.6740\n",
      "Epoch 144, Loss: 0.6731, Val Loss: 0.6217\n",
      "Saving model at epoch 144 with loss 0.6731\n",
      "Epoch 145, Loss: 0.6723, Val Loss: 0.6212\n",
      "Saving model at epoch 145 with loss 0.6723\n",
      "Epoch 146, Loss: 0.6715, Val Loss: 0.6207\n",
      "Saving model at epoch 146 with loss 0.6715\n",
      "Epoch 147, Loss: 0.6707, Val Loss: 0.6202\n",
      "Saving model at epoch 147 with loss 0.6707\n",
      "Epoch 148, Loss: 0.6700, Val Loss: 0.6197\n",
      "Saving model at epoch 148 with loss 0.6700\n",
      "Epoch 149, Loss: 0.6692, Val Loss: 0.6192\n",
      "Saving model at epoch 149 with loss 0.6692\n",
      "Epoch 150, Loss: 0.6685, Val Loss: 0.6188\n",
      "Saving model at epoch 150 with loss 0.6685\n",
      "Epoch 151, Loss: 0.6677, Val Loss: 0.6183\n",
      "Saving model at epoch 151 with loss 0.6677\n",
      "Epoch 152, Loss: 0.6670, Val Loss: 0.6179\n",
      "Saving model at epoch 152 with loss 0.6670\n",
      "Epoch 153, Loss: 0.6663, Val Loss: 0.6174\n",
      "Saving model at epoch 153 with loss 0.6663\n",
      "Epoch 154, Loss: 0.6656, Val Loss: 0.6170\n",
      "Saving model at epoch 154 with loss 0.6656\n",
      "Epoch 155, Loss: 0.6649, Val Loss: 0.6166\n",
      "Saving model at epoch 155 with loss 0.6649\n",
      "Epoch 156, Loss: 0.6643, Val Loss: 0.6162\n",
      "Saving model at epoch 156 with loss 0.6643\n",
      "Epoch 157, Loss: 0.6636, Val Loss: 0.6158\n",
      "Saving model at epoch 157 with loss 0.6636\n",
      "Epoch 158, Loss: 0.6630, Val Loss: 0.6154\n",
      "Saving model at epoch 158 with loss 0.6630\n",
      "Epoch 159, Loss: 0.6624, Val Loss: 0.6150\n",
      "Saving model at epoch 159 with loss 0.6624\n",
      "Epoch 160, Loss: 0.6617, Val Loss: 0.6146\n",
      "Saving model at epoch 160 with loss 0.6617\n",
      "Epoch 161, Loss: 0.6611, Val Loss: 0.6142\n",
      "Saving model at epoch 161 with loss 0.6611\n",
      "Epoch 162, Loss: 0.6605, Val Loss: 0.6138\n",
      "Saving model at epoch 162 with loss 0.6605\n",
      "Epoch 163, Loss: 0.6599, Val Loss: 0.6134\n",
      "Saving model at epoch 163 with loss 0.6599\n",
      "Epoch 164, Loss: 0.6594, Val Loss: 0.6131\n",
      "Saving model at epoch 164 with loss 0.6594\n",
      "Epoch 165, Loss: 0.6588, Val Loss: 0.6127\n",
      "Saving model at epoch 165 with loss 0.6588\n",
      "Epoch 166, Loss: 0.6582, Val Loss: 0.6123\n",
      "Saving model at epoch 166 with loss 0.6582\n",
      "Epoch 167, Loss: 0.6577, Val Loss: 0.6120\n",
      "Saving model at epoch 167 with loss 0.6577\n",
      "Epoch 168, Loss: 0.6571, Val Loss: 0.6116\n",
      "Saving model at epoch 168 with loss 0.6571\n",
      "Epoch 169, Loss: 0.6566, Val Loss: 0.6113\n",
      "Saving model at epoch 169 with loss 0.6566\n",
      "Epoch 170, Loss: 0.6561, Val Loss: 0.6109\n",
      "Saving model at epoch 170 with loss 0.6561\n",
      "Epoch 171, Loss: 0.6556, Val Loss: 0.6106\n",
      "Saving model at epoch 171 with loss 0.6556\n",
      "Epoch 172, Loss: 0.6550, Val Loss: 0.6103\n",
      "Saving model at epoch 172 with loss 0.6550\n",
      "Epoch 173, Loss: 0.6545, Val Loss: 0.6099\n",
      "Saving model at epoch 173 with loss 0.6545\n",
      "Epoch 174, Loss: 0.6540, Val Loss: 0.6096\n",
      "Saving model at epoch 174 with loss 0.6540\n",
      "Epoch 175, Loss: 0.6536, Val Loss: 0.6093\n",
      "Saving model at epoch 175 with loss 0.6536\n",
      "Epoch 176, Loss: 0.6531, Val Loss: 0.6090\n",
      "Saving model at epoch 176 with loss 0.6531\n",
      "Epoch 177, Loss: 0.6526, Val Loss: 0.6086\n",
      "Saving model at epoch 177 with loss 0.6526\n",
      "Epoch 178, Loss: 0.6521, Val Loss: 0.6083\n",
      "Saving model at epoch 178 with loss 0.6521\n",
      "Epoch 179, Loss: 0.6517, Val Loss: 0.6080\n",
      "Saving model at epoch 179 with loss 0.6517\n",
      "Epoch 180, Loss: 0.6512, Val Loss: 0.6077\n",
      "Saving model at epoch 180 with loss 0.6512\n",
      "Epoch 181, Loss: 0.6507, Val Loss: 0.6074\n",
      "Saving model at epoch 181 with loss 0.6507\n",
      "Epoch 182, Loss: 0.6503, Val Loss: 0.6071\n",
      "Saving model at epoch 182 with loss 0.6503\n",
      "Epoch 183, Loss: 0.6498, Val Loss: 0.6068\n",
      "Saving model at epoch 183 with loss 0.6498\n",
      "Epoch 184, Loss: 0.6494, Val Loss: 0.6065\n",
      "Saving model at epoch 184 with loss 0.6494\n",
      "Epoch 185, Loss: 0.6490, Val Loss: 0.6062\n",
      "Saving model at epoch 185 with loss 0.6490\n",
      "Epoch 186, Loss: 0.6485, Val Loss: 0.6059\n",
      "Saving model at epoch 186 with loss 0.6485\n",
      "Epoch 187, Loss: 0.6481, Val Loss: 0.6056\n",
      "Saving model at epoch 187 with loss 0.6481\n",
      "Epoch 188, Loss: 0.6477, Val Loss: 0.6053\n",
      "Saving model at epoch 188 with loss 0.6477\n",
      "Epoch 189, Loss: 0.6473, Val Loss: 0.6051\n",
      "Saving model at epoch 189 with loss 0.6473\n",
      "Epoch 190, Loss: 0.6469, Val Loss: 0.6048\n",
      "Saving model at epoch 190 with loss 0.6469\n",
      "Epoch 191, Loss: 0.6465, Val Loss: 0.6045\n",
      "Saving model at epoch 191 with loss 0.6465\n",
      "Epoch 192, Loss: 0.6461, Val Loss: 0.6042\n",
      "Saving model at epoch 192 with loss 0.6461\n",
      "Epoch 193, Loss: 0.6457, Val Loss: 0.6040\n",
      "Saving model at epoch 193 with loss 0.6457\n",
      "Epoch 194, Loss: 0.6453, Val Loss: 0.6037\n",
      "Saving model at epoch 194 with loss 0.6453\n",
      "Epoch 195, Loss: 0.6449, Val Loss: 0.6034\n",
      "Saving model at epoch 195 with loss 0.6449\n",
      "Epoch 196, Loss: 0.6446, Val Loss: 0.6032\n",
      "Saving model at epoch 196 with loss 0.6446\n",
      "Epoch 197, Loss: 0.6442, Val Loss: 0.6029\n",
      "Saving model at epoch 197 with loss 0.6442\n",
      "Epoch 198, Loss: 0.6438, Val Loss: 0.6026\n",
      "Saving model at epoch 198 with loss 0.6438\n",
      "Epoch 199, Loss: 0.6434, Val Loss: 0.6024\n",
      "Saving model at epoch 199 with loss 0.6434\n",
      "Epoch 200, Loss: 0.6431, Val Loss: 0.6021\n",
      "Saving model at epoch 200 with loss 0.6431\n",
      "Epoch 201, Loss: 0.6427, Val Loss: 0.6019\n",
      "Saving model at epoch 201 with loss 0.6427\n",
      "Epoch 202, Loss: 0.6424, Val Loss: 0.6016\n",
      "Saving model at epoch 202 with loss 0.6424\n",
      "Epoch 203, Loss: 0.6420, Val Loss: 0.6014\n",
      "Saving model at epoch 203 with loss 0.6420\n",
      "Epoch 204, Loss: 0.6417, Val Loss: 0.6011\n",
      "Saving model at epoch 204 with loss 0.6417\n",
      "Epoch 205, Loss: 0.6413, Val Loss: 0.6009\n",
      "Saving model at epoch 205 with loss 0.6413\n",
      "Epoch 206, Loss: 0.6410, Val Loss: 0.6006\n",
      "Saving model at epoch 206 with loss 0.6410\n",
      "Epoch 207, Loss: 0.6406, Val Loss: 0.6004\n",
      "Saving model at epoch 207 with loss 0.6406\n",
      "Epoch 208, Loss: 0.6403, Val Loss: 0.6002\n",
      "Saving model at epoch 208 with loss 0.6403\n",
      "Epoch 209, Loss: 0.6400, Val Loss: 0.5999\n",
      "Saving model at epoch 209 with loss 0.6400\n",
      "Epoch 210, Loss: 0.6397, Val Loss: 0.5997\n",
      "Saving model at epoch 210 with loss 0.6397\n",
      "Epoch 211, Loss: 0.6393, Val Loss: 0.5995\n",
      "Saving model at epoch 211 with loss 0.6393\n",
      "Epoch 212, Loss: 0.6390, Val Loss: 0.5992\n",
      "Saving model at epoch 212 with loss 0.6390\n",
      "Epoch 213, Loss: 0.6387, Val Loss: 0.5990\n",
      "Saving model at epoch 213 with loss 0.6387\n",
      "Epoch 214, Loss: 0.6384, Val Loss: 0.5988\n",
      "Saving model at epoch 214 with loss 0.6384\n",
      "Epoch 215, Loss: 0.6381, Val Loss: 0.5985\n",
      "Saving model at epoch 215 with loss 0.6381\n",
      "Epoch 216, Loss: 0.6378, Val Loss: 0.5983\n",
      "Saving model at epoch 216 with loss 0.6378\n",
      "Epoch 217, Loss: 0.6375, Val Loss: 0.5981\n",
      "Saving model at epoch 217 with loss 0.6375\n",
      "Epoch 218, Loss: 0.6372, Val Loss: 0.5979\n",
      "Saving model at epoch 218 with loss 0.6372\n",
      "Epoch 219, Loss: 0.6368, Val Loss: 0.5977\n",
      "Saving model at epoch 219 with loss 0.6368\n",
      "Epoch 220, Loss: 0.6365, Val Loss: 0.5974\n",
      "Saving model at epoch 220 with loss 0.6365\n",
      "Epoch 221, Loss: 0.6363, Val Loss: 0.5972\n",
      "Saving model at epoch 221 with loss 0.6363\n",
      "Epoch 222, Loss: 0.6360, Val Loss: 0.5970\n",
      "Saving model at epoch 222 with loss 0.6360\n",
      "Epoch 223, Loss: 0.6357, Val Loss: 0.5968\n",
      "Saving model at epoch 223 with loss 0.6357\n",
      "Epoch 224, Loss: 0.6354, Val Loss: 0.5966\n",
      "Saving model at epoch 224 with loss 0.6354\n",
      "Epoch 225, Loss: 0.6351, Val Loss: 0.5964\n",
      "Saving model at epoch 225 with loss 0.6351\n",
      "Epoch 226, Loss: 0.6348, Val Loss: 0.5961\n",
      "Saving model at epoch 226 with loss 0.6348\n",
      "Epoch 227, Loss: 0.6345, Val Loss: 0.5959\n",
      "Saving model at epoch 227 with loss 0.6345\n",
      "Epoch 228, Loss: 0.6342, Val Loss: 0.5957\n",
      "Saving model at epoch 228 with loss 0.6342\n",
      "Epoch 229, Loss: 0.6340, Val Loss: 0.5955\n",
      "Saving model at epoch 229 with loss 0.6340\n",
      "Epoch 230, Loss: 0.6337, Val Loss: 0.5953\n",
      "Saving model at epoch 230 with loss 0.6337\n",
      "Epoch 231, Loss: 0.6334, Val Loss: 0.5951\n",
      "Saving model at epoch 231 with loss 0.6334\n",
      "Epoch 232, Loss: 0.6332, Val Loss: 0.5949\n",
      "Saving model at epoch 232 with loss 0.6332\n",
      "Epoch 233, Loss: 0.6329, Val Loss: 0.5947\n",
      "Saving model at epoch 233 with loss 0.6329\n",
      "Epoch 234, Loss: 0.6326, Val Loss: 0.5945\n",
      "Saving model at epoch 234 with loss 0.6326\n",
      "Epoch 235, Loss: 0.6324, Val Loss: 0.5943\n",
      "Saving model at epoch 235 with loss 0.6324\n",
      "Epoch 236, Loss: 0.6321, Val Loss: 0.5941\n",
      "Saving model at epoch 236 with loss 0.6321\n",
      "Epoch 237, Loss: 0.6318, Val Loss: 0.5939\n",
      "Saving model at epoch 237 with loss 0.6318\n",
      "Epoch 238, Loss: 0.6316, Val Loss: 0.5937\n",
      "Saving model at epoch 238 with loss 0.6316\n",
      "Epoch 239, Loss: 0.6313, Val Loss: 0.5935\n",
      "Saving model at epoch 239 with loss 0.6313\n",
      "Epoch 240, Loss: 0.6311, Val Loss: 0.5933\n",
      "Saving model at epoch 240 with loss 0.6311\n",
      "Epoch 241, Loss: 0.6308, Val Loss: 0.5932\n",
      "Saving model at epoch 241 with loss 0.6308\n",
      "Epoch 242, Loss: 0.6306, Val Loss: 0.5930\n",
      "Saving model at epoch 242 with loss 0.6306\n",
      "Epoch 243, Loss: 0.6303, Val Loss: 0.5928\n",
      "Saving model at epoch 243 with loss 0.6303\n",
      "Epoch 244, Loss: 0.6301, Val Loss: 0.5926\n",
      "Saving model at epoch 244 with loss 0.6301\n",
      "Epoch 245, Loss: 0.6298, Val Loss: 0.5924\n",
      "Saving model at epoch 245 with loss 0.6298\n",
      "Epoch 246, Loss: 0.6296, Val Loss: 0.5922\n",
      "Saving model at epoch 246 with loss 0.6296\n",
      "Epoch 247, Loss: 0.6293, Val Loss: 0.5920\n",
      "Saving model at epoch 247 with loss 0.6293\n",
      "Epoch 248, Loss: 0.6291, Val Loss: 0.5919\n",
      "Saving model at epoch 248 with loss 0.6291\n",
      "Epoch 249, Loss: 0.6289, Val Loss: 0.5917\n",
      "Saving model at epoch 249 with loss 0.6289\n",
      "Epoch 250, Loss: 0.6286, Val Loss: 0.5915\n",
      "Saving model at epoch 250 with loss 0.6286\n",
      "Epoch 251, Loss: 0.6284, Val Loss: 0.5913\n",
      "Saving model at epoch 251 with loss 0.6284\n",
      "Epoch 252, Loss: 0.6282, Val Loss: 0.5911\n",
      "Saving model at epoch 252 with loss 0.6282\n",
      "Epoch 253, Loss: 0.6279, Val Loss: 0.5910\n",
      "Saving model at epoch 253 with loss 0.6279\n",
      "Epoch 254, Loss: 0.6277, Val Loss: 0.5908\n",
      "Saving model at epoch 254 with loss 0.6277\n",
      "Epoch 255, Loss: 0.6275, Val Loss: 0.5906\n",
      "Saving model at epoch 255 with loss 0.6275\n",
      "Epoch 256, Loss: 0.6272, Val Loss: 0.5904\n",
      "Saving model at epoch 256 with loss 0.6272\n",
      "Epoch 257, Loss: 0.6270, Val Loss: 0.5903\n",
      "Saving model at epoch 257 with loss 0.6270\n",
      "Epoch 258, Loss: 0.6268, Val Loss: 0.5901\n",
      "Saving model at epoch 258 with loss 0.6268\n",
      "Epoch 259, Loss: 0.6266, Val Loss: 0.5899\n",
      "Saving model at epoch 259 with loss 0.6266\n",
      "Epoch 260, Loss: 0.6264, Val Loss: 0.5897\n",
      "Saving model at epoch 260 with loss 0.6264\n",
      "Epoch 261, Loss: 0.6262, Val Loss: 0.5896\n",
      "Saving model at epoch 261 with loss 0.6262\n",
      "Epoch 262, Loss: 0.6260, Val Loss: 0.5894\n",
      "Saving model at epoch 262 with loss 0.6260\n",
      "Epoch 263, Loss: 0.6257, Val Loss: 0.5892\n",
      "Saving model at epoch 263 with loss 0.6257\n",
      "Epoch 264, Loss: 0.6255, Val Loss: 0.5890\n",
      "Saving model at epoch 264 with loss 0.6255\n",
      "Epoch 265, Loss: 0.6253, Val Loss: 0.5889\n",
      "Saving model at epoch 265 with loss 0.6253\n",
      "Epoch 266, Loss: 0.6251, Val Loss: 0.5887\n",
      "Saving model at epoch 266 with loss 0.6251\n",
      "Epoch 267, Loss: 0.6249, Val Loss: 0.5885\n",
      "Saving model at epoch 267 with loss 0.6249\n",
      "Epoch 268, Loss: 0.6247, Val Loss: 0.5884\n",
      "Saving model at epoch 268 with loss 0.6247\n",
      "Epoch 269, Loss: 0.6244, Val Loss: 0.5882\n",
      "Saving model at epoch 269 with loss 0.6244\n",
      "Epoch 270, Loss: 0.6242, Val Loss: 0.5880\n",
      "Saving model at epoch 270 with loss 0.6242\n",
      "Epoch 271, Loss: 0.6240, Val Loss: 0.5879\n",
      "Saving model at epoch 271 with loss 0.6240\n",
      "Epoch 272, Loss: 0.6238, Val Loss: 0.5877\n",
      "Saving model at epoch 272 with loss 0.6238\n",
      "Epoch 273, Loss: 0.6236, Val Loss: 0.5875\n",
      "Saving model at epoch 273 with loss 0.6236\n",
      "Epoch 274, Loss: 0.6234, Val Loss: 0.5874\n",
      "Saving model at epoch 274 with loss 0.6234\n",
      "Epoch 275, Loss: 0.6232, Val Loss: 0.5872\n",
      "Saving model at epoch 275 with loss 0.6232\n",
      "Epoch 276, Loss: 0.6230, Val Loss: 0.5871\n",
      "Saving model at epoch 276 with loss 0.6230\n",
      "Epoch 277, Loss: 0.6228, Val Loss: 0.5869\n",
      "Saving model at epoch 277 with loss 0.6228\n",
      "Epoch 278, Loss: 0.6226, Val Loss: 0.5867\n",
      "Saving model at epoch 278 with loss 0.6226\n",
      "Epoch 279, Loss: 0.6224, Val Loss: 0.5866\n",
      "Saving model at epoch 279 with loss 0.6224\n",
      "Epoch 280, Loss: 0.6222, Val Loss: 0.5864\n",
      "Saving model at epoch 280 with loss 0.6222\n",
      "Epoch 281, Loss: 0.6220, Val Loss: 0.5863\n",
      "Saving model at epoch 281 with loss 0.6220\n",
      "Epoch 282, Loss: 0.6218, Val Loss: 0.5861\n",
      "Saving model at epoch 282 with loss 0.6218\n",
      "Epoch 283, Loss: 0.6216, Val Loss: 0.5860\n",
      "Saving model at epoch 283 with loss 0.6216\n",
      "Epoch 284, Loss: 0.6214, Val Loss: 0.5858\n",
      "Saving model at epoch 284 with loss 0.6214\n",
      "Epoch 285, Loss: 0.6213, Val Loss: 0.5857\n",
      "Saving model at epoch 285 with loss 0.6213\n",
      "Epoch 286, Loss: 0.6211, Val Loss: 0.5855\n",
      "Saving model at epoch 286 with loss 0.6211\n",
      "Epoch 287, Loss: 0.6209, Val Loss: 0.5854\n",
      "Saving model at epoch 287 with loss 0.6209\n",
      "Epoch 288, Loss: 0.6207, Val Loss: 0.5852\n",
      "Saving model at epoch 288 with loss 0.6207\n",
      "Epoch 289, Loss: 0.6205, Val Loss: 0.5851\n",
      "Saving model at epoch 289 with loss 0.6205\n",
      "Epoch 290, Loss: 0.6203, Val Loss: 0.5849\n",
      "Saving model at epoch 290 with loss 0.6203\n",
      "Epoch 291, Loss: 0.6202, Val Loss: 0.5848\n",
      "Saving model at epoch 291 with loss 0.6202\n",
      "Epoch 292, Loss: 0.6200, Val Loss: 0.5847\n",
      "Saving model at epoch 292 with loss 0.6200\n",
      "Epoch 293, Loss: 0.6198, Val Loss: 0.5845\n",
      "Saving model at epoch 293 with loss 0.6198\n",
      "Epoch 294, Loss: 0.6196, Val Loss: 0.5844\n",
      "Saving model at epoch 294 with loss 0.6196\n",
      "Epoch 295, Loss: 0.6194, Val Loss: 0.5842\n",
      "Saving model at epoch 295 with loss 0.6194\n",
      "Epoch 296, Loss: 0.6193, Val Loss: 0.5841\n",
      "Saving model at epoch 296 with loss 0.6193\n",
      "Epoch 297, Loss: 0.6191, Val Loss: 0.5840\n",
      "Saving model at epoch 297 with loss 0.6191\n",
      "Epoch 298, Loss: 0.6189, Val Loss: 0.5838\n",
      "Saving model at epoch 298 with loss 0.6189\n",
      "Epoch 299, Loss: 0.6187, Val Loss: 0.5837\n",
      "Saving model at epoch 299 with loss 0.6187\n",
      "Epoch 300, Loss: 0.6186, Val Loss: 0.5836\n",
      "Saving model at epoch 300 with loss 0.6186\n",
      "Epoch 301, Loss: 0.6184, Val Loss: 0.5834\n",
      "Saving model at epoch 301 with loss 0.6184\n",
      "Epoch 302, Loss: 0.6182, Val Loss: 0.5833\n",
      "Saving model at epoch 302 with loss 0.6182\n",
      "Epoch 303, Loss: 0.6181, Val Loss: 0.5832\n",
      "Saving model at epoch 303 with loss 0.6181\n",
      "Epoch 304, Loss: 0.6179, Val Loss: 0.5830\n",
      "Saving model at epoch 304 with loss 0.6179\n",
      "Epoch 305, Loss: 0.6177, Val Loss: 0.5829\n",
      "Saving model at epoch 305 with loss 0.6177\n",
      "Epoch 306, Loss: 0.6176, Val Loss: 0.5828\n",
      "Saving model at epoch 306 with loss 0.6176\n",
      "Epoch 307, Loss: 0.6174, Val Loss: 0.5826\n",
      "Saving model at epoch 307 with loss 0.6174\n",
      "Epoch 308, Loss: 0.6172, Val Loss: 0.5825\n",
      "Saving model at epoch 308 with loss 0.6172\n",
      "Epoch 309, Loss: 0.6171, Val Loss: 0.5824\n",
      "Saving model at epoch 309 with loss 0.6171\n",
      "Epoch 310, Loss: 0.6169, Val Loss: 0.5823\n",
      "Saving model at epoch 310 with loss 0.6169\n",
      "Epoch 311, Loss: 0.6167, Val Loss: 0.5821\n",
      "Saving model at epoch 311 with loss 0.6167\n",
      "Epoch 312, Loss: 0.6166, Val Loss: 0.5820\n",
      "Saving model at epoch 312 with loss 0.6166\n",
      "Epoch 313, Loss: 0.6164, Val Loss: 0.5819\n",
      "Saving model at epoch 313 with loss 0.6164\n",
      "Epoch 314, Loss: 0.6163, Val Loss: 0.5818\n",
      "Saving model at epoch 314 with loss 0.6163\n",
      "Epoch 315, Loss: 0.6161, Val Loss: 0.5816\n",
      "Saving model at epoch 315 with loss 0.6161\n",
      "Epoch 316, Loss: 0.6160, Val Loss: 0.5815\n",
      "Saving model at epoch 316 with loss 0.6160\n",
      "Epoch 317, Loss: 0.6158, Val Loss: 0.5814\n",
      "Saving model at epoch 317 with loss 0.6158\n",
      "Epoch 318, Loss: 0.6156, Val Loss: 0.5813\n",
      "Saving model at epoch 318 with loss 0.6156\n",
      "Epoch 319, Loss: 0.6155, Val Loss: 0.5812\n",
      "Saving model at epoch 319 with loss 0.6155\n",
      "Epoch 320, Loss: 0.6153, Val Loss: 0.5811\n",
      "Saving model at epoch 320 with loss 0.6153\n",
      "Epoch 321, Loss: 0.6152, Val Loss: 0.5809\n",
      "Saving model at epoch 321 with loss 0.6152\n",
      "Epoch 322, Loss: 0.6150, Val Loss: 0.5808\n",
      "Saving model at epoch 322 with loss 0.6150\n",
      "Epoch 323, Loss: 0.6149, Val Loss: 0.5807\n",
      "Saving model at epoch 323 with loss 0.6149\n",
      "Epoch 324, Loss: 0.6147, Val Loss: 0.5806\n",
      "Saving model at epoch 324 with loss 0.6147\n",
      "Epoch 325, Loss: 0.6146, Val Loss: 0.5805\n",
      "Saving model at epoch 325 with loss 0.6146\n",
      "Epoch 326, Loss: 0.6144, Val Loss: 0.5804\n",
      "Saving model at epoch 326 with loss 0.6144\n",
      "Epoch 327, Loss: 0.6143, Val Loss: 0.5802\n",
      "Saving model at epoch 327 with loss 0.6143\n",
      "Epoch 328, Loss: 0.6141, Val Loss: 0.5801\n",
      "Saving model at epoch 328 with loss 0.6141\n",
      "Epoch 329, Loss: 0.6140, Val Loss: 0.5800\n",
      "Saving model at epoch 329 with loss 0.6140\n",
      "Epoch 330, Loss: 0.6139, Val Loss: 0.5799\n",
      "Saving model at epoch 330 with loss 0.6139\n",
      "Epoch 331, Loss: 0.6137, Val Loss: 0.5798\n",
      "Saving model at epoch 331 with loss 0.6137\n",
      "Epoch 332, Loss: 0.6136, Val Loss: 0.5797\n",
      "Saving model at epoch 332 with loss 0.6136\n",
      "Epoch 333, Loss: 0.6134, Val Loss: 0.5796\n",
      "Saving model at epoch 333 with loss 0.6134\n",
      "Epoch 334, Loss: 0.6133, Val Loss: 0.5795\n",
      "Saving model at epoch 334 with loss 0.6133\n",
      "Epoch 335, Loss: 0.6131, Val Loss: 0.5794\n",
      "Saving model at epoch 335 with loss 0.6131\n",
      "Epoch 336, Loss: 0.6130, Val Loss: 0.5793\n",
      "Saving model at epoch 336 with loss 0.6130\n",
      "Epoch 337, Loss: 0.6129, Val Loss: 0.5792\n",
      "Saving model at epoch 337 with loss 0.6129\n",
      "Epoch 338, Loss: 0.6127, Val Loss: 0.5790\n",
      "Saving model at epoch 338 with loss 0.6127\n",
      "Epoch 339, Loss: 0.6126, Val Loss: 0.5789\n",
      "Saving model at epoch 339 with loss 0.6126\n",
      "Epoch 340, Loss: 0.6125, Val Loss: 0.5788\n",
      "Saving model at epoch 340 with loss 0.6125\n",
      "Epoch 341, Loss: 0.6123, Val Loss: 0.5787\n",
      "Saving model at epoch 341 with loss 0.6123\n",
      "Epoch 342, Loss: 0.6122, Val Loss: 0.5786\n",
      "Saving model at epoch 342 with loss 0.6122\n",
      "Epoch 343, Loss: 0.6121, Val Loss: 0.5785\n",
      "Saving model at epoch 343 with loss 0.6121\n",
      "Epoch 344, Loss: 0.6119, Val Loss: 0.5784\n",
      "Saving model at epoch 344 with loss 0.6119\n",
      "Epoch 345, Loss: 0.6118, Val Loss: 0.5783\n",
      "Saving model at epoch 345 with loss 0.6118\n",
      "Epoch 346, Loss: 0.6117, Val Loss: 0.5782\n",
      "Saving model at epoch 346 with loss 0.6117\n",
      "Epoch 347, Loss: 0.6115, Val Loss: 0.5781\n",
      "Saving model at epoch 347 with loss 0.6115\n",
      "Epoch 348, Loss: 0.6114, Val Loss: 0.5780\n",
      "Saving model at epoch 348 with loss 0.6114\n",
      "Epoch 349, Loss: 0.6113, Val Loss: 0.5779\n",
      "Saving model at epoch 349 with loss 0.6113\n",
      "Epoch 350, Loss: 0.6111, Val Loss: 0.5778\n",
      "Saving model at epoch 350 with loss 0.6111\n",
      "Epoch 351, Loss: 0.6110, Val Loss: 0.5777\n",
      "Saving model at epoch 351 with loss 0.6110\n",
      "Epoch 352, Loss: 0.6109, Val Loss: 0.5776\n",
      "Saving model at epoch 352 with loss 0.6109\n",
      "Epoch 353, Loss: 0.6108, Val Loss: 0.5775\n",
      "Saving model at epoch 353 with loss 0.6108\n",
      "Epoch 354, Loss: 0.6106, Val Loss: 0.5774\n",
      "Saving model at epoch 354 with loss 0.6106\n",
      "Epoch 355, Loss: 0.6105, Val Loss: 0.5773\n",
      "Saving model at epoch 355 with loss 0.6105\n",
      "Epoch 356, Loss: 0.6104, Val Loss: 0.5773\n",
      "Saving model at epoch 356 with loss 0.6104\n",
      "Epoch 357, Loss: 0.6103, Val Loss: 0.5772\n",
      "Saving model at epoch 357 with loss 0.6103\n",
      "Epoch 358, Loss: 0.6101, Val Loss: 0.5771\n",
      "Saving model at epoch 358 with loss 0.6101\n",
      "Epoch 359, Loss: 0.6100, Val Loss: 0.5770\n",
      "Saving model at epoch 359 with loss 0.6100\n",
      "Epoch 360, Loss: 0.6099, Val Loss: 0.5769\n",
      "Saving model at epoch 360 with loss 0.6099\n",
      "Epoch 361, Loss: 0.6098, Val Loss: 0.5768\n",
      "Saving model at epoch 361 with loss 0.6098\n",
      "Epoch 362, Loss: 0.6096, Val Loss: 0.5767\n",
      "Saving model at epoch 362 with loss 0.6096\n",
      "Epoch 363, Loss: 0.6095, Val Loss: 0.5766\n",
      "Saving model at epoch 363 with loss 0.6095\n",
      "Epoch 364, Loss: 0.6094, Val Loss: 0.5765\n",
      "Saving model at epoch 364 with loss 0.6094\n",
      "Epoch 365, Loss: 0.6093, Val Loss: 0.5764\n",
      "Saving model at epoch 365 with loss 0.6093\n",
      "Epoch 366, Loss: 0.6092, Val Loss: 0.5763\n",
      "Saving model at epoch 366 with loss 0.6092\n",
      "Epoch 367, Loss: 0.6091, Val Loss: 0.5763\n",
      "Saving model at epoch 367 with loss 0.6091\n",
      "Epoch 368, Loss: 0.6089, Val Loss: 0.5762\n",
      "Saving model at epoch 368 with loss 0.6089\n",
      "Epoch 369, Loss: 0.6088, Val Loss: 0.5761\n",
      "Saving model at epoch 369 with loss 0.6088\n",
      "Epoch 370, Loss: 0.6087, Val Loss: 0.5760\n",
      "Saving model at epoch 370 with loss 0.6087\n",
      "Epoch 371, Loss: 0.6086, Val Loss: 0.5759\n",
      "Saving model at epoch 371 with loss 0.6086\n",
      "Epoch 372, Loss: 0.6085, Val Loss: 0.5758\n",
      "Saving model at epoch 372 with loss 0.6085\n",
      "Epoch 373, Loss: 0.6084, Val Loss: 0.5757\n",
      "Saving model at epoch 373 with loss 0.6084\n",
      "Epoch 374, Loss: 0.6083, Val Loss: 0.5756\n",
      "Saving model at epoch 374 with loss 0.6083\n",
      "Epoch 375, Loss: 0.6081, Val Loss: 0.5756\n",
      "Saving model at epoch 375 with loss 0.6081\n",
      "Epoch 376, Loss: 0.6080, Val Loss: 0.5755\n",
      "Saving model at epoch 376 with loss 0.6080\n",
      "Epoch 377, Loss: 0.6079, Val Loss: 0.5754\n",
      "Saving model at epoch 377 with loss 0.6079\n",
      "Epoch 378, Loss: 0.6078, Val Loss: 0.5753\n",
      "Saving model at epoch 378 with loss 0.6078\n",
      "Epoch 379, Loss: 0.6077, Val Loss: 0.5752\n",
      "Saving model at epoch 379 with loss 0.6077\n",
      "Epoch 380, Loss: 0.6076, Val Loss: 0.5752\n",
      "Saving model at epoch 380 with loss 0.6076\n",
      "Epoch 381, Loss: 0.6075, Val Loss: 0.5751\n",
      "Saving model at epoch 381 with loss 0.6075\n",
      "Epoch 382, Loss: 0.6074, Val Loss: 0.5750\n",
      "Saving model at epoch 382 with loss 0.6074\n",
      "Epoch 383, Loss: 0.6073, Val Loss: 0.5749\n",
      "Saving model at epoch 383 with loss 0.6073\n",
      "Epoch 384, Loss: 0.6072, Val Loss: 0.5748\n",
      "Saving model at epoch 384 with loss 0.6072\n",
      "Epoch 385, Loss: 0.6070, Val Loss: 0.5748\n",
      "Saving model at epoch 385 with loss 0.6070\n",
      "Epoch 386, Loss: 0.6069, Val Loss: 0.5747\n",
      "Saving model at epoch 386 with loss 0.6069\n",
      "Epoch 387, Loss: 0.6068, Val Loss: 0.5746\n",
      "Saving model at epoch 387 with loss 0.6068\n",
      "Epoch 388, Loss: 0.6067, Val Loss: 0.5745\n",
      "Saving model at epoch 388 with loss 0.6067\n",
      "Epoch 389, Loss: 0.6066, Val Loss: 0.5744\n",
      "Saving model at epoch 389 with loss 0.6066\n",
      "Epoch 390, Loss: 0.6065, Val Loss: 0.5744\n",
      "Saving model at epoch 390 with loss 0.6065\n",
      "Epoch 391, Loss: 0.6064, Val Loss: 0.5743\n",
      "Saving model at epoch 391 with loss 0.6064\n",
      "Epoch 392, Loss: 0.6063, Val Loss: 0.5742\n",
      "Saving model at epoch 392 with loss 0.6063\n",
      "Epoch 393, Loss: 0.6062, Val Loss: 0.5741\n",
      "Saving model at epoch 393 with loss 0.6062\n",
      "Epoch 394, Loss: 0.6061, Val Loss: 0.5741\n",
      "Saving model at epoch 394 with loss 0.6061\n",
      "Epoch 395, Loss: 0.6060, Val Loss: 0.5740\n",
      "Saving model at epoch 395 with loss 0.6060\n",
      "Epoch 396, Loss: 0.6059, Val Loss: 0.5739\n",
      "Saving model at epoch 396 with loss 0.6059\n",
      "Epoch 397, Loss: 0.6058, Val Loss: 0.5738\n",
      "Saving model at epoch 397 with loss 0.6058\n",
      "Epoch 398, Loss: 0.6057, Val Loss: 0.5738\n",
      "Saving model at epoch 398 with loss 0.6057\n",
      "Epoch 399, Loss: 0.6056, Val Loss: 0.5737\n",
      "Saving model at epoch 399 with loss 0.6056\n",
      "Epoch 400, Loss: 0.6055, Val Loss: 0.5736\n",
      "Saving model at epoch 400 with loss 0.6055\n",
      "Epoch 401, Loss: 0.6054, Val Loss: 0.5736\n",
      "Saving model at epoch 401 with loss 0.6054\n",
      "Epoch 402, Loss: 0.6053, Val Loss: 0.5735\n",
      "Saving model at epoch 402 with loss 0.6053\n",
      "Epoch 403, Loss: 0.6052, Val Loss: 0.5734\n",
      "Saving model at epoch 403 with loss 0.6052\n",
      "Epoch 404, Loss: 0.6051, Val Loss: 0.5733\n",
      "Saving model at epoch 404 with loss 0.6051\n",
      "Epoch 405, Loss: 0.6050, Val Loss: 0.5733\n",
      "Saving model at epoch 405 with loss 0.6050\n",
      "Epoch 406, Loss: 0.6049, Val Loss: 0.5732\n",
      "Saving model at epoch 406 with loss 0.6049\n",
      "Epoch 407, Loss: 0.6048, Val Loss: 0.5731\n",
      "Saving model at epoch 407 with loss 0.6048\n",
      "Epoch 408, Loss: 0.6047, Val Loss: 0.5731\n",
      "Saving model at epoch 408 with loss 0.6047\n",
      "Epoch 409, Loss: 0.6047, Val Loss: 0.5730\n",
      "Saving model at epoch 409 with loss 0.6047\n",
      "Epoch 410, Loss: 0.6046, Val Loss: 0.5729\n",
      "Saving model at epoch 410 with loss 0.6046\n",
      "Epoch 411, Loss: 0.6045, Val Loss: 0.5729\n",
      "Saving model at epoch 411 with loss 0.6045\n",
      "Epoch 412, Loss: 0.6044, Val Loss: 0.5728\n",
      "Saving model at epoch 412 with loss 0.6044\n",
      "Epoch 413, Loss: 0.6043, Val Loss: 0.5727\n",
      "Saving model at epoch 413 with loss 0.6043\n",
      "Epoch 414, Loss: 0.6042, Val Loss: 0.5727\n",
      "Saving model at epoch 414 with loss 0.6042\n",
      "Epoch 415, Loss: 0.6041, Val Loss: 0.5726\n",
      "Saving model at epoch 415 with loss 0.6041\n",
      "Epoch 416, Loss: 0.6040, Val Loss: 0.5725\n",
      "Saving model at epoch 416 with loss 0.6040\n",
      "Epoch 417, Loss: 0.6039, Val Loss: 0.5725\n",
      "Saving model at epoch 417 with loss 0.6039\n",
      "Epoch 418, Loss: 0.6038, Val Loss: 0.5724\n",
      "Saving model at epoch 418 with loss 0.6038\n",
      "Epoch 419, Loss: 0.6037, Val Loss: 0.5723\n",
      "Saving model at epoch 419 with loss 0.6037\n",
      "Epoch 420, Loss: 0.6036, Val Loss: 0.5723\n",
      "Saving model at epoch 420 with loss 0.6036\n",
      "Epoch 421, Loss: 0.6036, Val Loss: 0.5722\n",
      "Saving model at epoch 421 with loss 0.6036\n",
      "Epoch 422, Loss: 0.6035, Val Loss: 0.5721\n",
      "Saving model at epoch 422 with loss 0.6035\n",
      "Epoch 423, Loss: 0.6034, Val Loss: 0.5721\n",
      "Saving model at epoch 423 with loss 0.6034\n",
      "Epoch 424, Loss: 0.6033, Val Loss: 0.5720\n",
      "Saving model at epoch 424 with loss 0.6033\n",
      "Epoch 425, Loss: 0.6032, Val Loss: 0.5720\n",
      "Saving model at epoch 425 with loss 0.6032\n",
      "Epoch 426, Loss: 0.6031, Val Loss: 0.5719\n",
      "Saving model at epoch 426 with loss 0.6031\n",
      "Epoch 427, Loss: 0.6030, Val Loss: 0.5718\n",
      "Saving model at epoch 427 with loss 0.6030\n",
      "Epoch 428, Loss: 0.6029, Val Loss: 0.5718\n",
      "Saving model at epoch 428 with loss 0.6029\n",
      "Epoch 429, Loss: 0.6029, Val Loss: 0.5717\n",
      "Saving model at epoch 429 with loss 0.6029\n",
      "Epoch 430, Loss: 0.6028, Val Loss: 0.5717\n",
      "Saving model at epoch 430 with loss 0.6028\n",
      "Epoch 431, Loss: 0.6027, Val Loss: 0.5716\n",
      "Saving model at epoch 431 with loss 0.6027\n",
      "Epoch 432, Loss: 0.6026, Val Loss: 0.5715\n",
      "Saving model at epoch 432 with loss 0.6026\n",
      "Epoch 433, Loss: 0.6025, Val Loss: 0.5715\n",
      "Saving model at epoch 433 with loss 0.6025\n",
      "Epoch 434, Loss: 0.6024, Val Loss: 0.5714\n",
      "Saving model at epoch 434 with loss 0.6024\n",
      "Epoch 435, Loss: 0.6023, Val Loss: 0.5714\n",
      "Saving model at epoch 435 with loss 0.6023\n",
      "Epoch 436, Loss: 0.6023, Val Loss: 0.5713\n",
      "Saving model at epoch 436 with loss 0.6023\n",
      "Epoch 437, Loss: 0.6022, Val Loss: 0.5712\n",
      "Saving model at epoch 437 with loss 0.6022\n",
      "Epoch 438, Loss: 0.6021, Val Loss: 0.5712\n",
      "Saving model at epoch 438 with loss 0.6021\n",
      "Epoch 439, Loss: 0.6020, Val Loss: 0.5711\n",
      "Saving model at epoch 439 with loss 0.6020\n",
      "Epoch 440, Loss: 0.6019, Val Loss: 0.5711\n",
      "Saving model at epoch 440 with loss 0.6019\n",
      "Epoch 441, Loss: 0.6018, Val Loss: 0.5710\n",
      "Saving model at epoch 441 with loss 0.6018\n",
      "Epoch 442, Loss: 0.6018, Val Loss: 0.5709\n",
      "Saving model at epoch 442 with loss 0.6018\n",
      "Epoch 443, Loss: 0.6017, Val Loss: 0.5709\n",
      "Saving model at epoch 443 with loss 0.6017\n",
      "Epoch 444, Loss: 0.6016, Val Loss: 0.5708\n",
      "Saving model at epoch 444 with loss 0.6016\n",
      "Epoch 445, Loss: 0.6015, Val Loss: 0.5708\n",
      "Saving model at epoch 445 with loss 0.6015\n",
      "Epoch 446, Loss: 0.6014, Val Loss: 0.5707\n",
      "Saving model at epoch 446 with loss 0.6014\n",
      "Epoch 447, Loss: 0.6014, Val Loss: 0.5707\n",
      "Saving model at epoch 447 with loss 0.6014\n",
      "Epoch 448, Loss: 0.6013, Val Loss: 0.5706\n",
      "Saving model at epoch 448 with loss 0.6013\n",
      "Epoch 449, Loss: 0.6012, Val Loss: 0.5705\n",
      "Saving model at epoch 449 with loss 0.6012\n",
      "Epoch 450, Loss: 0.6011, Val Loss: 0.5705\n",
      "Saving model at epoch 450 with loss 0.6011\n",
      "Epoch 451, Loss: 0.6010, Val Loss: 0.5704\n",
      "Saving model at epoch 451 with loss 0.6010\n",
      "Epoch 452, Loss: 0.6010, Val Loss: 0.5704\n",
      "Saving model at epoch 452 with loss 0.6010\n",
      "Epoch 453, Loss: 0.6009, Val Loss: 0.5703\n",
      "Saving model at epoch 453 with loss 0.6009\n",
      "Epoch 454, Loss: 0.6008, Val Loss: 0.5703\n",
      "Saving model at epoch 454 with loss 0.6008\n",
      "Epoch 455, Loss: 0.6007, Val Loss: 0.5702\n",
      "Saving model at epoch 455 with loss 0.6007\n",
      "Epoch 456, Loss: 0.6006, Val Loss: 0.5702\n",
      "Saving model at epoch 456 with loss 0.6006\n",
      "Epoch 457, Loss: 0.6006, Val Loss: 0.5701\n",
      "Saving model at epoch 457 with loss 0.6006\n",
      "Epoch 458, Loss: 0.6005, Val Loss: 0.5700\n",
      "Saving model at epoch 458 with loss 0.6005\n",
      "Epoch 459, Loss: 0.6004, Val Loss: 0.5700\n",
      "Saving model at epoch 459 with loss 0.6004\n",
      "Epoch 460, Loss: 0.6003, Val Loss: 0.5699\n",
      "Saving model at epoch 460 with loss 0.6003\n",
      "Epoch 461, Loss: 0.6003, Val Loss: 0.5699\n",
      "Saving model at epoch 461 with loss 0.6003\n",
      "Epoch 462, Loss: 0.6002, Val Loss: 0.5698\n",
      "Saving model at epoch 462 with loss 0.6002\n",
      "Epoch 463, Loss: 0.6001, Val Loss: 0.5698\n",
      "Saving model at epoch 463 with loss 0.6001\n",
      "Epoch 464, Loss: 0.6000, Val Loss: 0.5697\n",
      "Saving model at epoch 464 with loss 0.6000\n",
      "Epoch 465, Loss: 0.5999, Val Loss: 0.5697\n",
      "Saving model at epoch 465 with loss 0.5999\n",
      "Epoch 466, Loss: 0.5999, Val Loss: 0.5696\n",
      "Saving model at epoch 466 with loss 0.5999\n",
      "Epoch 467, Loss: 0.5998, Val Loss: 0.5696\n",
      "Saving model at epoch 467 with loss 0.5998\n",
      "Epoch 468, Loss: 0.5997, Val Loss: 0.5695\n",
      "Saving model at epoch 468 with loss 0.5997\n",
      "Epoch 469, Loss: 0.5996, Val Loss: 0.5695\n",
      "Saving model at epoch 469 with loss 0.5996\n",
      "Epoch 470, Loss: 0.5996, Val Loss: 0.5694\n",
      "Saving model at epoch 470 with loss 0.5996\n",
      "Epoch 471, Loss: 0.5995, Val Loss: 0.5694\n",
      "Saving model at epoch 471 with loss 0.5995\n",
      "Epoch 472, Loss: 0.5994, Val Loss: 0.5693\n",
      "Saving model at epoch 472 with loss 0.5994\n",
      "Epoch 473, Loss: 0.5993, Val Loss: 0.5693\n",
      "Saving model at epoch 473 with loss 0.5993\n",
      "Epoch 474, Loss: 0.5993, Val Loss: 0.5692\n",
      "Saving model at epoch 474 with loss 0.5993\n",
      "Epoch 475, Loss: 0.5992, Val Loss: 0.5692\n",
      "Saving model at epoch 475 with loss 0.5992\n",
      "Epoch 476, Loss: 0.5991, Val Loss: 0.5691\n",
      "Saving model at epoch 476 with loss 0.5991\n",
      "Epoch 477, Loss: 0.5991, Val Loss: 0.5691\n",
      "Saving model at epoch 477 with loss 0.5991\n",
      "Epoch 478, Loss: 0.5990, Val Loss: 0.5690\n",
      "Saving model at epoch 478 with loss 0.5990\n",
      "Epoch 479, Loss: 0.5989, Val Loss: 0.5690\n",
      "Saving model at epoch 479 with loss 0.5989\n",
      "Epoch 480, Loss: 0.5988, Val Loss: 0.5689\n",
      "Saving model at epoch 480 with loss 0.5988\n",
      "Epoch 481, Loss: 0.5988, Val Loss: 0.5689\n",
      "Saving model at epoch 481 with loss 0.5988\n",
      "Epoch 482, Loss: 0.5987, Val Loss: 0.5688\n",
      "Saving model at epoch 482 with loss 0.5987\n",
      "Epoch 483, Loss: 0.5986, Val Loss: 0.5688\n",
      "Saving model at epoch 483 with loss 0.5986\n",
      "Epoch 484, Loss: 0.5986, Val Loss: 0.5687\n",
      "Saving model at epoch 484 with loss 0.5986\n",
      "Epoch 485, Loss: 0.5985, Val Loss: 0.5687\n",
      "Saving model at epoch 485 with loss 0.5985\n",
      "Epoch 486, Loss: 0.5984, Val Loss: 0.5686\n",
      "Saving model at epoch 486 with loss 0.5984\n",
      "Epoch 487, Loss: 0.5983, Val Loss: 0.5686\n",
      "Saving model at epoch 487 with loss 0.5983\n",
      "Epoch 488, Loss: 0.5983, Val Loss: 0.5685\n",
      "Saving model at epoch 488 with loss 0.5983\n",
      "Epoch 489, Loss: 0.5982, Val Loss: 0.5685\n",
      "Saving model at epoch 489 with loss 0.5982\n",
      "Epoch 490, Loss: 0.5981, Val Loss: 0.5684\n",
      "Saving model at epoch 490 with loss 0.5981\n",
      "Epoch 491, Loss: 0.5981, Val Loss: 0.5684\n",
      "Saving model at epoch 491 with loss 0.5981\n",
      "Epoch 492, Loss: 0.5980, Val Loss: 0.5683\n",
      "Saving model at epoch 492 with loss 0.5980\n",
      "Epoch 493, Loss: 0.5979, Val Loss: 0.5683\n",
      "Saving model at epoch 493 with loss 0.5979\n",
      "Epoch 494, Loss: 0.5979, Val Loss: 0.5682\n",
      "Saving model at epoch 494 with loss 0.5979\n",
      "Epoch 495, Loss: 0.5978, Val Loss: 0.5682\n",
      "Saving model at epoch 495 with loss 0.5978\n",
      "Epoch 496, Loss: 0.5977, Val Loss: 0.5681\n",
      "Saving model at epoch 496 with loss 0.5977\n",
      "Epoch 497, Loss: 0.5976, Val Loss: 0.5681\n",
      "Saving model at epoch 497 with loss 0.5976\n",
      "Epoch 498, Loss: 0.5976, Val Loss: 0.5680\n",
      "Saving model at epoch 498 with loss 0.5976\n",
      "Epoch 499, Loss: 0.5975, Val Loss: 0.5680\n",
      "Saving model at epoch 499 with loss 0.5975\n",
      "Epoch 500, Loss: 0.5974, Val Loss: 0.5680\n",
      "Saving model at epoch 500 with loss 0.5974\n",
      "Epoch 501, Loss: 0.5974, Val Loss: 0.5679\n",
      "Saving model at epoch 501 with loss 0.5974\n",
      "Epoch 502, Loss: 0.5973, Val Loss: 0.5679\n",
      "Saving model at epoch 502 with loss 0.5973\n",
      "Epoch 503, Loss: 0.5972, Val Loss: 0.5678\n",
      "Saving model at epoch 503 with loss 0.5972\n",
      "Epoch 504, Loss: 0.5972, Val Loss: 0.5678\n",
      "Saving model at epoch 504 with loss 0.5972\n",
      "Epoch 505, Loss: 0.5971, Val Loss: 0.5677\n",
      "Saving model at epoch 505 with loss 0.5971\n",
      "Epoch 506, Loss: 0.5970, Val Loss: 0.5677\n",
      "Saving model at epoch 506 with loss 0.5970\n",
      "Epoch 507, Loss: 0.5970, Val Loss: 0.5676\n",
      "Saving model at epoch 507 with loss 0.5970\n",
      "Epoch 508, Loss: 0.5969, Val Loss: 0.5676\n",
      "Saving model at epoch 508 with loss 0.5969\n",
      "Epoch 509, Loss: 0.5968, Val Loss: 0.5675\n",
      "Saving model at epoch 509 with loss 0.5968\n",
      "Epoch 510, Loss: 0.5968, Val Loss: 0.5675\n",
      "Saving model at epoch 510 with loss 0.5968\n",
      "Epoch 511, Loss: 0.5967, Val Loss: 0.5675\n",
      "Saving model at epoch 511 with loss 0.5967\n",
      "Epoch 512, Loss: 0.5967, Val Loss: 0.5674\n",
      "Saving model at epoch 512 with loss 0.5967\n",
      "Epoch 513, Loss: 0.5966, Val Loss: 0.5674\n",
      "Saving model at epoch 513 with loss 0.5966\n",
      "Epoch 514, Loss: 0.5965, Val Loss: 0.5673\n",
      "Saving model at epoch 514 with loss 0.5965\n",
      "Epoch 515, Loss: 0.5965, Val Loss: 0.5673\n",
      "Saving model at epoch 515 with loss 0.5965\n",
      "Epoch 516, Loss: 0.5964, Val Loss: 0.5672\n",
      "Saving model at epoch 516 with loss 0.5964\n",
      "Epoch 517, Loss: 0.5963, Val Loss: 0.5672\n",
      "Saving model at epoch 517 with loss 0.5963\n",
      "Epoch 518, Loss: 0.5963, Val Loss: 0.5671\n",
      "Saving model at epoch 518 with loss 0.5963\n",
      "Epoch 519, Loss: 0.5962, Val Loss: 0.5671\n",
      "Saving model at epoch 519 with loss 0.5962\n",
      "Epoch 520, Loss: 0.5961, Val Loss: 0.5671\n",
      "Saving model at epoch 520 with loss 0.5961\n",
      "Epoch 521, Loss: 0.5961, Val Loss: 0.5670\n",
      "Saving model at epoch 521 with loss 0.5961\n",
      "Epoch 522, Loss: 0.5960, Val Loss: 0.5670\n",
      "Saving model at epoch 522 with loss 0.5960\n",
      "Epoch 523, Loss: 0.5959, Val Loss: 0.5669\n",
      "Saving model at epoch 523 with loss 0.5959\n",
      "Epoch 524, Loss: 0.5959, Val Loss: 0.5669\n",
      "Saving model at epoch 524 with loss 0.5959\n",
      "Epoch 525, Loss: 0.5958, Val Loss: 0.5668\n",
      "Saving model at epoch 525 with loss 0.5958\n",
      "Epoch 526, Loss: 0.5958, Val Loss: 0.5668\n",
      "Saving model at epoch 526 with loss 0.5958\n",
      "Epoch 527, Loss: 0.5957, Val Loss: 0.5668\n",
      "Saving model at epoch 527 with loss 0.5957\n",
      "Epoch 528, Loss: 0.5956, Val Loss: 0.5667\n",
      "Saving model at epoch 528 with loss 0.5956\n",
      "Epoch 529, Loss: 0.5956, Val Loss: 0.5667\n",
      "Saving model at epoch 529 with loss 0.5956\n",
      "Epoch 530, Loss: 0.5955, Val Loss: 0.5666\n",
      "Saving model at epoch 530 with loss 0.5955\n",
      "Epoch 531, Loss: 0.5954, Val Loss: 0.5666\n",
      "Saving model at epoch 531 with loss 0.5954\n",
      "Epoch 532, Loss: 0.5954, Val Loss: 0.5665\n",
      "Saving model at epoch 532 with loss 0.5954\n",
      "Epoch 533, Loss: 0.5953, Val Loss: 0.5665\n",
      "Saving model at epoch 533 with loss 0.5953\n",
      "Epoch 534, Loss: 0.5953, Val Loss: 0.5665\n",
      "Saving model at epoch 534 with loss 0.5953\n",
      "Epoch 535, Loss: 0.5952, Val Loss: 0.5664\n",
      "Saving model at epoch 535 with loss 0.5952\n",
      "Epoch 536, Loss: 0.5951, Val Loss: 0.5664\n",
      "Saving model at epoch 536 with loss 0.5951\n",
      "Epoch 537, Loss: 0.5951, Val Loss: 0.5663\n",
      "Saving model at epoch 537 with loss 0.5951\n",
      "Epoch 538, Loss: 0.5950, Val Loss: 0.5663\n",
      "Saving model at epoch 538 with loss 0.5950\n",
      "Epoch 539, Loss: 0.5950, Val Loss: 0.5662\n",
      "Saving model at epoch 539 with loss 0.5950\n",
      "Epoch 540, Loss: 0.5949, Val Loss: 0.5662\n",
      "Saving model at epoch 540 with loss 0.5949\n",
      "Epoch 541, Loss: 0.5948, Val Loss: 0.5662\n",
      "Saving model at epoch 541 with loss 0.5948\n",
      "Epoch 542, Loss: 0.5948, Val Loss: 0.5661\n",
      "Saving model at epoch 542 with loss 0.5948\n",
      "Epoch 543, Loss: 0.5947, Val Loss: 0.5661\n",
      "Saving model at epoch 543 with loss 0.5947\n",
      "Epoch 544, Loss: 0.5946, Val Loss: 0.5660\n",
      "Saving model at epoch 544 with loss 0.5946\n",
      "Epoch 545, Loss: 0.5946, Val Loss: 0.5660\n",
      "Saving model at epoch 545 with loss 0.5946\n",
      "Epoch 546, Loss: 0.5945, Val Loss: 0.5660\n",
      "Saving model at epoch 546 with loss 0.5945\n",
      "Epoch 547, Loss: 0.5945, Val Loss: 0.5659\n",
      "Saving model at epoch 547 with loss 0.5945\n",
      "Epoch 548, Loss: 0.5944, Val Loss: 0.5659\n",
      "Saving model at epoch 548 with loss 0.5944\n",
      "Epoch 549, Loss: 0.5943, Val Loss: 0.5658\n",
      "Saving model at epoch 549 with loss 0.5943\n",
      "Epoch 550, Loss: 0.5943, Val Loss: 0.5658\n",
      "Saving model at epoch 550 with loss 0.5943\n",
      "Epoch 551, Loss: 0.5942, Val Loss: 0.5658\n",
      "Saving model at epoch 551 with loss 0.5942\n",
      "Epoch 552, Loss: 0.5942, Val Loss: 0.5657\n",
      "Saving model at epoch 552 with loss 0.5942\n",
      "Epoch 553, Loss: 0.5941, Val Loss: 0.5657\n",
      "Saving model at epoch 553 with loss 0.5941\n",
      "Epoch 554, Loss: 0.5941, Val Loss: 0.5656\n",
      "Saving model at epoch 554 with loss 0.5941\n",
      "Epoch 555, Loss: 0.5940, Val Loss: 0.5656\n",
      "Saving model at epoch 555 with loss 0.5940\n",
      "Epoch 556, Loss: 0.5939, Val Loss: 0.5656\n",
      "Saving model at epoch 556 with loss 0.5939\n",
      "Epoch 557, Loss: 0.5939, Val Loss: 0.5655\n",
      "Saving model at epoch 557 with loss 0.5939\n",
      "Epoch 558, Loss: 0.5938, Val Loss: 0.5655\n",
      "Saving model at epoch 558 with loss 0.5938\n",
      "Epoch 559, Loss: 0.5938, Val Loss: 0.5654\n",
      "Saving model at epoch 559 with loss 0.5938\n",
      "Epoch 560, Loss: 0.5937, Val Loss: 0.5654\n",
      "Saving model at epoch 560 with loss 0.5937\n",
      "Epoch 561, Loss: 0.5936, Val Loss: 0.5654\n",
      "Saving model at epoch 561 with loss 0.5936\n",
      "Epoch 562, Loss: 0.5936, Val Loss: 0.5653\n",
      "Saving model at epoch 562 with loss 0.5936\n",
      "Epoch 563, Loss: 0.5935, Val Loss: 0.5653\n",
      "Saving model at epoch 563 with loss 0.5935\n",
      "Epoch 564, Loss: 0.5935, Val Loss: 0.5652\n",
      "Saving model at epoch 564 with loss 0.5935\n",
      "Epoch 565, Loss: 0.5934, Val Loss: 0.5652\n",
      "Saving model at epoch 565 with loss 0.5934\n",
      "Epoch 566, Loss: 0.5934, Val Loss: 0.5652\n",
      "Saving model at epoch 566 with loss 0.5934\n",
      "Epoch 567, Loss: 0.5933, Val Loss: 0.5651\n",
      "Saving model at epoch 567 with loss 0.5933\n",
      "Epoch 568, Loss: 0.5932, Val Loss: 0.5651\n",
      "Saving model at epoch 568 with loss 0.5932\n",
      "Epoch 569, Loss: 0.5932, Val Loss: 0.5651\n",
      "Saving model at epoch 569 with loss 0.5932\n",
      "Epoch 570, Loss: 0.5931, Val Loss: 0.5650\n",
      "Saving model at epoch 570 with loss 0.5931\n",
      "Epoch 571, Loss: 0.5931, Val Loss: 0.5650\n",
      "Saving model at epoch 571 with loss 0.5931\n",
      "Epoch 572, Loss: 0.5930, Val Loss: 0.5649\n",
      "Saving model at epoch 572 with loss 0.5930\n",
      "Epoch 573, Loss: 0.5930, Val Loss: 0.5649\n",
      "Saving model at epoch 573 with loss 0.5930\n",
      "Epoch 574, Loss: 0.5929, Val Loss: 0.5649\n",
      "Saving model at epoch 574 with loss 0.5929\n",
      "Epoch 575, Loss: 0.5928, Val Loss: 0.5648\n",
      "Saving model at epoch 575 with loss 0.5928\n",
      "Epoch 576, Loss: 0.5928, Val Loss: 0.5648\n",
      "Saving model at epoch 576 with loss 0.5928\n",
      "Epoch 577, Loss: 0.5927, Val Loss: 0.5647\n",
      "Saving model at epoch 577 with loss 0.5927\n",
      "Epoch 578, Loss: 0.5927, Val Loss: 0.5647\n",
      "Saving model at epoch 578 with loss 0.5927\n",
      "Epoch 579, Loss: 0.5926, Val Loss: 0.5647\n",
      "Saving model at epoch 579 with loss 0.5926\n",
      "Epoch 580, Loss: 0.5926, Val Loss: 0.5646\n",
      "Saving model at epoch 580 with loss 0.5926\n",
      "Epoch 581, Loss: 0.5925, Val Loss: 0.5646\n",
      "Saving model at epoch 581 with loss 0.5925\n",
      "Epoch 582, Loss: 0.5925, Val Loss: 0.5646\n",
      "Saving model at epoch 582 with loss 0.5925\n",
      "Epoch 583, Loss: 0.5924, Val Loss: 0.5645\n",
      "Saving model at epoch 583 with loss 0.5924\n",
      "Epoch 584, Loss: 0.5923, Val Loss: 0.5645\n",
      "Saving model at epoch 584 with loss 0.5923\n",
      "Epoch 585, Loss: 0.5923, Val Loss: 0.5644\n",
      "Saving model at epoch 585 with loss 0.5923\n",
      "Epoch 586, Loss: 0.5922, Val Loss: 0.5644\n",
      "Saving model at epoch 586 with loss 0.5922\n",
      "Epoch 587, Loss: 0.5922, Val Loss: 0.5644\n",
      "Saving model at epoch 587 with loss 0.5922\n",
      "Epoch 588, Loss: 0.5921, Val Loss: 0.5643\n",
      "Saving model at epoch 588 with loss 0.5921\n",
      "Epoch 589, Loss: 0.5921, Val Loss: 0.5643\n",
      "Saving model at epoch 589 with loss 0.5921\n",
      "Epoch 590, Loss: 0.5920, Val Loss: 0.5643\n",
      "Saving model at epoch 590 with loss 0.5920\n",
      "Epoch 591, Loss: 0.5920, Val Loss: 0.5642\n",
      "Saving model at epoch 591 with loss 0.5920\n",
      "Epoch 592, Loss: 0.5919, Val Loss: 0.5642\n",
      "Saving model at epoch 592 with loss 0.5919\n",
      "Epoch 593, Loss: 0.5919, Val Loss: 0.5642\n",
      "Saving model at epoch 593 with loss 0.5919\n",
      "Epoch 594, Loss: 0.5918, Val Loss: 0.5641\n",
      "Saving model at epoch 594 with loss 0.5918\n",
      "Epoch 595, Loss: 0.5917, Val Loss: 0.5641\n",
      "Saving model at epoch 595 with loss 0.5917\n",
      "Epoch 596, Loss: 0.5917, Val Loss: 0.5640\n",
      "Saving model at epoch 596 with loss 0.5917\n",
      "Epoch 597, Loss: 0.5916, Val Loss: 0.5640\n",
      "Saving model at epoch 597 with loss 0.5916\n",
      "Epoch 598, Loss: 0.5916, Val Loss: 0.5640\n",
      "Saving model at epoch 598 with loss 0.5916\n",
      "Epoch 599, Loss: 0.5915, Val Loss: 0.5639\n",
      "Saving model at epoch 599 with loss 0.5915\n",
      "Epoch 600, Loss: 0.5915, Val Loss: 0.5639\n",
      "Saving model at epoch 600 with loss 0.5915\n",
      "Epoch 601, Loss: 0.5914, Val Loss: 0.5639\n",
      "Saving model at epoch 601 with loss 0.5914\n",
      "Epoch 602, Loss: 0.5914, Val Loss: 0.5638\n",
      "Saving model at epoch 602 with loss 0.5914\n",
      "Epoch 603, Loss: 0.5913, Val Loss: 0.5638\n",
      "Saving model at epoch 603 with loss 0.5913\n",
      "Epoch 604, Loss: 0.5913, Val Loss: 0.5638\n",
      "Saving model at epoch 604 with loss 0.5913\n",
      "Epoch 605, Loss: 0.5912, Val Loss: 0.5637\n",
      "Saving model at epoch 605 with loss 0.5912\n",
      "Epoch 606, Loss: 0.5912, Val Loss: 0.5637\n",
      "Saving model at epoch 606 with loss 0.5912\n",
      "Epoch 607, Loss: 0.5911, Val Loss: 0.5636\n",
      "Saving model at epoch 607 with loss 0.5911\n",
      "Epoch 608, Loss: 0.5911, Val Loss: 0.5636\n",
      "Saving model at epoch 608 with loss 0.5911\n",
      "Epoch 609, Loss: 0.5910, Val Loss: 0.5636\n",
      "Saving model at epoch 609 with loss 0.5910\n",
      "Epoch 610, Loss: 0.5909, Val Loss: 0.5635\n",
      "Saving model at epoch 610 with loss 0.5909\n",
      "Epoch 611, Loss: 0.5909, Val Loss: 0.5635\n",
      "Saving model at epoch 611 with loss 0.5909\n",
      "Epoch 612, Loss: 0.5908, Val Loss: 0.5635\n",
      "Saving model at epoch 612 with loss 0.5908\n",
      "Epoch 613, Loss: 0.5908, Val Loss: 0.5634\n",
      "Saving model at epoch 613 with loss 0.5908\n",
      "Epoch 614, Loss: 0.5907, Val Loss: 0.5634\n",
      "Saving model at epoch 614 with loss 0.5907\n",
      "Epoch 615, Loss: 0.5907, Val Loss: 0.5634\n",
      "Saving model at epoch 615 with loss 0.5907\n",
      "Epoch 616, Loss: 0.5906, Val Loss: 0.5633\n",
      "Saving model at epoch 616 with loss 0.5906\n",
      "Epoch 617, Loss: 0.5906, Val Loss: 0.5633\n",
      "Saving model at epoch 617 with loss 0.5906\n",
      "Epoch 618, Loss: 0.5905, Val Loss: 0.5633\n",
      "Saving model at epoch 618 with loss 0.5905\n",
      "Epoch 619, Loss: 0.5905, Val Loss: 0.5632\n",
      "Saving model at epoch 619 with loss 0.5905\n",
      "Epoch 620, Loss: 0.5904, Val Loss: 0.5632\n",
      "Saving model at epoch 620 with loss 0.5904\n",
      "Epoch 621, Loss: 0.5904, Val Loss: 0.5632\n",
      "Saving model at epoch 621 with loss 0.5904\n",
      "Epoch 622, Loss: 0.5903, Val Loss: 0.5631\n",
      "Saving model at epoch 622 with loss 0.5903\n",
      "Epoch 623, Loss: 0.5903, Val Loss: 0.5631\n",
      "Saving model at epoch 623 with loss 0.5903\n",
      "Epoch 624, Loss: 0.5902, Val Loss: 0.5630\n",
      "Saving model at epoch 624 with loss 0.5902\n",
      "Epoch 625, Loss: 0.5902, Val Loss: 0.5630\n",
      "Saving model at epoch 625 with loss 0.5902\n",
      "Epoch 626, Loss: 0.5901, Val Loss: 0.5630\n",
      "Saving model at epoch 626 with loss 0.5901\n",
      "Epoch 627, Loss: 0.5901, Val Loss: 0.5629\n",
      "Saving model at epoch 627 with loss 0.5901\n",
      "Epoch 628, Loss: 0.5900, Val Loss: 0.5629\n",
      "Saving model at epoch 628 with loss 0.5900\n",
      "Epoch 629, Loss: 0.5900, Val Loss: 0.5629\n",
      "Saving model at epoch 629 with loss 0.5900\n",
      "Epoch 630, Loss: 0.5899, Val Loss: 0.5628\n",
      "Saving model at epoch 630 with loss 0.5899\n",
      "Epoch 631, Loss: 0.5899, Val Loss: 0.5628\n",
      "Saving model at epoch 631 with loss 0.5899\n",
      "Epoch 632, Loss: 0.5898, Val Loss: 0.5628\n",
      "Saving model at epoch 632 with loss 0.5898\n",
      "Epoch 633, Loss: 0.5898, Val Loss: 0.5627\n",
      "Saving model at epoch 633 with loss 0.5898\n",
      "Epoch 634, Loss: 0.5897, Val Loss: 0.5627\n",
      "Saving model at epoch 634 with loss 0.5897\n",
      "Epoch 635, Loss: 0.5897, Val Loss: 0.5627\n",
      "Saving model at epoch 635 with loss 0.5897\n",
      "Epoch 636, Loss: 0.5896, Val Loss: 0.5626\n",
      "Saving model at epoch 636 with loss 0.5896\n",
      "Epoch 637, Loss: 0.5896, Val Loss: 0.5626\n",
      "Saving model at epoch 637 with loss 0.5896\n",
      "Epoch 638, Loss: 0.5895, Val Loss: 0.5626\n",
      "Saving model at epoch 638 with loss 0.5895\n",
      "Epoch 639, Loss: 0.5895, Val Loss: 0.5625\n",
      "Saving model at epoch 639 with loss 0.5895\n",
      "Epoch 640, Loss: 0.5894, Val Loss: 0.5625\n",
      "Saving model at epoch 640 with loss 0.5894\n",
      "Epoch 641, Loss: 0.5894, Val Loss: 0.5625\n",
      "Saving model at epoch 641 with loss 0.5894\n",
      "Epoch 642, Loss: 0.5893, Val Loss: 0.5624\n",
      "Saving model at epoch 642 with loss 0.5893\n",
      "Epoch 643, Loss: 0.5893, Val Loss: 0.5624\n",
      "Saving model at epoch 643 with loss 0.5893\n",
      "Epoch 644, Loss: 0.5892, Val Loss: 0.5624\n",
      "Saving model at epoch 644 with loss 0.5892\n",
      "Epoch 645, Loss: 0.5892, Val Loss: 0.5623\n",
      "Saving model at epoch 645 with loss 0.5892\n",
      "Epoch 646, Loss: 0.5891, Val Loss: 0.5623\n",
      "Saving model at epoch 646 with loss 0.5891\n",
      "Epoch 647, Loss: 0.5891, Val Loss: 0.5623\n",
      "Saving model at epoch 647 with loss 0.5891\n",
      "Epoch 648, Loss: 0.5890, Val Loss: 0.5622\n",
      "Saving model at epoch 648 with loss 0.5890\n",
      "Epoch 649, Loss: 0.5890, Val Loss: 0.5622\n",
      "Saving model at epoch 649 with loss 0.5890\n",
      "Epoch 650, Loss: 0.5889, Val Loss: 0.5622\n",
      "Saving model at epoch 650 with loss 0.5889\n",
      "Epoch 651, Loss: 0.5889, Val Loss: 0.5621\n",
      "Saving model at epoch 651 with loss 0.5889\n",
      "Epoch 652, Loss: 0.5888, Val Loss: 0.5621\n",
      "Saving model at epoch 652 with loss 0.5888\n",
      "Epoch 653, Loss: 0.5888, Val Loss: 0.5621\n",
      "Saving model at epoch 653 with loss 0.5888\n",
      "Epoch 654, Loss: 0.5887, Val Loss: 0.5620\n",
      "Saving model at epoch 654 with loss 0.5887\n",
      "Epoch 655, Loss: 0.5887, Val Loss: 0.5620\n",
      "Saving model at epoch 655 with loss 0.5887\n",
      "Epoch 656, Loss: 0.5886, Val Loss: 0.5620\n",
      "Saving model at epoch 656 with loss 0.5886\n",
      "Epoch 657, Loss: 0.5886, Val Loss: 0.5619\n",
      "Saving model at epoch 657 with loss 0.5886\n",
      "Epoch 658, Loss: 0.5885, Val Loss: 0.5619\n",
      "Saving model at epoch 658 with loss 0.5885\n",
      "Epoch 659, Loss: 0.5885, Val Loss: 0.5619\n",
      "Saving model at epoch 659 with loss 0.5885\n",
      "Epoch 660, Loss: 0.5884, Val Loss: 0.5618\n",
      "Saving model at epoch 660 with loss 0.5884\n",
      "Epoch 661, Loss: 0.5884, Val Loss: 0.5618\n",
      "Saving model at epoch 661 with loss 0.5884\n",
      "Epoch 662, Loss: 0.5883, Val Loss: 0.5618\n",
      "Saving model at epoch 662 with loss 0.5883\n",
      "Epoch 663, Loss: 0.5883, Val Loss: 0.5617\n",
      "Saving model at epoch 663 with loss 0.5883\n",
      "Epoch 664, Loss: 0.5882, Val Loss: 0.5617\n",
      "Saving model at epoch 664 with loss 0.5882\n",
      "Epoch 665, Loss: 0.5882, Val Loss: 0.5617\n",
      "Saving model at epoch 665 with loss 0.5882\n",
      "Epoch 666, Loss: 0.5882, Val Loss: 0.5616\n",
      "Saving model at epoch 666 with loss 0.5882\n",
      "Epoch 667, Loss: 0.5881, Val Loss: 0.5616\n",
      "Saving model at epoch 667 with loss 0.5881\n",
      "Epoch 668, Loss: 0.5881, Val Loss: 0.5616\n",
      "Saving model at epoch 668 with loss 0.5881\n",
      "Epoch 669, Loss: 0.5880, Val Loss: 0.5615\n",
      "Saving model at epoch 669 with loss 0.5880\n",
      "Epoch 670, Loss: 0.5880, Val Loss: 0.5615\n",
      "Saving model at epoch 670 with loss 0.5880\n",
      "Epoch 671, Loss: 0.5879, Val Loss: 0.5615\n",
      "Saving model at epoch 671 with loss 0.5879\n",
      "Epoch 672, Loss: 0.5879, Val Loss: 0.5614\n",
      "Saving model at epoch 672 with loss 0.5879\n",
      "Epoch 673, Loss: 0.5878, Val Loss: 0.5614\n",
      "Saving model at epoch 673 with loss 0.5878\n",
      "Epoch 674, Loss: 0.5878, Val Loss: 0.5614\n",
      "Saving model at epoch 674 with loss 0.5878\n",
      "Epoch 675, Loss: 0.5877, Val Loss: 0.5613\n",
      "Saving model at epoch 675 with loss 0.5877\n",
      "Epoch 676, Loss: 0.5877, Val Loss: 0.5613\n",
      "Saving model at epoch 676 with loss 0.5877\n",
      "Epoch 677, Loss: 0.5876, Val Loss: 0.5613\n",
      "Saving model at epoch 677 with loss 0.5876\n",
      "Epoch 678, Loss: 0.5876, Val Loss: 0.5612\n",
      "Saving model at epoch 678 with loss 0.5876\n",
      "Epoch 679, Loss: 0.5875, Val Loss: 0.5612\n",
      "Saving model at epoch 679 with loss 0.5875\n",
      "Epoch 680, Loss: 0.5875, Val Loss: 0.5612\n",
      "Saving model at epoch 680 with loss 0.5875\n",
      "Epoch 681, Loss: 0.5874, Val Loss: 0.5611\n",
      "Saving model at epoch 681 with loss 0.5874\n",
      "Epoch 682, Loss: 0.5874, Val Loss: 0.5611\n",
      "Saving model at epoch 682 with loss 0.5874\n",
      "Epoch 683, Loss: 0.5873, Val Loss: 0.5611\n",
      "Saving model at epoch 683 with loss 0.5873\n",
      "Epoch 684, Loss: 0.5873, Val Loss: 0.5610\n",
      "Saving model at epoch 684 with loss 0.5873\n",
      "Epoch 685, Loss: 0.5873, Val Loss: 0.5610\n",
      "Saving model at epoch 685 with loss 0.5873\n",
      "Epoch 686, Loss: 0.5872, Val Loss: 0.5610\n",
      "Saving model at epoch 686 with loss 0.5872\n",
      "Epoch 687, Loss: 0.5872, Val Loss: 0.5609\n",
      "Saving model at epoch 687 with loss 0.5872\n",
      "Epoch 688, Loss: 0.5871, Val Loss: 0.5609\n",
      "Saving model at epoch 688 with loss 0.5871\n",
      "Epoch 689, Loss: 0.5871, Val Loss: 0.5609\n",
      "Saving model at epoch 689 with loss 0.5871\n",
      "Epoch 690, Loss: 0.5870, Val Loss: 0.5608\n",
      "Saving model at epoch 690 with loss 0.5870\n",
      "Epoch 691, Loss: 0.5870, Val Loss: 0.5608\n",
      "Saving model at epoch 691 with loss 0.5870\n",
      "Epoch 692, Loss: 0.5869, Val Loss: 0.5608\n",
      "Saving model at epoch 692 with loss 0.5869\n",
      "Epoch 693, Loss: 0.5869, Val Loss: 0.5607\n",
      "Saving model at epoch 693 with loss 0.5869\n",
      "Epoch 694, Loss: 0.5868, Val Loss: 0.5607\n",
      "Saving model at epoch 694 with loss 0.5868\n",
      "Epoch 695, Loss: 0.5868, Val Loss: 0.5607\n",
      "Saving model at epoch 695 with loss 0.5868\n",
      "Epoch 696, Loss: 0.5867, Val Loss: 0.5606\n",
      "Saving model at epoch 696 with loss 0.5867\n",
      "Epoch 697, Loss: 0.5867, Val Loss: 0.5606\n",
      "Saving model at epoch 697 with loss 0.5867\n",
      "Epoch 698, Loss: 0.5867, Val Loss: 0.5606\n",
      "Saving model at epoch 698 with loss 0.5867\n",
      "Epoch 699, Loss: 0.5866, Val Loss: 0.5606\n",
      "Saving model at epoch 699 with loss 0.5866\n",
      "Epoch 700, Loss: 0.5866, Val Loss: 0.5605\n",
      "Saving model at epoch 700 with loss 0.5866\n",
      "Epoch 701, Loss: 0.5865, Val Loss: 0.5605\n",
      "Saving model at epoch 701 with loss 0.5865\n",
      "Epoch 702, Loss: 0.5865, Val Loss: 0.5605\n",
      "Saving model at epoch 702 with loss 0.5865\n",
      "Epoch 703, Loss: 0.5864, Val Loss: 0.5604\n",
      "Saving model at epoch 703 with loss 0.5864\n",
      "Epoch 704, Loss: 0.5864, Val Loss: 0.5604\n",
      "Saving model at epoch 704 with loss 0.5864\n",
      "Epoch 705, Loss: 0.5863, Val Loss: 0.5604\n",
      "Saving model at epoch 705 with loss 0.5863\n",
      "Epoch 706, Loss: 0.5863, Val Loss: 0.5603\n",
      "Saving model at epoch 706 with loss 0.5863\n",
      "Epoch 707, Loss: 0.5862, Val Loss: 0.5603\n",
      "Saving model at epoch 707 with loss 0.5862\n",
      "Epoch 708, Loss: 0.5862, Val Loss: 0.5603\n",
      "Saving model at epoch 708 with loss 0.5862\n",
      "Epoch 709, Loss: 0.5861, Val Loss: 0.5602\n",
      "Saving model at epoch 709 with loss 0.5861\n",
      "Epoch 710, Loss: 0.5861, Val Loss: 0.5602\n",
      "Saving model at epoch 710 with loss 0.5861\n",
      "Epoch 711, Loss: 0.5861, Val Loss: 0.5602\n",
      "Saving model at epoch 711 with loss 0.5861\n",
      "Epoch 712, Loss: 0.5860, Val Loss: 0.5601\n",
      "Saving model at epoch 712 with loss 0.5860\n",
      "Epoch 713, Loss: 0.5860, Val Loss: 0.5601\n",
      "Saving model at epoch 713 with loss 0.5860\n",
      "Epoch 714, Loss: 0.5859, Val Loss: 0.5601\n",
      "Saving model at epoch 714 with loss 0.5859\n",
      "Epoch 715, Loss: 0.5859, Val Loss: 0.5600\n",
      "Saving model at epoch 715 with loss 0.5859\n",
      "Epoch 716, Loss: 0.5858, Val Loss: 0.5600\n",
      "Saving model at epoch 716 with loss 0.5858\n",
      "Epoch 717, Loss: 0.5858, Val Loss: 0.5600\n",
      "Saving model at epoch 717 with loss 0.5858\n",
      "Epoch 718, Loss: 0.5857, Val Loss: 0.5599\n",
      "Saving model at epoch 718 with loss 0.5857\n",
      "Epoch 719, Loss: 0.5857, Val Loss: 0.5599\n",
      "Saving model at epoch 719 with loss 0.5857\n",
      "Epoch 720, Loss: 0.5856, Val Loss: 0.5599\n",
      "Saving model at epoch 720 with loss 0.5856\n",
      "Epoch 721, Loss: 0.5856, Val Loss: 0.5599\n",
      "Saving model at epoch 721 with loss 0.5856\n",
      "Epoch 722, Loss: 0.5856, Val Loss: 0.5598\n",
      "Saving model at epoch 722 with loss 0.5856\n",
      "Epoch 723, Loss: 0.5855, Val Loss: 0.5598\n",
      "Saving model at epoch 723 with loss 0.5855\n",
      "Epoch 724, Loss: 0.5855, Val Loss: 0.5598\n",
      "Saving model at epoch 724 with loss 0.5855\n",
      "Epoch 725, Loss: 0.5854, Val Loss: 0.5597\n",
      "Saving model at epoch 725 with loss 0.5854\n",
      "Epoch 726, Loss: 0.5854, Val Loss: 0.5597\n",
      "Saving model at epoch 726 with loss 0.5854\n",
      "Epoch 727, Loss: 0.5853, Val Loss: 0.5597\n",
      "Saving model at epoch 727 with loss 0.5853\n",
      "Epoch 728, Loss: 0.5853, Val Loss: 0.5596\n",
      "Saving model at epoch 728 with loss 0.5853\n",
      "Epoch 729, Loss: 0.5852, Val Loss: 0.5596\n",
      "Saving model at epoch 729 with loss 0.5852\n",
      "Epoch 730, Loss: 0.5852, Val Loss: 0.5596\n",
      "Saving model at epoch 730 with loss 0.5852\n",
      "Epoch 731, Loss: 0.5851, Val Loss: 0.5595\n",
      "Saving model at epoch 731 with loss 0.5851\n",
      "Epoch 732, Loss: 0.5851, Val Loss: 0.5595\n",
      "Saving model at epoch 732 with loss 0.5851\n",
      "Epoch 733, Loss: 0.5851, Val Loss: 0.5595\n",
      "Saving model at epoch 733 with loss 0.5851\n",
      "Epoch 734, Loss: 0.5850, Val Loss: 0.5594\n",
      "Saving model at epoch 734 with loss 0.5850\n",
      "Epoch 735, Loss: 0.5850, Val Loss: 0.5594\n",
      "Saving model at epoch 735 with loss 0.5850\n",
      "Epoch 736, Loss: 0.5849, Val Loss: 0.5594\n",
      "Saving model at epoch 736 with loss 0.5849\n",
      "Epoch 737, Loss: 0.5849, Val Loss: 0.5593\n",
      "Saving model at epoch 737 with loss 0.5849\n",
      "Epoch 738, Loss: 0.5848, Val Loss: 0.5593\n",
      "Saving model at epoch 738 with loss 0.5848\n",
      "Epoch 739, Loss: 0.5848, Val Loss: 0.5593\n",
      "Saving model at epoch 739 with loss 0.5848\n",
      "Epoch 740, Loss: 0.5847, Val Loss: 0.5592\n",
      "Saving model at epoch 740 with loss 0.5847\n",
      "Epoch 741, Loss: 0.5847, Val Loss: 0.5592\n",
      "Saving model at epoch 741 with loss 0.5847\n",
      "Epoch 742, Loss: 0.5847, Val Loss: 0.5592\n",
      "Saving model at epoch 742 with loss 0.5847\n",
      "Epoch 743, Loss: 0.5846, Val Loss: 0.5591\n",
      "Saving model at epoch 743 with loss 0.5846\n",
      "Epoch 744, Loss: 0.5846, Val Loss: 0.5591\n",
      "Saving model at epoch 744 with loss 0.5846\n",
      "Epoch 745, Loss: 0.5845, Val Loss: 0.5591\n",
      "Saving model at epoch 745 with loss 0.5845\n",
      "Epoch 746, Loss: 0.5845, Val Loss: 0.5591\n",
      "Saving model at epoch 746 with loss 0.5845\n",
      "Epoch 747, Loss: 0.5844, Val Loss: 0.5590\n",
      "Saving model at epoch 747 with loss 0.5844\n",
      "Epoch 748, Loss: 0.5844, Val Loss: 0.5590\n",
      "Saving model at epoch 748 with loss 0.5844\n",
      "Epoch 749, Loss: 0.5843, Val Loss: 0.5590\n",
      "Saving model at epoch 749 with loss 0.5843\n",
      "Epoch 750, Loss: 0.5843, Val Loss: 0.5589\n",
      "Saving model at epoch 750 with loss 0.5843\n",
      "Epoch 751, Loss: 0.5842, Val Loss: 0.5589\n",
      "Saving model at epoch 751 with loss 0.5842\n",
      "Epoch 752, Loss: 0.5842, Val Loss: 0.5589\n",
      "Saving model at epoch 752 with loss 0.5842\n",
      "Epoch 753, Loss: 0.5842, Val Loss: 0.5588\n",
      "Saving model at epoch 753 with loss 0.5842\n",
      "Epoch 754, Loss: 0.5841, Val Loss: 0.5588\n",
      "Saving model at epoch 754 with loss 0.5841\n",
      "Epoch 755, Loss: 0.5841, Val Loss: 0.5588\n",
      "Saving model at epoch 755 with loss 0.5841\n",
      "Epoch 756, Loss: 0.5840, Val Loss: 0.5587\n",
      "Saving model at epoch 756 with loss 0.5840\n",
      "Epoch 757, Loss: 0.5840, Val Loss: 0.5587\n",
      "Saving model at epoch 757 with loss 0.5840\n",
      "Epoch 758, Loss: 0.5839, Val Loss: 0.5587\n",
      "Saving model at epoch 758 with loss 0.5839\n",
      "Epoch 759, Loss: 0.5839, Val Loss: 0.5586\n",
      "Saving model at epoch 759 with loss 0.5839\n",
      "Epoch 760, Loss: 0.5838, Val Loss: 0.5586\n",
      "Saving model at epoch 760 with loss 0.5838\n",
      "Epoch 761, Loss: 0.5838, Val Loss: 0.5586\n",
      "Saving model at epoch 761 with loss 0.5838\n",
      "Epoch 762, Loss: 0.5838, Val Loss: 0.5585\n",
      "Saving model at epoch 762 with loss 0.5838\n",
      "Epoch 763, Loss: 0.5837, Val Loss: 0.5585\n",
      "Saving model at epoch 763 with loss 0.5837\n",
      "Epoch 764, Loss: 0.5837, Val Loss: 0.5585\n",
      "Saving model at epoch 764 with loss 0.5837\n",
      "Epoch 765, Loss: 0.5836, Val Loss: 0.5584\n",
      "Saving model at epoch 765 with loss 0.5836\n",
      "Epoch 766, Loss: 0.5836, Val Loss: 0.5584\n",
      "Saving model at epoch 766 with loss 0.5836\n",
      "Epoch 767, Loss: 0.5835, Val Loss: 0.5584\n",
      "Saving model at epoch 767 with loss 0.5835\n",
      "Epoch 768, Loss: 0.5835, Val Loss: 0.5583\n",
      "Saving model at epoch 768 with loss 0.5835\n",
      "Epoch 769, Loss: 0.5834, Val Loss: 0.5583\n",
      "Saving model at epoch 769 with loss 0.5834\n",
      "Epoch 770, Loss: 0.5834, Val Loss: 0.5583\n",
      "Saving model at epoch 770 with loss 0.5834\n",
      "Epoch 771, Loss: 0.5834, Val Loss: 0.5582\n",
      "Saving model at epoch 771 with loss 0.5834\n",
      "Epoch 772, Loss: 0.5833, Val Loss: 0.5582\n",
      "Saving model at epoch 772 with loss 0.5833\n",
      "Epoch 773, Loss: 0.5833, Val Loss: 0.5582\n",
      "Saving model at epoch 773 with loss 0.5833\n",
      "Epoch 774, Loss: 0.5832, Val Loss: 0.5582\n",
      "Saving model at epoch 774 with loss 0.5832\n",
      "Epoch 775, Loss: 0.5832, Val Loss: 0.5581\n",
      "Saving model at epoch 775 with loss 0.5832\n",
      "Epoch 776, Loss: 0.5831, Val Loss: 0.5581\n",
      "Saving model at epoch 776 with loss 0.5831\n",
      "Epoch 777, Loss: 0.5831, Val Loss: 0.5581\n",
      "Saving model at epoch 777 with loss 0.5831\n",
      "Epoch 778, Loss: 0.5830, Val Loss: 0.5580\n",
      "Saving model at epoch 778 with loss 0.5830\n",
      "Epoch 779, Loss: 0.5830, Val Loss: 0.5580\n",
      "Saving model at epoch 779 with loss 0.5830\n",
      "Epoch 780, Loss: 0.5830, Val Loss: 0.5580\n",
      "Saving model at epoch 780 with loss 0.5830\n",
      "Epoch 781, Loss: 0.5829, Val Loss: 0.5579\n",
      "Saving model at epoch 781 with loss 0.5829\n",
      "Epoch 782, Loss: 0.5829, Val Loss: 0.5579\n",
      "Saving model at epoch 782 with loss 0.5829\n",
      "Epoch 783, Loss: 0.5828, Val Loss: 0.5579\n",
      "Saving model at epoch 783 with loss 0.5828\n",
      "Epoch 784, Loss: 0.5828, Val Loss: 0.5578\n",
      "Saving model at epoch 784 with loss 0.5828\n",
      "Epoch 785, Loss: 0.5827, Val Loss: 0.5578\n",
      "Saving model at epoch 785 with loss 0.5827\n",
      "Epoch 786, Loss: 0.5827, Val Loss: 0.5578\n",
      "Saving model at epoch 786 with loss 0.5827\n",
      "Epoch 787, Loss: 0.5826, Val Loss: 0.5577\n",
      "Saving model at epoch 787 with loss 0.5826\n",
      "Epoch 788, Loss: 0.5826, Val Loss: 0.5577\n",
      "Saving model at epoch 788 with loss 0.5826\n",
      "Epoch 789, Loss: 0.5825, Val Loss: 0.5577\n",
      "Saving model at epoch 789 with loss 0.5825\n",
      "Epoch 790, Loss: 0.5825, Val Loss: 0.5576\n",
      "Saving model at epoch 790 with loss 0.5825\n",
      "Epoch 791, Loss: 0.5825, Val Loss: 0.5576\n",
      "Saving model at epoch 791 with loss 0.5825\n",
      "Epoch 792, Loss: 0.5824, Val Loss: 0.5576\n",
      "Saving model at epoch 792 with loss 0.5824\n",
      "Epoch 793, Loss: 0.5824, Val Loss: 0.5575\n",
      "Saving model at epoch 793 with loss 0.5824\n",
      "Epoch 794, Loss: 0.5823, Val Loss: 0.5575\n",
      "Saving model at epoch 794 with loss 0.5823\n",
      "Epoch 795, Loss: 0.5823, Val Loss: 0.5575\n",
      "Saving model at epoch 795 with loss 0.5823\n",
      "Epoch 796, Loss: 0.5822, Val Loss: 0.5574\n",
      "Saving model at epoch 796 with loss 0.5822\n",
      "Epoch 797, Loss: 0.5822, Val Loss: 0.5574\n",
      "Saving model at epoch 797 with loss 0.5822\n",
      "Epoch 798, Loss: 0.5821, Val Loss: 0.5574\n",
      "Saving model at epoch 798 with loss 0.5821\n",
      "Epoch 799, Loss: 0.5821, Val Loss: 0.5573\n",
      "Saving model at epoch 799 with loss 0.5821\n",
      "Epoch 800, Loss: 0.5820, Val Loss: 0.5573\n",
      "Saving model at epoch 800 with loss 0.5820\n",
      "Epoch 801, Loss: 0.5820, Val Loss: 0.5573\n",
      "Saving model at epoch 801 with loss 0.5820\n",
      "Epoch 802, Loss: 0.5820, Val Loss: 0.5572\n",
      "Saving model at epoch 802 with loss 0.5820\n",
      "Epoch 803, Loss: 0.5819, Val Loss: 0.5572\n",
      "Saving model at epoch 803 with loss 0.5819\n",
      "Epoch 804, Loss: 0.5819, Val Loss: 0.5572\n",
      "Saving model at epoch 804 with loss 0.5819\n",
      "Epoch 805, Loss: 0.5818, Val Loss: 0.5571\n",
      "Saving model at epoch 805 with loss 0.5818\n",
      "Epoch 806, Loss: 0.5818, Val Loss: 0.5571\n",
      "Saving model at epoch 806 with loss 0.5818\n",
      "Epoch 807, Loss: 0.5817, Val Loss: 0.5571\n",
      "Saving model at epoch 807 with loss 0.5817\n",
      "Epoch 808, Loss: 0.5817, Val Loss: 0.5570\n",
      "Saving model at epoch 808 with loss 0.5817\n",
      "Epoch 809, Loss: 0.5816, Val Loss: 0.5570\n",
      "Saving model at epoch 809 with loss 0.5816\n",
      "Epoch 810, Loss: 0.5816, Val Loss: 0.5570\n",
      "Saving model at epoch 810 with loss 0.5816\n",
      "Epoch 811, Loss: 0.5815, Val Loss: 0.5569\n",
      "Saving model at epoch 811 with loss 0.5815\n",
      "Epoch 812, Loss: 0.5815, Val Loss: 0.5569\n",
      "Saving model at epoch 812 with loss 0.5815\n",
      "Epoch 813, Loss: 0.5815, Val Loss: 0.5569\n",
      "Saving model at epoch 813 with loss 0.5815\n",
      "Epoch 814, Loss: 0.5814, Val Loss: 0.5568\n",
      "Saving model at epoch 814 with loss 0.5814\n",
      "Epoch 815, Loss: 0.5814, Val Loss: 0.5568\n",
      "Saving model at epoch 815 with loss 0.5814\n",
      "Epoch 816, Loss: 0.5813, Val Loss: 0.5568\n",
      "Saving model at epoch 816 with loss 0.5813\n",
      "Epoch 817, Loss: 0.5813, Val Loss: 0.5567\n",
      "Saving model at epoch 817 with loss 0.5813\n",
      "Epoch 818, Loss: 0.5812, Val Loss: 0.5567\n",
      "Saving model at epoch 818 with loss 0.5812\n",
      "Epoch 819, Loss: 0.5812, Val Loss: 0.5567\n",
      "Saving model at epoch 819 with loss 0.5812\n",
      "Epoch 820, Loss: 0.5811, Val Loss: 0.5566\n",
      "Saving model at epoch 820 with loss 0.5811\n",
      "Epoch 821, Loss: 0.5811, Val Loss: 0.5566\n",
      "Saving model at epoch 821 with loss 0.5811\n",
      "Epoch 822, Loss: 0.5810, Val Loss: 0.5566\n",
      "Saving model at epoch 822 with loss 0.5810\n",
      "Epoch 823, Loss: 0.5810, Val Loss: 0.5565\n",
      "Saving model at epoch 823 with loss 0.5810\n",
      "Epoch 824, Loss: 0.5810, Val Loss: 0.5565\n",
      "Saving model at epoch 824 with loss 0.5810\n",
      "Epoch 825, Loss: 0.5809, Val Loss: 0.5565\n",
      "Saving model at epoch 825 with loss 0.5809\n",
      "Epoch 826, Loss: 0.5809, Val Loss: 0.5564\n",
      "Saving model at epoch 826 with loss 0.5809\n",
      "Epoch 827, Loss: 0.5808, Val Loss: 0.5564\n",
      "Saving model at epoch 827 with loss 0.5808\n",
      "Epoch 828, Loss: 0.5808, Val Loss: 0.5564\n",
      "Saving model at epoch 828 with loss 0.5808\n",
      "Epoch 829, Loss: 0.5807, Val Loss: 0.5563\n",
      "Saving model at epoch 829 with loss 0.5807\n",
      "Epoch 830, Loss: 0.5807, Val Loss: 0.5563\n",
      "Saving model at epoch 830 with loss 0.5807\n",
      "Epoch 831, Loss: 0.5806, Val Loss: 0.5563\n",
      "Saving model at epoch 831 with loss 0.5806\n",
      "Epoch 832, Loss: 0.5806, Val Loss: 0.5562\n",
      "Saving model at epoch 832 with loss 0.5806\n",
      "Epoch 833, Loss: 0.5805, Val Loss: 0.5562\n",
      "Saving model at epoch 833 with loss 0.5805\n",
      "Epoch 834, Loss: 0.5805, Val Loss: 0.5562\n",
      "Saving model at epoch 834 with loss 0.5805\n",
      "Epoch 835, Loss: 0.5805, Val Loss: 0.5561\n",
      "Saving model at epoch 835 with loss 0.5805\n",
      "Epoch 836, Loss: 0.5804, Val Loss: 0.5561\n",
      "Saving model at epoch 836 with loss 0.5804\n",
      "Epoch 837, Loss: 0.5804, Val Loss: 0.5561\n",
      "Saving model at epoch 837 with loss 0.5804\n",
      "Epoch 838, Loss: 0.5803, Val Loss: 0.5560\n",
      "Saving model at epoch 838 with loss 0.5803\n",
      "Epoch 839, Loss: 0.5803, Val Loss: 0.5560\n",
      "Saving model at epoch 839 with loss 0.5803\n",
      "Epoch 840, Loss: 0.5802, Val Loss: 0.5560\n",
      "Saving model at epoch 840 with loss 0.5802\n",
      "Epoch 841, Loss: 0.5802, Val Loss: 0.5559\n",
      "Saving model at epoch 841 with loss 0.5802\n",
      "Epoch 842, Loss: 0.5801, Val Loss: 0.5559\n",
      "Saving model at epoch 842 with loss 0.5801\n",
      "Epoch 843, Loss: 0.5801, Val Loss: 0.5559\n",
      "Saving model at epoch 843 with loss 0.5801\n",
      "Epoch 844, Loss: 0.5800, Val Loss: 0.5558\n",
      "Saving model at epoch 844 with loss 0.5800\n",
      "Epoch 845, Loss: 0.5800, Val Loss: 0.5558\n",
      "Saving model at epoch 845 with loss 0.5800\n",
      "Epoch 846, Loss: 0.5800, Val Loss: 0.5558\n",
      "Saving model at epoch 846 with loss 0.5800\n",
      "Epoch 847, Loss: 0.5799, Val Loss: 0.5557\n",
      "Saving model at epoch 847 with loss 0.5799\n",
      "Epoch 848, Loss: 0.5799, Val Loss: 0.5557\n",
      "Saving model at epoch 848 with loss 0.5799\n",
      "Epoch 849, Loss: 0.5798, Val Loss: 0.5557\n",
      "Saving model at epoch 849 with loss 0.5798\n",
      "Epoch 850, Loss: 0.5798, Val Loss: 0.5556\n",
      "Saving model at epoch 850 with loss 0.5798\n",
      "Epoch 851, Loss: 0.5797, Val Loss: 0.5556\n",
      "Saving model at epoch 851 with loss 0.5797\n",
      "Epoch 852, Loss: 0.5797, Val Loss: 0.5556\n",
      "Saving model at epoch 852 with loss 0.5797\n",
      "Epoch 853, Loss: 0.5796, Val Loss: 0.5555\n",
      "Saving model at epoch 853 with loss 0.5796\n",
      "Epoch 854, Loss: 0.5796, Val Loss: 0.5555\n",
      "Saving model at epoch 854 with loss 0.5796\n",
      "Epoch 855, Loss: 0.5795, Val Loss: 0.5555\n",
      "Saving model at epoch 855 with loss 0.5795\n",
      "Epoch 856, Loss: 0.5795, Val Loss: 0.5554\n",
      "Saving model at epoch 856 with loss 0.5795\n",
      "Epoch 857, Loss: 0.5794, Val Loss: 0.5554\n",
      "Saving model at epoch 857 with loss 0.5794\n",
      "Epoch 858, Loss: 0.5794, Val Loss: 0.5554\n",
      "Saving model at epoch 858 with loss 0.5794\n",
      "Epoch 859, Loss: 0.5794, Val Loss: 0.5553\n",
      "Saving model at epoch 859 with loss 0.5794\n",
      "Epoch 860, Loss: 0.5793, Val Loss: 0.5553\n",
      "Saving model at epoch 860 with loss 0.5793\n",
      "Epoch 861, Loss: 0.5793, Val Loss: 0.5553\n",
      "Saving model at epoch 861 with loss 0.5793\n",
      "Epoch 862, Loss: 0.5792, Val Loss: 0.5552\n",
      "Saving model at epoch 862 with loss 0.5792\n",
      "Epoch 863, Loss: 0.5792, Val Loss: 0.5552\n",
      "Saving model at epoch 863 with loss 0.5792\n",
      "Epoch 864, Loss: 0.5791, Val Loss: 0.5552\n",
      "Saving model at epoch 864 with loss 0.5791\n",
      "Epoch 865, Loss: 0.5791, Val Loss: 0.5551\n",
      "Saving model at epoch 865 with loss 0.5791\n",
      "Epoch 866, Loss: 0.5790, Val Loss: 0.5551\n",
      "Saving model at epoch 866 with loss 0.5790\n",
      "Epoch 867, Loss: 0.5790, Val Loss: 0.5550\n",
      "Saving model at epoch 867 with loss 0.5790\n",
      "Epoch 868, Loss: 0.5789, Val Loss: 0.5550\n",
      "Saving model at epoch 868 with loss 0.5789\n",
      "Epoch 869, Loss: 0.5789, Val Loss: 0.5550\n",
      "Saving model at epoch 869 with loss 0.5789\n",
      "Epoch 870, Loss: 0.5789, Val Loss: 0.5549\n",
      "Saving model at epoch 870 with loss 0.5789\n",
      "Epoch 871, Loss: 0.5788, Val Loss: 0.5549\n",
      "Saving model at epoch 871 with loss 0.5788\n",
      "Epoch 872, Loss: 0.5788, Val Loss: 0.5549\n",
      "Saving model at epoch 872 with loss 0.5788\n",
      "Epoch 873, Loss: 0.5787, Val Loss: 0.5548\n",
      "Saving model at epoch 873 with loss 0.5787\n",
      "Epoch 874, Loss: 0.5787, Val Loss: 0.5548\n",
      "Saving model at epoch 874 with loss 0.5787\n",
      "Epoch 875, Loss: 0.5786, Val Loss: 0.5548\n",
      "Saving model at epoch 875 with loss 0.5786\n",
      "Epoch 876, Loss: 0.5786, Val Loss: 0.5547\n",
      "Saving model at epoch 876 with loss 0.5786\n",
      "Epoch 877, Loss: 0.5785, Val Loss: 0.5547\n",
      "Saving model at epoch 877 with loss 0.5785\n",
      "Epoch 878, Loss: 0.5785, Val Loss: 0.5547\n",
      "Saving model at epoch 878 with loss 0.5785\n",
      "Epoch 879, Loss: 0.5784, Val Loss: 0.5546\n",
      "Saving model at epoch 879 with loss 0.5784\n",
      "Epoch 880, Loss: 0.5784, Val Loss: 0.5546\n",
      "Saving model at epoch 880 with loss 0.5784\n",
      "Epoch 881, Loss: 0.5783, Val Loss: 0.5546\n",
      "Saving model at epoch 881 with loss 0.5783\n",
      "Epoch 882, Loss: 0.5783, Val Loss: 0.5545\n",
      "Saving model at epoch 882 with loss 0.5783\n",
      "Epoch 883, Loss: 0.5783, Val Loss: 0.5545\n",
      "Saving model at epoch 883 with loss 0.5783\n",
      "Epoch 884, Loss: 0.5782, Val Loss: 0.5545\n",
      "Saving model at epoch 884 with loss 0.5782\n",
      "Epoch 885, Loss: 0.5782, Val Loss: 0.5544\n",
      "Saving model at epoch 885 with loss 0.5782\n",
      "Epoch 886, Loss: 0.5781, Val Loss: 0.5544\n",
      "Saving model at epoch 886 with loss 0.5781\n",
      "Epoch 887, Loss: 0.5781, Val Loss: 0.5543\n",
      "Saving model at epoch 887 with loss 0.5781\n",
      "Epoch 888, Loss: 0.5780, Val Loss: 0.5543\n",
      "Saving model at epoch 888 with loss 0.5780\n",
      "Epoch 889, Loss: 0.5780, Val Loss: 0.5543\n",
      "Saving model at epoch 889 with loss 0.5780\n",
      "Epoch 890, Loss: 0.5779, Val Loss: 0.5542\n",
      "Saving model at epoch 890 with loss 0.5779\n",
      "Epoch 891, Loss: 0.5779, Val Loss: 0.5542\n",
      "Saving model at epoch 891 with loss 0.5779\n",
      "Epoch 892, Loss: 0.5778, Val Loss: 0.5542\n",
      "Saving model at epoch 892 with loss 0.5778\n",
      "Epoch 893, Loss: 0.5778, Val Loss: 0.5541\n",
      "Saving model at epoch 893 with loss 0.5778\n",
      "Epoch 894, Loss: 0.5777, Val Loss: 0.5541\n",
      "Saving model at epoch 894 with loss 0.5777\n",
      "Epoch 895, Loss: 0.5777, Val Loss: 0.5541\n",
      "Saving model at epoch 895 with loss 0.5777\n",
      "Epoch 896, Loss: 0.5776, Val Loss: 0.5540\n",
      "Saving model at epoch 896 with loss 0.5776\n",
      "Epoch 897, Loss: 0.5776, Val Loss: 0.5540\n",
      "Saving model at epoch 897 with loss 0.5776\n",
      "Epoch 898, Loss: 0.5776, Val Loss: 0.5540\n",
      "Saving model at epoch 898 with loss 0.5776\n",
      "Epoch 899, Loss: 0.5775, Val Loss: 0.5539\n",
      "Saving model at epoch 899 with loss 0.5775\n",
      "Epoch 900, Loss: 0.5775, Val Loss: 0.5539\n",
      "Saving model at epoch 900 with loss 0.5775\n",
      "Epoch 901, Loss: 0.5774, Val Loss: 0.5538\n",
      "Saving model at epoch 901 with loss 0.5774\n",
      "Epoch 902, Loss: 0.5774, Val Loss: 0.5538\n",
      "Saving model at epoch 902 with loss 0.5774\n",
      "Epoch 903, Loss: 0.5773, Val Loss: 0.5538\n",
      "Saving model at epoch 903 with loss 0.5773\n",
      "Epoch 904, Loss: 0.5773, Val Loss: 0.5537\n",
      "Saving model at epoch 904 with loss 0.5773\n",
      "Epoch 905, Loss: 0.5772, Val Loss: 0.5537\n",
      "Saving model at epoch 905 with loss 0.5772\n",
      "Epoch 906, Loss: 0.5772, Val Loss: 0.5537\n",
      "Saving model at epoch 906 with loss 0.5772\n",
      "Epoch 907, Loss: 0.5771, Val Loss: 0.5536\n",
      "Saving model at epoch 907 with loss 0.5771\n",
      "Epoch 908, Loss: 0.5771, Val Loss: 0.5536\n",
      "Saving model at epoch 908 with loss 0.5771\n",
      "Epoch 909, Loss: 0.5770, Val Loss: 0.5536\n",
      "Saving model at epoch 909 with loss 0.5770\n",
      "Epoch 910, Loss: 0.5770, Val Loss: 0.5535\n",
      "Saving model at epoch 910 with loss 0.5770\n",
      "Epoch 911, Loss: 0.5769, Val Loss: 0.5535\n",
      "Saving model at epoch 911 with loss 0.5769\n",
      "Epoch 912, Loss: 0.5769, Val Loss: 0.5534\n",
      "Saving model at epoch 912 with loss 0.5769\n",
      "Epoch 913, Loss: 0.5768, Val Loss: 0.5534\n",
      "Saving model at epoch 913 with loss 0.5768\n",
      "Epoch 914, Loss: 0.5768, Val Loss: 0.5534\n",
      "Saving model at epoch 914 with loss 0.5768\n",
      "Epoch 915, Loss: 0.5768, Val Loss: 0.5533\n",
      "Saving model at epoch 915 with loss 0.5768\n",
      "Epoch 916, Loss: 0.5767, Val Loss: 0.5533\n",
      "Saving model at epoch 916 with loss 0.5767\n",
      "Epoch 917, Loss: 0.5767, Val Loss: 0.5533\n",
      "Saving model at epoch 917 with loss 0.5767\n",
      "Epoch 918, Loss: 0.5766, Val Loss: 0.5532\n",
      "Saving model at epoch 918 with loss 0.5766\n",
      "Epoch 919, Loss: 0.5766, Val Loss: 0.5532\n",
      "Saving model at epoch 919 with loss 0.5766\n",
      "Epoch 920, Loss: 0.5765, Val Loss: 0.5531\n",
      "Saving model at epoch 920 with loss 0.5765\n",
      "Epoch 921, Loss: 0.5765, Val Loss: 0.5531\n",
      "Saving model at epoch 921 with loss 0.5765\n",
      "Epoch 922, Loss: 0.5764, Val Loss: 0.5531\n",
      "Saving model at epoch 922 with loss 0.5764\n",
      "Epoch 923, Loss: 0.5764, Val Loss: 0.5530\n",
      "Saving model at epoch 923 with loss 0.5764\n",
      "Epoch 924, Loss: 0.5763, Val Loss: 0.5530\n",
      "Saving model at epoch 924 with loss 0.5763\n",
      "Epoch 925, Loss: 0.5763, Val Loss: 0.5530\n",
      "Saving model at epoch 925 with loss 0.5763\n",
      "Epoch 926, Loss: 0.5762, Val Loss: 0.5529\n",
      "Saving model at epoch 926 with loss 0.5762\n",
      "Epoch 927, Loss: 0.5762, Val Loss: 0.5529\n",
      "Saving model at epoch 927 with loss 0.5762\n",
      "Epoch 928, Loss: 0.5761, Val Loss: 0.5528\n",
      "Saving model at epoch 928 with loss 0.5761\n",
      "Epoch 929, Loss: 0.5761, Val Loss: 0.5528\n",
      "Saving model at epoch 929 with loss 0.5761\n",
      "Epoch 930, Loss: 0.5760, Val Loss: 0.5528\n",
      "Saving model at epoch 930 with loss 0.5760\n",
      "Epoch 931, Loss: 0.5760, Val Loss: 0.5527\n",
      "Saving model at epoch 931 with loss 0.5760\n",
      "Epoch 932, Loss: 0.5759, Val Loss: 0.5527\n",
      "Saving model at epoch 932 with loss 0.5759\n",
      "Epoch 933, Loss: 0.5759, Val Loss: 0.5527\n",
      "Saving model at epoch 933 with loss 0.5759\n",
      "Epoch 934, Loss: 0.5758, Val Loss: 0.5526\n",
      "Saving model at epoch 934 with loss 0.5758\n",
      "Epoch 935, Loss: 0.5758, Val Loss: 0.5526\n",
      "Saving model at epoch 935 with loss 0.5758\n",
      "Epoch 936, Loss: 0.5757, Val Loss: 0.5525\n",
      "Saving model at epoch 936 with loss 0.5757\n",
      "Epoch 937, Loss: 0.5757, Val Loss: 0.5525\n",
      "Saving model at epoch 937 with loss 0.5757\n",
      "Epoch 938, Loss: 0.5757, Val Loss: 0.5525\n",
      "Saving model at epoch 938 with loss 0.5757\n",
      "Epoch 939, Loss: 0.5756, Val Loss: 0.5524\n",
      "Saving model at epoch 939 with loss 0.5756\n",
      "Epoch 940, Loss: 0.5756, Val Loss: 0.5524\n",
      "Saving model at epoch 940 with loss 0.5756\n",
      "Epoch 941, Loss: 0.5755, Val Loss: 0.5524\n",
      "Saving model at epoch 941 with loss 0.5755\n",
      "Epoch 942, Loss: 0.5755, Val Loss: 0.5523\n",
      "Saving model at epoch 942 with loss 0.5755\n",
      "Epoch 943, Loss: 0.5754, Val Loss: 0.5523\n",
      "Saving model at epoch 943 with loss 0.5754\n",
      "Epoch 944, Loss: 0.5754, Val Loss: 0.5522\n",
      "Saving model at epoch 944 with loss 0.5754\n",
      "Epoch 945, Loss: 0.5753, Val Loss: 0.5522\n",
      "Saving model at epoch 945 with loss 0.5753\n",
      "Epoch 946, Loss: 0.5753, Val Loss: 0.5522\n",
      "Saving model at epoch 946 with loss 0.5753\n",
      "Epoch 947, Loss: 0.5752, Val Loss: 0.5521\n",
      "Saving model at epoch 947 with loss 0.5752\n",
      "Epoch 948, Loss: 0.5752, Val Loss: 0.5521\n",
      "Saving model at epoch 948 with loss 0.5752\n",
      "Epoch 949, Loss: 0.5751, Val Loss: 0.5521\n",
      "Saving model at epoch 949 with loss 0.5751\n",
      "Epoch 950, Loss: 0.5751, Val Loss: 0.5520\n",
      "Saving model at epoch 950 with loss 0.5751\n",
      "Epoch 951, Loss: 0.5750, Val Loss: 0.5520\n",
      "Saving model at epoch 951 with loss 0.5750\n",
      "Epoch 952, Loss: 0.5750, Val Loss: 0.5519\n",
      "Saving model at epoch 952 with loss 0.5750\n",
      "Epoch 953, Loss: 0.5749, Val Loss: 0.5519\n",
      "Saving model at epoch 953 with loss 0.5749\n",
      "Epoch 954, Loss: 0.5749, Val Loss: 0.5519\n",
      "Saving model at epoch 954 with loss 0.5749\n",
      "Epoch 955, Loss: 0.5748, Val Loss: 0.5518\n",
      "Saving model at epoch 955 with loss 0.5748\n",
      "Epoch 956, Loss: 0.5748, Val Loss: 0.5518\n",
      "Saving model at epoch 956 with loss 0.5748\n",
      "Epoch 957, Loss: 0.5747, Val Loss: 0.5517\n",
      "Saving model at epoch 957 with loss 0.5747\n",
      "Epoch 958, Loss: 0.5747, Val Loss: 0.5517\n",
      "Saving model at epoch 958 with loss 0.5747\n",
      "Epoch 959, Loss: 0.5746, Val Loss: 0.5517\n",
      "Saving model at epoch 959 with loss 0.5746\n",
      "Epoch 960, Loss: 0.5746, Val Loss: 0.5516\n",
      "Saving model at epoch 960 with loss 0.5746\n",
      "Epoch 961, Loss: 0.5745, Val Loss: 0.5516\n",
      "Saving model at epoch 961 with loss 0.5745\n",
      "Epoch 962, Loss: 0.5745, Val Loss: 0.5515\n",
      "Saving model at epoch 962 with loss 0.5745\n",
      "Epoch 963, Loss: 0.5744, Val Loss: 0.5515\n",
      "Saving model at epoch 963 with loss 0.5744\n",
      "Epoch 964, Loss: 0.5744, Val Loss: 0.5515\n",
      "Saving model at epoch 964 with loss 0.5744\n",
      "Epoch 965, Loss: 0.5743, Val Loss: 0.5514\n",
      "Saving model at epoch 965 with loss 0.5743\n",
      "Epoch 966, Loss: 0.5743, Val Loss: 0.5514\n",
      "Saving model at epoch 966 with loss 0.5743\n",
      "Epoch 967, Loss: 0.5742, Val Loss: 0.5514\n",
      "Saving model at epoch 967 with loss 0.5742\n",
      "Epoch 968, Loss: 0.5742, Val Loss: 0.5513\n",
      "Saving model at epoch 968 with loss 0.5742\n",
      "Epoch 969, Loss: 0.5741, Val Loss: 0.5513\n",
      "Saving model at epoch 969 with loss 0.5741\n",
      "Epoch 970, Loss: 0.5741, Val Loss: 0.5512\n",
      "Saving model at epoch 970 with loss 0.5741\n",
      "Epoch 971, Loss: 0.5740, Val Loss: 0.5512\n",
      "Saving model at epoch 971 with loss 0.5740\n",
      "Epoch 972, Loss: 0.5740, Val Loss: 0.5512\n",
      "Saving model at epoch 972 with loss 0.5740\n",
      "Epoch 973, Loss: 0.5739, Val Loss: 0.5511\n",
      "Saving model at epoch 973 with loss 0.5739\n",
      "Epoch 974, Loss: 0.5739, Val Loss: 0.5511\n",
      "Saving model at epoch 974 with loss 0.5739\n",
      "Epoch 975, Loss: 0.5739, Val Loss: 0.5510\n",
      "Saving model at epoch 975 with loss 0.5739\n",
      "Epoch 976, Loss: 0.5738, Val Loss: 0.5510\n",
      "Saving model at epoch 976 with loss 0.5738\n",
      "Epoch 977, Loss: 0.5738, Val Loss: 0.5510\n",
      "Saving model at epoch 977 with loss 0.5738\n",
      "Epoch 978, Loss: 0.5737, Val Loss: 0.5509\n",
      "Saving model at epoch 978 with loss 0.5737\n",
      "Epoch 979, Loss: 0.5737, Val Loss: 0.5509\n",
      "Saving model at epoch 979 with loss 0.5737\n",
      "Epoch 980, Loss: 0.5736, Val Loss: 0.5508\n",
      "Saving model at epoch 980 with loss 0.5736\n",
      "Epoch 981, Loss: 0.5736, Val Loss: 0.5508\n",
      "Saving model at epoch 981 with loss 0.5736\n",
      "Epoch 982, Loss: 0.5735, Val Loss: 0.5508\n",
      "Saving model at epoch 982 with loss 0.5735\n",
      "Epoch 983, Loss: 0.5735, Val Loss: 0.5507\n",
      "Saving model at epoch 983 with loss 0.5735\n",
      "Epoch 984, Loss: 0.5734, Val Loss: 0.5507\n",
      "Saving model at epoch 984 with loss 0.5734\n",
      "Epoch 985, Loss: 0.5734, Val Loss: 0.5506\n",
      "Saving model at epoch 985 with loss 0.5734\n",
      "Epoch 986, Loss: 0.5733, Val Loss: 0.5506\n",
      "Saving model at epoch 986 with loss 0.5733\n",
      "Epoch 987, Loss: 0.5733, Val Loss: 0.5506\n",
      "Saving model at epoch 987 with loss 0.5733\n",
      "Epoch 988, Loss: 0.5732, Val Loss: 0.5505\n",
      "Saving model at epoch 988 with loss 0.5732\n",
      "Epoch 989, Loss: 0.5732, Val Loss: 0.5505\n",
      "Saving model at epoch 989 with loss 0.5732\n",
      "Epoch 990, Loss: 0.5731, Val Loss: 0.5504\n",
      "Saving model at epoch 990 with loss 0.5731\n",
      "Epoch 991, Loss: 0.5731, Val Loss: 0.5504\n",
      "Saving model at epoch 991 with loss 0.5731\n",
      "Epoch 992, Loss: 0.5730, Val Loss: 0.5504\n",
      "Saving model at epoch 992 with loss 0.5730\n",
      "Epoch 993, Loss: 0.5730, Val Loss: 0.5503\n",
      "Saving model at epoch 993 with loss 0.5730\n",
      "Epoch 994, Loss: 0.5729, Val Loss: 0.5503\n",
      "Saving model at epoch 994 with loss 0.5729\n",
      "Epoch 995, Loss: 0.5729, Val Loss: 0.5502\n",
      "Saving model at epoch 995 with loss 0.5729\n",
      "Epoch 996, Loss: 0.5728, Val Loss: 0.5502\n",
      "Saving model at epoch 996 with loss 0.5728\n",
      "Epoch 997, Loss: 0.5728, Val Loss: 0.5501\n",
      "Saving model at epoch 997 with loss 0.5728\n",
      "Epoch 998, Loss: 0.5727, Val Loss: 0.5501\n",
      "Saving model at epoch 998 with loss 0.5727\n",
      "Epoch 999, Loss: 0.5726, Val Loss: 0.5501\n",
      "Saving model at epoch 999 with loss 0.5726\n",
      "Test Loss (MSE): 0.6108\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the features we want to keep\n",
    "# Keep only pickup and drop off positions\n",
    "features_model1 = [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]\n",
    "\n",
    "# Split 10% of the training data as validation set\n",
    "model1_x = select_features(X_train, features_model1)\n",
    "model1_y = y_train\n",
    "model1_x_train, model1_x_val, model1_y_train, model1_y_val = train_test_split(model1_x, model1_y, test_size=0.1, random_state=0)\n",
    "model1_x_test = select_features(X_test, features_model1)\n",
    "\n",
    "# Define Model\n",
    "model1 = Sequential()\n",
    "model1.add(LinearLayer(model1_x_train.shape[1], 4))\n",
    "model1.add(ReLU())\n",
    "model1.add(LinearLayer(4, 4))\n",
    "model1.add(ReLU())\n",
    "model1.add(LinearLayer(4, 4))\n",
    "model1.add(ReLU())\n",
    "model1.add(LinearLayer(4, 1))\n",
    "\n",
    "# Training Setup\n",
    "epochs_model1 = 1000\n",
    "learning_rate_model1 = 0.01\n",
    "loss_function_model1 = MeanSquaredErrorLoss()\n",
    "\n",
    "best_loss_model1 = float(\"inf\")\n",
    "patience_model1 = 3  # Early stopping patience\n",
    "stagnation_model1 = 0\n",
    "losses_model1 = []\n",
    "val_losses_model1 = []\n",
    "\n",
    "for epoch_model1 in range(epochs_model1):\n",
    "    predictions_model1 = model1.forward(model1_x_train)\n",
    "    loss_model1 = loss_function_model1.forward(predictions_model1, model1_y_train)\n",
    "    losses_model1.append(loss_model1)\n",
    "    \n",
    "    val_predictions_model1 = model1.forward(model1_x_val)\n",
    "    val_loss_model1 = loss_function_model1.forward(val_predictions_model1, model1_y_val)\n",
    "    val_losses_model1.append(val_loss_model1)\n",
    "    \n",
    "    print(f\"Epoch {epoch_model1}, Loss: {loss_model1:.4f}, Val Loss: {val_loss_model1:.4f}\")\n",
    "    \n",
    "    grad_output_model1 = loss_function_model1.backward()\n",
    "    model1.backward(grad_output_model1, learning_rate_model1)\n",
    "\n",
    "    # Early stopping\n",
    "    if loss_model1 < best_loss_model1:\n",
    "        best_loss_model1 = loss_model1\n",
    "        stagnation_model1 = 0\n",
    "        print(f\"Saving model at epoch {epoch_model1} with loss {loss_model1:.4f}\")\n",
    "    else:\n",
    "        stagnation_model1 += 1\n",
    "        if stagnation_model1 >= patience_model1:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load best model and evaluate on test set\n",
    "test_predictions_model1 = model1.forward(model1_x_test)\n",
    "test_loss_model1 = loss_function_model1.forward(test_predictions_model1, y_test)\n",
    "print(f\"Test Loss (MSE): {test_loss_model1:.4f}\")\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses_model1, label='Training Loss')\n",
    "plt.plot(val_losses_model1, label='Validation Loss', linestyle='dashed')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickup Time, Position and Dropoff Time, Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 46.2181, Val Loss: 46.3487\n",
      "Saving model at epoch 0 with loss 46.2181\n",
      "Epoch 1, Loss: 40.8533, Val Loss: 40.8892\n",
      "Saving model at epoch 1 with loss 40.8533\n",
      "Epoch 2, Loss: 37.6247, Val Loss: 37.6351\n",
      "Saving model at epoch 2 with loss 37.6247\n",
      "Epoch 3, Loss: 34.6855, Val Loss: 34.6815\n",
      "Saving model at epoch 3 with loss 34.6855\n",
      "Epoch 4, Loss: 31.6511, Val Loss: 31.6443\n",
      "Saving model at epoch 4 with loss 31.6511\n",
      "Epoch 5, Loss: 28.2940, Val Loss: 28.3011\n",
      "Saving model at epoch 5 with loss 28.2940\n",
      "Epoch 6, Loss: 24.4603, Val Loss: 24.5111\n",
      "Saving model at epoch 6 with loss 24.4603\n",
      "Epoch 7, Loss: 20.1506, Val Loss: 20.2834\n",
      "Saving model at epoch 7 with loss 20.1506\n",
      "Epoch 8, Loss: 15.7335, Val Loss: 15.9766\n",
      "Saving model at epoch 8 with loss 15.7335\n",
      "Epoch 9, Loss: 12.0551, Val Loss: 12.3740\n",
      "Saving model at epoch 9 with loss 12.0551\n",
      "Epoch 10, Loss: 9.5629, Val Loss: 9.8618\n",
      "Saving model at epoch 10 with loss 9.5629\n",
      "Epoch 11, Loss: 7.8965, Val Loss: 8.1352\n",
      "Saving model at epoch 11 with loss 7.8965\n",
      "Epoch 12, Loss: 6.6349, Val Loss: 6.8387\n",
      "Saving model at epoch 12 with loss 6.6349\n",
      "Epoch 13, Loss: 5.6397, Val Loss: 5.8324\n",
      "Saving model at epoch 13 with loss 5.6397\n",
      "Epoch 14, Loss: 4.8709, Val Loss: 5.0529\n",
      "Saving model at epoch 14 with loss 4.8709\n",
      "Epoch 15, Loss: 4.2847, Val Loss: 4.4570\n",
      "Saving model at epoch 15 with loss 4.2847\n",
      "Epoch 16, Loss: 3.8382, Val Loss: 4.0018\n",
      "Saving model at epoch 16 with loss 3.8382\n",
      "Epoch 17, Loss: 3.4951, Val Loss: 3.6498\n",
      "Saving model at epoch 17 with loss 3.4951\n",
      "Epoch 18, Loss: 3.2255, Val Loss: 3.3707\n",
      "Saving model at epoch 18 with loss 3.2255\n",
      "Epoch 19, Loss: 3.0091, Val Loss: 3.1442\n",
      "Saving model at epoch 19 with loss 3.0091\n",
      "Epoch 20, Loss: 2.8308, Val Loss: 2.9555\n",
      "Saving model at epoch 20 with loss 2.8308\n",
      "Epoch 21, Loss: 2.6811, Val Loss: 2.7951\n",
      "Saving model at epoch 21 with loss 2.6811\n",
      "Epoch 22, Loss: 2.5528, Val Loss: 2.6560\n",
      "Saving model at epoch 22 with loss 2.5528\n",
      "Epoch 23, Loss: 2.4410, Val Loss: 2.5330\n",
      "Saving model at epoch 23 with loss 2.4410\n",
      "Epoch 24, Loss: 2.3406, Val Loss: 2.4208\n",
      "Saving model at epoch 24 with loss 2.3406\n",
      "Epoch 25, Loss: 2.2502, Val Loss: 2.3194\n",
      "Saving model at epoch 25 with loss 2.2502\n",
      "Epoch 26, Loss: 2.1683, Val Loss: 2.2270\n",
      "Saving model at epoch 26 with loss 2.1683\n",
      "Epoch 27, Loss: 2.0935, Val Loss: 2.1420\n",
      "Saving model at epoch 27 with loss 2.0935\n",
      "Epoch 28, Loss: 2.0247, Val Loss: 2.0635\n",
      "Saving model at epoch 28 with loss 2.0247\n",
      "Epoch 29, Loss: 1.9614, Val Loss: 1.9906\n",
      "Saving model at epoch 29 with loss 1.9614\n",
      "Epoch 30, Loss: 1.9027, Val Loss: 1.9239\n",
      "Saving model at epoch 30 with loss 1.9027\n",
      "Epoch 31, Loss: 1.8489, Val Loss: 1.8615\n",
      "Saving model at epoch 31 with loss 1.8489\n",
      "Epoch 32, Loss: 1.7986, Val Loss: 1.8038\n",
      "Saving model at epoch 32 with loss 1.7986\n",
      "Epoch 33, Loss: 1.7507, Val Loss: 1.7493\n",
      "Saving model at epoch 33 with loss 1.7507\n",
      "Epoch 34, Loss: 1.7064, Val Loss: 1.6981\n",
      "Saving model at epoch 34 with loss 1.7064\n",
      "Epoch 35, Loss: 1.6648, Val Loss: 1.6506\n",
      "Saving model at epoch 35 with loss 1.6648\n",
      "Epoch 36, Loss: 1.6247, Val Loss: 1.6056\n",
      "Saving model at epoch 36 with loss 1.6247\n",
      "Epoch 37, Loss: 1.5876, Val Loss: 1.5630\n",
      "Saving model at epoch 37 with loss 1.5876\n",
      "Epoch 38, Loss: 1.5525, Val Loss: 1.5234\n",
      "Saving model at epoch 38 with loss 1.5525\n",
      "Epoch 39, Loss: 1.5185, Val Loss: 1.4852\n",
      "Saving model at epoch 39 with loss 1.5185\n",
      "Epoch 40, Loss: 1.4864, Val Loss: 1.4478\n",
      "Saving model at epoch 40 with loss 1.4864\n",
      "Epoch 41, Loss: 1.4559, Val Loss: 1.4129\n",
      "Saving model at epoch 41 with loss 1.4559\n",
      "Epoch 42, Loss: 1.4264, Val Loss: 1.3797\n",
      "Saving model at epoch 42 with loss 1.4264\n",
      "Epoch 43, Loss: 1.3990, Val Loss: 1.3482\n",
      "Saving model at epoch 43 with loss 1.3990\n",
      "Epoch 44, Loss: 1.3729, Val Loss: 1.3186\n",
      "Saving model at epoch 44 with loss 1.3729\n",
      "Epoch 45, Loss: 1.3475, Val Loss: 1.2907\n",
      "Saving model at epoch 45 with loss 1.3475\n",
      "Epoch 46, Loss: 1.3240, Val Loss: 1.2642\n",
      "Saving model at epoch 46 with loss 1.3240\n",
      "Epoch 47, Loss: 1.3015, Val Loss: 1.2393\n",
      "Saving model at epoch 47 with loss 1.3015\n",
      "Epoch 48, Loss: 1.2797, Val Loss: 1.2159\n",
      "Saving model at epoch 48 with loss 1.2797\n",
      "Epoch 49, Loss: 1.2593, Val Loss: 1.1935\n",
      "Saving model at epoch 49 with loss 1.2593\n",
      "Epoch 50, Loss: 1.2399, Val Loss: 1.1723\n",
      "Saving model at epoch 50 with loss 1.2399\n",
      "Epoch 51, Loss: 1.2211, Val Loss: 1.1525\n",
      "Saving model at epoch 51 with loss 1.2211\n",
      "Epoch 52, Loss: 1.2035, Val Loss: 1.1334\n",
      "Saving model at epoch 52 with loss 1.2035\n",
      "Epoch 53, Loss: 1.1867, Val Loss: 1.1153\n",
      "Saving model at epoch 53 with loss 1.1867\n",
      "Epoch 54, Loss: 1.1705, Val Loss: 1.0983\n",
      "Saving model at epoch 54 with loss 1.1705\n",
      "Epoch 55, Loss: 1.1548, Val Loss: 1.0819\n",
      "Saving model at epoch 55 with loss 1.1548\n",
      "Epoch 56, Loss: 1.1400, Val Loss: 1.0663\n",
      "Saving model at epoch 56 with loss 1.1400\n",
      "Epoch 57, Loss: 1.1258, Val Loss: 1.0515\n",
      "Saving model at epoch 57 with loss 1.1258\n",
      "Epoch 58, Loss: 1.1120, Val Loss: 1.0374\n",
      "Saving model at epoch 58 with loss 1.1120\n",
      "Epoch 59, Loss: 1.0989, Val Loss: 1.0238\n",
      "Saving model at epoch 59 with loss 1.0989\n",
      "Epoch 60, Loss: 1.0863, Val Loss: 1.0108\n",
      "Saving model at epoch 60 with loss 1.0863\n",
      "Epoch 61, Loss: 1.0738, Val Loss: 0.9977\n",
      "Saving model at epoch 61 with loss 1.0738\n",
      "Epoch 62, Loss: 1.0620, Val Loss: 0.9851\n",
      "Saving model at epoch 62 with loss 1.0620\n",
      "Epoch 63, Loss: 1.0508, Val Loss: 0.9733\n",
      "Saving model at epoch 63 with loss 1.0508\n",
      "Epoch 64, Loss: 1.0397, Val Loss: 0.9620\n",
      "Saving model at epoch 64 with loss 1.0397\n",
      "Epoch 65, Loss: 1.0294, Val Loss: 0.9512\n",
      "Saving model at epoch 65 with loss 1.0294\n",
      "Epoch 66, Loss: 1.0194, Val Loss: 0.9409\n",
      "Saving model at epoch 66 with loss 1.0194\n",
      "Epoch 67, Loss: 1.0096, Val Loss: 0.9311\n",
      "Saving model at epoch 67 with loss 1.0096\n",
      "Epoch 68, Loss: 1.0003, Val Loss: 0.9216\n",
      "Saving model at epoch 68 with loss 1.0003\n",
      "Epoch 69, Loss: 0.9914, Val Loss: 0.9126\n",
      "Saving model at epoch 69 with loss 0.9914\n",
      "Epoch 70, Loss: 0.9826, Val Loss: 0.9040\n",
      "Saving model at epoch 70 with loss 0.9826\n",
      "Epoch 71, Loss: 0.9742, Val Loss: 0.8956\n",
      "Saving model at epoch 71 with loss 0.9742\n",
      "Epoch 72, Loss: 0.9662, Val Loss: 0.8876\n",
      "Saving model at epoch 72 with loss 0.9662\n",
      "Epoch 73, Loss: 0.9583, Val Loss: 0.8799\n",
      "Saving model at epoch 73 with loss 0.9583\n",
      "Epoch 74, Loss: 0.9507, Val Loss: 0.8725\n",
      "Saving model at epoch 74 with loss 0.9507\n",
      "Epoch 75, Loss: 0.9434, Val Loss: 0.8653\n",
      "Saving model at epoch 75 with loss 0.9434\n",
      "Epoch 76, Loss: 0.9362, Val Loss: 0.8584\n",
      "Saving model at epoch 76 with loss 0.9362\n",
      "Epoch 77, Loss: 0.9293, Val Loss: 0.8518\n",
      "Saving model at epoch 77 with loss 0.9293\n",
      "Epoch 78, Loss: 0.9227, Val Loss: 0.8453\n",
      "Saving model at epoch 78 with loss 0.9227\n",
      "Epoch 79, Loss: 0.9162, Val Loss: 0.8391\n",
      "Saving model at epoch 79 with loss 0.9162\n",
      "Epoch 80, Loss: 0.9098, Val Loss: 0.8331\n",
      "Saving model at epoch 80 with loss 0.9098\n",
      "Epoch 81, Loss: 0.9037, Val Loss: 0.8273\n",
      "Saving model at epoch 81 with loss 0.9037\n",
      "Epoch 82, Loss: 0.8978, Val Loss: 0.8217\n",
      "Saving model at epoch 82 with loss 0.8978\n",
      "Epoch 83, Loss: 0.8921, Val Loss: 0.8162\n",
      "Saving model at epoch 83 with loss 0.8921\n",
      "Epoch 84, Loss: 0.8864, Val Loss: 0.8110\n",
      "Saving model at epoch 84 with loss 0.8864\n",
      "Epoch 85, Loss: 0.8810, Val Loss: 0.8058\n",
      "Saving model at epoch 85 with loss 0.8810\n",
      "Epoch 86, Loss: 0.8757, Val Loss: 0.8009\n",
      "Saving model at epoch 86 with loss 0.8757\n",
      "Epoch 87, Loss: 0.8705, Val Loss: 0.7961\n",
      "Saving model at epoch 87 with loss 0.8705\n",
      "Epoch 88, Loss: 0.8655, Val Loss: 0.7914\n",
      "Saving model at epoch 88 with loss 0.8655\n",
      "Epoch 89, Loss: 0.8606, Val Loss: 0.7868\n",
      "Saving model at epoch 89 with loss 0.8606\n",
      "Epoch 90, Loss: 0.8559, Val Loss: 0.7824\n",
      "Saving model at epoch 90 with loss 0.8559\n",
      "Epoch 91, Loss: 0.8513, Val Loss: 0.7782\n",
      "Saving model at epoch 91 with loss 0.8513\n",
      "Epoch 92, Loss: 0.8468, Val Loss: 0.7740\n",
      "Saving model at epoch 92 with loss 0.8468\n",
      "Epoch 93, Loss: 0.8425, Val Loss: 0.7699\n",
      "Saving model at epoch 93 with loss 0.8425\n",
      "Epoch 94, Loss: 0.8382, Val Loss: 0.7660\n",
      "Saving model at epoch 94 with loss 0.8382\n",
      "Epoch 95, Loss: 0.8340, Val Loss: 0.7622\n",
      "Saving model at epoch 95 with loss 0.8340\n",
      "Epoch 96, Loss: 0.8300, Val Loss: 0.7585\n",
      "Saving model at epoch 96 with loss 0.8300\n",
      "Epoch 97, Loss: 0.8260, Val Loss: 0.7548\n",
      "Saving model at epoch 97 with loss 0.8260\n",
      "Epoch 98, Loss: 0.8221, Val Loss: 0.7513\n",
      "Saving model at epoch 98 with loss 0.8221\n",
      "Epoch 99, Loss: 0.8183, Val Loss: 0.7479\n",
      "Saving model at epoch 99 with loss 0.8183\n",
      "Epoch 100, Loss: 0.8147, Val Loss: 0.7445\n",
      "Saving model at epoch 100 with loss 0.8147\n",
      "Epoch 101, Loss: 0.8111, Val Loss: 0.7413\n",
      "Saving model at epoch 101 with loss 0.8111\n",
      "Epoch 102, Loss: 0.8075, Val Loss: 0.7381\n",
      "Saving model at epoch 102 with loss 0.8075\n",
      "Epoch 103, Loss: 0.8040, Val Loss: 0.7350\n",
      "Saving model at epoch 103 with loss 0.8040\n",
      "Epoch 104, Loss: 0.8006, Val Loss: 0.7321\n",
      "Saving model at epoch 104 with loss 0.8006\n",
      "Epoch 105, Loss: 0.7973, Val Loss: 0.7291\n",
      "Saving model at epoch 105 with loss 0.7973\n",
      "Epoch 106, Loss: 0.7941, Val Loss: 0.7263\n",
      "Saving model at epoch 106 with loss 0.7941\n",
      "Epoch 107, Loss: 0.7909, Val Loss: 0.7235\n",
      "Saving model at epoch 107 with loss 0.7909\n",
      "Epoch 108, Loss: 0.7878, Val Loss: 0.7208\n",
      "Saving model at epoch 108 with loss 0.7878\n",
      "Epoch 109, Loss: 0.7847, Val Loss: 0.7181\n",
      "Saving model at epoch 109 with loss 0.7847\n",
      "Epoch 110, Loss: 0.7817, Val Loss: 0.7155\n",
      "Saving model at epoch 110 with loss 0.7817\n",
      "Epoch 111, Loss: 0.7788, Val Loss: 0.7129\n",
      "Saving model at epoch 111 with loss 0.7788\n",
      "Epoch 112, Loss: 0.7759, Val Loss: 0.7104\n",
      "Saving model at epoch 112 with loss 0.7759\n",
      "Epoch 113, Loss: 0.7731, Val Loss: 0.7080\n",
      "Saving model at epoch 113 with loss 0.7731\n",
      "Epoch 114, Loss: 0.7703, Val Loss: 0.7056\n",
      "Saving model at epoch 114 with loss 0.7703\n",
      "Epoch 115, Loss: 0.7676, Val Loss: 0.7032\n",
      "Saving model at epoch 115 with loss 0.7676\n",
      "Epoch 116, Loss: 0.7649, Val Loss: 0.7009\n",
      "Saving model at epoch 116 with loss 0.7649\n",
      "Epoch 117, Loss: 0.7623, Val Loss: 0.6986\n",
      "Saving model at epoch 117 with loss 0.7623\n",
      "Epoch 118, Loss: 0.7597, Val Loss: 0.6964\n",
      "Saving model at epoch 118 with loss 0.7597\n",
      "Epoch 119, Loss: 0.7572, Val Loss: 0.6942\n",
      "Saving model at epoch 119 with loss 0.7572\n",
      "Epoch 120, Loss: 0.7547, Val Loss: 0.6921\n",
      "Saving model at epoch 120 with loss 0.7547\n",
      "Epoch 121, Loss: 0.7523, Val Loss: 0.6900\n",
      "Saving model at epoch 121 with loss 0.7523\n",
      "Epoch 122, Loss: 0.7499, Val Loss: 0.6879\n",
      "Saving model at epoch 122 with loss 0.7499\n",
      "Epoch 123, Loss: 0.7475, Val Loss: 0.6859\n",
      "Saving model at epoch 123 with loss 0.7475\n",
      "Epoch 124, Loss: 0.7452, Val Loss: 0.6839\n",
      "Saving model at epoch 124 with loss 0.7452\n",
      "Epoch 125, Loss: 0.7430, Val Loss: 0.6820\n",
      "Saving model at epoch 125 with loss 0.7430\n",
      "Epoch 126, Loss: 0.7407, Val Loss: 0.6800\n",
      "Saving model at epoch 126 with loss 0.7407\n",
      "Epoch 127, Loss: 0.7385, Val Loss: 0.6781\n",
      "Saving model at epoch 127 with loss 0.7385\n",
      "Epoch 128, Loss: 0.7364, Val Loss: 0.6763\n",
      "Saving model at epoch 128 with loss 0.7364\n",
      "Epoch 129, Loss: 0.7342, Val Loss: 0.6744\n",
      "Saving model at epoch 129 with loss 0.7342\n",
      "Epoch 130, Loss: 0.7322, Val Loss: 0.6726\n",
      "Saving model at epoch 130 with loss 0.7322\n",
      "Epoch 131, Loss: 0.7301, Val Loss: 0.6709\n",
      "Saving model at epoch 131 with loss 0.7301\n",
      "Epoch 132, Loss: 0.7281, Val Loss: 0.6691\n",
      "Saving model at epoch 132 with loss 0.7281\n",
      "Epoch 133, Loss: 0.7261, Val Loss: 0.6674\n",
      "Saving model at epoch 133 with loss 0.7261\n",
      "Epoch 134, Loss: 0.7241, Val Loss: 0.6657\n",
      "Saving model at epoch 134 with loss 0.7241\n",
      "Epoch 135, Loss: 0.7222, Val Loss: 0.6641\n",
      "Saving model at epoch 135 with loss 0.7222\n",
      "Epoch 136, Loss: 0.7203, Val Loss: 0.6624\n",
      "Saving model at epoch 136 with loss 0.7203\n",
      "Epoch 137, Loss: 0.7184, Val Loss: 0.6608\n",
      "Saving model at epoch 137 with loss 0.7184\n",
      "Epoch 138, Loss: 0.7166, Val Loss: 0.6593\n",
      "Saving model at epoch 138 with loss 0.7166\n",
      "Epoch 139, Loss: 0.7148, Val Loss: 0.6577\n",
      "Saving model at epoch 139 with loss 0.7148\n",
      "Epoch 140, Loss: 0.7130, Val Loss: 0.6562\n",
      "Saving model at epoch 140 with loss 0.7130\n",
      "Epoch 141, Loss: 0.7112, Val Loss: 0.6546\n",
      "Saving model at epoch 141 with loss 0.7112\n",
      "Epoch 142, Loss: 0.7095, Val Loss: 0.6532\n",
      "Saving model at epoch 142 with loss 0.7095\n",
      "Epoch 143, Loss: 0.7078, Val Loss: 0.6517\n",
      "Saving model at epoch 143 with loss 0.7078\n",
      "Epoch 144, Loss: 0.7061, Val Loss: 0.6502\n",
      "Saving model at epoch 144 with loss 0.7061\n",
      "Epoch 145, Loss: 0.7045, Val Loss: 0.6488\n",
      "Saving model at epoch 145 with loss 0.7045\n",
      "Epoch 146, Loss: 0.7028, Val Loss: 0.6474\n",
      "Saving model at epoch 146 with loss 0.7028\n",
      "Epoch 147, Loss: 0.7012, Val Loss: 0.6460\n",
      "Saving model at epoch 147 with loss 0.7012\n",
      "Epoch 148, Loss: 0.6997, Val Loss: 0.6447\n",
      "Saving model at epoch 148 with loss 0.6997\n",
      "Epoch 149, Loss: 0.6981, Val Loss: 0.6433\n",
      "Saving model at epoch 149 with loss 0.6981\n",
      "Epoch 150, Loss: 0.6966, Val Loss: 0.6420\n",
      "Saving model at epoch 150 with loss 0.6966\n",
      "Epoch 151, Loss: 0.6950, Val Loss: 0.6407\n",
      "Saving model at epoch 151 with loss 0.6950\n",
      "Epoch 152, Loss: 0.6935, Val Loss: 0.6394\n",
      "Saving model at epoch 152 with loss 0.6935\n",
      "Epoch 153, Loss: 0.6921, Val Loss: 0.6381\n",
      "Saving model at epoch 153 with loss 0.6921\n",
      "Epoch 154, Loss: 0.6906, Val Loss: 0.6368\n",
      "Saving model at epoch 154 with loss 0.6906\n",
      "Epoch 155, Loss: 0.6892, Val Loss: 0.6356\n",
      "Saving model at epoch 155 with loss 0.6892\n",
      "Epoch 156, Loss: 0.6877, Val Loss: 0.6344\n",
      "Saving model at epoch 156 with loss 0.6877\n",
      "Epoch 157, Loss: 0.6863, Val Loss: 0.6332\n",
      "Saving model at epoch 157 with loss 0.6863\n",
      "Epoch 158, Loss: 0.6849, Val Loss: 0.6319\n",
      "Saving model at epoch 158 with loss 0.6849\n",
      "Epoch 159, Loss: 0.6836, Val Loss: 0.6308\n",
      "Saving model at epoch 159 with loss 0.6836\n",
      "Epoch 160, Loss: 0.6822, Val Loss: 0.6296\n",
      "Saving model at epoch 160 with loss 0.6822\n",
      "Epoch 161, Loss: 0.6809, Val Loss: 0.6284\n",
      "Saving model at epoch 161 with loss 0.6809\n",
      "Epoch 162, Loss: 0.6796, Val Loss: 0.6273\n",
      "Saving model at epoch 162 with loss 0.6796\n",
      "Epoch 163, Loss: 0.6782, Val Loss: 0.6262\n",
      "Saving model at epoch 163 with loss 0.6782\n",
      "Epoch 164, Loss: 0.6770, Val Loss: 0.6250\n",
      "Saving model at epoch 164 with loss 0.6770\n",
      "Epoch 165, Loss: 0.6757, Val Loss: 0.6239\n",
      "Saving model at epoch 165 with loss 0.6757\n",
      "Epoch 166, Loss: 0.6744, Val Loss: 0.6228\n",
      "Saving model at epoch 166 with loss 0.6744\n",
      "Epoch 167, Loss: 0.6732, Val Loss: 0.6218\n",
      "Saving model at epoch 167 with loss 0.6732\n",
      "Epoch 168, Loss: 0.6720, Val Loss: 0.6207\n",
      "Saving model at epoch 168 with loss 0.6720\n",
      "Epoch 169, Loss: 0.6707, Val Loss: 0.6196\n",
      "Saving model at epoch 169 with loss 0.6707\n",
      "Epoch 170, Loss: 0.6695, Val Loss: 0.6186\n",
      "Saving model at epoch 170 with loss 0.6695\n",
      "Epoch 171, Loss: 0.6684, Val Loss: 0.6176\n",
      "Saving model at epoch 171 with loss 0.6684\n",
      "Epoch 172, Loss: 0.6672, Val Loss: 0.6166\n",
      "Saving model at epoch 172 with loss 0.6672\n",
      "Epoch 173, Loss: 0.6660, Val Loss: 0.6156\n",
      "Saving model at epoch 173 with loss 0.6660\n",
      "Epoch 174, Loss: 0.6649, Val Loss: 0.6145\n",
      "Saving model at epoch 174 with loss 0.6649\n",
      "Epoch 175, Loss: 0.6637, Val Loss: 0.6136\n",
      "Saving model at epoch 175 with loss 0.6637\n",
      "Epoch 176, Loss: 0.6626, Val Loss: 0.6126\n",
      "Saving model at epoch 176 with loss 0.6626\n",
      "Epoch 177, Loss: 0.6615, Val Loss: 0.6116\n",
      "Saving model at epoch 177 with loss 0.6615\n",
      "Epoch 178, Loss: 0.6604, Val Loss: 0.6107\n",
      "Saving model at epoch 178 with loss 0.6604\n",
      "Epoch 179, Loss: 0.6593, Val Loss: 0.6097\n",
      "Saving model at epoch 179 with loss 0.6593\n",
      "Epoch 180, Loss: 0.6583, Val Loss: 0.6088\n",
      "Saving model at epoch 180 with loss 0.6583\n",
      "Epoch 181, Loss: 0.6572, Val Loss: 0.6078\n",
      "Saving model at epoch 181 with loss 0.6572\n",
      "Epoch 182, Loss: 0.6561, Val Loss: 0.6069\n",
      "Saving model at epoch 182 with loss 0.6561\n",
      "Epoch 183, Loss: 0.6551, Val Loss: 0.6060\n",
      "Saving model at epoch 183 with loss 0.6551\n",
      "Epoch 184, Loss: 0.6541, Val Loss: 0.6051\n",
      "Saving model at epoch 184 with loss 0.6541\n",
      "Epoch 185, Loss: 0.6531, Val Loss: 0.6042\n",
      "Saving model at epoch 185 with loss 0.6531\n",
      "Epoch 186, Loss: 0.6521, Val Loss: 0.6034\n",
      "Saving model at epoch 186 with loss 0.6521\n",
      "Epoch 187, Loss: 0.6511, Val Loss: 0.6025\n",
      "Saving model at epoch 187 with loss 0.6511\n",
      "Epoch 188, Loss: 0.6501, Val Loss: 0.6016\n",
      "Saving model at epoch 188 with loss 0.6501\n",
      "Epoch 189, Loss: 0.6491, Val Loss: 0.6008\n",
      "Saving model at epoch 189 with loss 0.6491\n",
      "Epoch 190, Loss: 0.6481, Val Loss: 0.5999\n",
      "Saving model at epoch 190 with loss 0.6481\n",
      "Epoch 191, Loss: 0.6472, Val Loss: 0.5991\n",
      "Saving model at epoch 191 with loss 0.6472\n",
      "Epoch 192, Loss: 0.6462, Val Loss: 0.5982\n",
      "Saving model at epoch 192 with loss 0.6462\n",
      "Epoch 193, Loss: 0.6453, Val Loss: 0.5974\n",
      "Saving model at epoch 193 with loss 0.6453\n",
      "Epoch 194, Loss: 0.6444, Val Loss: 0.5966\n",
      "Saving model at epoch 194 with loss 0.6444\n",
      "Epoch 195, Loss: 0.6434, Val Loss: 0.5958\n",
      "Saving model at epoch 195 with loss 0.6434\n",
      "Epoch 196, Loss: 0.6425, Val Loss: 0.5950\n",
      "Saving model at epoch 196 with loss 0.6425\n",
      "Epoch 197, Loss: 0.6416, Val Loss: 0.5942\n",
      "Saving model at epoch 197 with loss 0.6416\n",
      "Epoch 198, Loss: 0.6407, Val Loss: 0.5934\n",
      "Saving model at epoch 198 with loss 0.6407\n",
      "Epoch 199, Loss: 0.6398, Val Loss: 0.5926\n",
      "Saving model at epoch 199 with loss 0.6398\n",
      "Epoch 200, Loss: 0.6390, Val Loss: 0.5918\n",
      "Saving model at epoch 200 with loss 0.6390\n",
      "Epoch 201, Loss: 0.6381, Val Loss: 0.5911\n",
      "Saving model at epoch 201 with loss 0.6381\n",
      "Epoch 202, Loss: 0.6372, Val Loss: 0.5903\n",
      "Saving model at epoch 202 with loss 0.6372\n",
      "Epoch 203, Loss: 0.6364, Val Loss: 0.5895\n",
      "Saving model at epoch 203 with loss 0.6364\n",
      "Epoch 204, Loss: 0.6355, Val Loss: 0.5888\n",
      "Saving model at epoch 204 with loss 0.6355\n",
      "Epoch 205, Loss: 0.6347, Val Loss: 0.5880\n",
      "Saving model at epoch 205 with loss 0.6347\n",
      "Epoch 206, Loss: 0.6338, Val Loss: 0.5873\n",
      "Saving model at epoch 206 with loss 0.6338\n",
      "Epoch 207, Loss: 0.6330, Val Loss: 0.5866\n",
      "Saving model at epoch 207 with loss 0.6330\n",
      "Epoch 208, Loss: 0.6322, Val Loss: 0.5858\n",
      "Saving model at epoch 208 with loss 0.6322\n",
      "Epoch 209, Loss: 0.6314, Val Loss: 0.5851\n",
      "Saving model at epoch 209 with loss 0.6314\n",
      "Epoch 210, Loss: 0.6306, Val Loss: 0.5844\n",
      "Saving model at epoch 210 with loss 0.6306\n",
      "Epoch 211, Loss: 0.6298, Val Loss: 0.5837\n",
      "Saving model at epoch 211 with loss 0.6298\n",
      "Epoch 212, Loss: 0.6290, Val Loss: 0.5830\n",
      "Saving model at epoch 212 with loss 0.6290\n",
      "Epoch 213, Loss: 0.6282, Val Loss: 0.5823\n",
      "Saving model at epoch 213 with loss 0.6282\n",
      "Epoch 214, Loss: 0.6274, Val Loss: 0.5816\n",
      "Saving model at epoch 214 with loss 0.6274\n",
      "Epoch 215, Loss: 0.6266, Val Loss: 0.5809\n",
      "Saving model at epoch 215 with loss 0.6266\n",
      "Epoch 216, Loss: 0.6259, Val Loss: 0.5802\n",
      "Saving model at epoch 216 with loss 0.6259\n",
      "Epoch 217, Loss: 0.6251, Val Loss: 0.5795\n",
      "Saving model at epoch 217 with loss 0.6251\n",
      "Epoch 218, Loss: 0.6244, Val Loss: 0.5789\n",
      "Saving model at epoch 218 with loss 0.6244\n",
      "Epoch 219, Loss: 0.6236, Val Loss: 0.5782\n",
      "Saving model at epoch 219 with loss 0.6236\n",
      "Epoch 220, Loss: 0.6229, Val Loss: 0.5775\n",
      "Saving model at epoch 220 with loss 0.6229\n",
      "Epoch 221, Loss: 0.6221, Val Loss: 0.5769\n",
      "Saving model at epoch 221 with loss 0.6221\n",
      "Epoch 222, Loss: 0.6214, Val Loss: 0.5762\n",
      "Saving model at epoch 222 with loss 0.6214\n",
      "Epoch 223, Loss: 0.6207, Val Loss: 0.5755\n",
      "Saving model at epoch 223 with loss 0.6207\n",
      "Epoch 224, Loss: 0.6199, Val Loss: 0.5749\n",
      "Saving model at epoch 224 with loss 0.6199\n",
      "Epoch 225, Loss: 0.6192, Val Loss: 0.5742\n",
      "Saving model at epoch 225 with loss 0.6192\n",
      "Epoch 226, Loss: 0.6185, Val Loss: 0.5736\n",
      "Saving model at epoch 226 with loss 0.6185\n",
      "Epoch 227, Loss: 0.6178, Val Loss: 0.5730\n",
      "Saving model at epoch 227 with loss 0.6178\n",
      "Epoch 228, Loss: 0.6171, Val Loss: 0.5723\n",
      "Saving model at epoch 228 with loss 0.6171\n",
      "Epoch 229, Loss: 0.6164, Val Loss: 0.5717\n",
      "Saving model at epoch 229 with loss 0.6164\n",
      "Epoch 230, Loss: 0.6157, Val Loss: 0.5711\n",
      "Saving model at epoch 230 with loss 0.6157\n",
      "Epoch 231, Loss: 0.6150, Val Loss: 0.5704\n",
      "Saving model at epoch 231 with loss 0.6150\n",
      "Epoch 232, Loss: 0.6143, Val Loss: 0.5698\n",
      "Saving model at epoch 232 with loss 0.6143\n",
      "Epoch 233, Loss: 0.6137, Val Loss: 0.5692\n",
      "Saving model at epoch 233 with loss 0.6137\n",
      "Epoch 234, Loss: 0.6130, Val Loss: 0.5686\n",
      "Saving model at epoch 234 with loss 0.6130\n",
      "Epoch 235, Loss: 0.6123, Val Loss: 0.5680\n",
      "Saving model at epoch 235 with loss 0.6123\n",
      "Epoch 236, Loss: 0.6117, Val Loss: 0.5674\n",
      "Saving model at epoch 236 with loss 0.6117\n",
      "Epoch 237, Loss: 0.6110, Val Loss: 0.5668\n",
      "Saving model at epoch 237 with loss 0.6110\n",
      "Epoch 238, Loss: 0.6103, Val Loss: 0.5662\n",
      "Saving model at epoch 238 with loss 0.6103\n",
      "Epoch 239, Loss: 0.6097, Val Loss: 0.5656\n",
      "Saving model at epoch 239 with loss 0.6097\n",
      "Epoch 240, Loss: 0.6090, Val Loss: 0.5650\n",
      "Saving model at epoch 240 with loss 0.6090\n",
      "Epoch 241, Loss: 0.6084, Val Loss: 0.5644\n",
      "Saving model at epoch 241 with loss 0.6084\n",
      "Epoch 242, Loss: 0.6077, Val Loss: 0.5638\n",
      "Saving model at epoch 242 with loss 0.6077\n",
      "Epoch 243, Loss: 0.6071, Val Loss: 0.5632\n",
      "Saving model at epoch 243 with loss 0.6071\n",
      "Epoch 244, Loss: 0.6064, Val Loss: 0.5626\n",
      "Saving model at epoch 244 with loss 0.6064\n",
      "Epoch 245, Loss: 0.6058, Val Loss: 0.5621\n",
      "Saving model at epoch 245 with loss 0.6058\n",
      "Epoch 246, Loss: 0.6052, Val Loss: 0.5615\n",
      "Saving model at epoch 246 with loss 0.6052\n",
      "Epoch 247, Loss: 0.6046, Val Loss: 0.5609\n",
      "Saving model at epoch 247 with loss 0.6046\n",
      "Epoch 248, Loss: 0.6039, Val Loss: 0.5604\n",
      "Saving model at epoch 248 with loss 0.6039\n",
      "Epoch 249, Loss: 0.6034, Val Loss: 0.5598\n",
      "Saving model at epoch 249 with loss 0.6034\n",
      "Epoch 250, Loss: 0.6027, Val Loss: 0.5592\n",
      "Saving model at epoch 250 with loss 0.6027\n",
      "Epoch 251, Loss: 0.6022, Val Loss: 0.5587\n",
      "Saving model at epoch 251 with loss 0.6022\n",
      "Epoch 252, Loss: 0.6015, Val Loss: 0.5581\n",
      "Saving model at epoch 252 with loss 0.6015\n",
      "Epoch 253, Loss: 0.6010, Val Loss: 0.5576\n",
      "Saving model at epoch 253 with loss 0.6010\n",
      "Epoch 254, Loss: 0.6003, Val Loss: 0.5570\n",
      "Saving model at epoch 254 with loss 0.6003\n",
      "Epoch 255, Loss: 0.5998, Val Loss: 0.5564\n",
      "Saving model at epoch 255 with loss 0.5998\n",
      "Epoch 256, Loss: 0.5991, Val Loss: 0.5559\n",
      "Saving model at epoch 256 with loss 0.5991\n",
      "Epoch 257, Loss: 0.5986, Val Loss: 0.5554\n",
      "Saving model at epoch 257 with loss 0.5986\n",
      "Epoch 258, Loss: 0.5980, Val Loss: 0.5548\n",
      "Saving model at epoch 258 with loss 0.5980\n",
      "Epoch 259, Loss: 0.5974, Val Loss: 0.5543\n",
      "Saving model at epoch 259 with loss 0.5974\n",
      "Epoch 260, Loss: 0.5968, Val Loss: 0.5537\n",
      "Saving model at epoch 260 with loss 0.5968\n",
      "Epoch 261, Loss: 0.5963, Val Loss: 0.5532\n",
      "Saving model at epoch 261 with loss 0.5963\n",
      "Epoch 262, Loss: 0.5956, Val Loss: 0.5526\n",
      "Saving model at epoch 262 with loss 0.5956\n",
      "Epoch 263, Loss: 0.5951, Val Loss: 0.5521\n",
      "Saving model at epoch 263 with loss 0.5951\n",
      "Epoch 264, Loss: 0.5945, Val Loss: 0.5516\n",
      "Saving model at epoch 264 with loss 0.5945\n",
      "Epoch 265, Loss: 0.5939, Val Loss: 0.5510\n",
      "Saving model at epoch 265 with loss 0.5939\n",
      "Epoch 266, Loss: 0.5933, Val Loss: 0.5505\n",
      "Saving model at epoch 266 with loss 0.5933\n",
      "Epoch 267, Loss: 0.5928, Val Loss: 0.5499\n",
      "Saving model at epoch 267 with loss 0.5928\n",
      "Epoch 268, Loss: 0.5922, Val Loss: 0.5494\n",
      "Saving model at epoch 268 with loss 0.5922\n",
      "Epoch 269, Loss: 0.5916, Val Loss: 0.5489\n",
      "Saving model at epoch 269 with loss 0.5916\n",
      "Epoch 270, Loss: 0.5911, Val Loss: 0.5483\n",
      "Saving model at epoch 270 with loss 0.5911\n",
      "Epoch 271, Loss: 0.5905, Val Loss: 0.5478\n",
      "Saving model at epoch 271 with loss 0.5905\n",
      "Epoch 272, Loss: 0.5899, Val Loss: 0.5473\n",
      "Saving model at epoch 272 with loss 0.5899\n",
      "Epoch 273, Loss: 0.5894, Val Loss: 0.5467\n",
      "Saving model at epoch 273 with loss 0.5894\n",
      "Epoch 274, Loss: 0.5888, Val Loss: 0.5462\n",
      "Saving model at epoch 274 with loss 0.5888\n",
      "Epoch 275, Loss: 0.5883, Val Loss: 0.5457\n",
      "Saving model at epoch 275 with loss 0.5883\n",
      "Epoch 276, Loss: 0.5877, Val Loss: 0.5451\n",
      "Saving model at epoch 276 with loss 0.5877\n",
      "Epoch 277, Loss: 0.5871, Val Loss: 0.5446\n",
      "Saving model at epoch 277 with loss 0.5871\n",
      "Epoch 278, Loss: 0.5866, Val Loss: 0.5441\n",
      "Saving model at epoch 278 with loss 0.5866\n",
      "Epoch 279, Loss: 0.5860, Val Loss: 0.5435\n",
      "Saving model at epoch 279 with loss 0.5860\n",
      "Epoch 280, Loss: 0.5854, Val Loss: 0.5430\n",
      "Saving model at epoch 280 with loss 0.5854\n",
      "Epoch 281, Loss: 0.5849, Val Loss: 0.5425\n",
      "Saving model at epoch 281 with loss 0.5849\n",
      "Epoch 282, Loss: 0.5843, Val Loss: 0.5420\n",
      "Saving model at epoch 282 with loss 0.5843\n",
      "Epoch 283, Loss: 0.5838, Val Loss: 0.5414\n",
      "Saving model at epoch 283 with loss 0.5838\n",
      "Epoch 284, Loss: 0.5832, Val Loss: 0.5409\n",
      "Saving model at epoch 284 with loss 0.5832\n",
      "Epoch 285, Loss: 0.5827, Val Loss: 0.5404\n",
      "Saving model at epoch 285 with loss 0.5827\n",
      "Epoch 286, Loss: 0.5821, Val Loss: 0.5399\n",
      "Saving model at epoch 286 with loss 0.5821\n",
      "Epoch 287, Loss: 0.5816, Val Loss: 0.5393\n",
      "Saving model at epoch 287 with loss 0.5816\n",
      "Epoch 288, Loss: 0.5810, Val Loss: 0.5388\n",
      "Saving model at epoch 288 with loss 0.5810\n",
      "Epoch 289, Loss: 0.5804, Val Loss: 0.5383\n",
      "Saving model at epoch 289 with loss 0.5804\n",
      "Epoch 290, Loss: 0.5799, Val Loss: 0.5378\n",
      "Saving model at epoch 290 with loss 0.5799\n",
      "Epoch 291, Loss: 0.5794, Val Loss: 0.5372\n",
      "Saving model at epoch 291 with loss 0.5794\n",
      "Epoch 292, Loss: 0.5788, Val Loss: 0.5367\n",
      "Saving model at epoch 292 with loss 0.5788\n",
      "Epoch 293, Loss: 0.5783, Val Loss: 0.5362\n",
      "Saving model at epoch 293 with loss 0.5783\n",
      "Epoch 294, Loss: 0.5777, Val Loss: 0.5357\n",
      "Saving model at epoch 294 with loss 0.5777\n",
      "Epoch 295, Loss: 0.5772, Val Loss: 0.5351\n",
      "Saving model at epoch 295 with loss 0.5772\n",
      "Epoch 296, Loss: 0.5767, Val Loss: 0.5346\n",
      "Saving model at epoch 296 with loss 0.5767\n",
      "Epoch 297, Loss: 0.5761, Val Loss: 0.5341\n",
      "Saving model at epoch 297 with loss 0.5761\n",
      "Epoch 298, Loss: 0.5756, Val Loss: 0.5336\n",
      "Saving model at epoch 298 with loss 0.5756\n",
      "Epoch 299, Loss: 0.5751, Val Loss: 0.5331\n",
      "Saving model at epoch 299 with loss 0.5751\n",
      "Epoch 300, Loss: 0.5745, Val Loss: 0.5326\n",
      "Saving model at epoch 300 with loss 0.5745\n",
      "Epoch 301, Loss: 0.5739, Val Loss: 0.5320\n",
      "Saving model at epoch 301 with loss 0.5739\n",
      "Epoch 302, Loss: 0.5734, Val Loss: 0.5315\n",
      "Saving model at epoch 302 with loss 0.5734\n",
      "Epoch 303, Loss: 0.5729, Val Loss: 0.5310\n",
      "Saving model at epoch 303 with loss 0.5729\n",
      "Epoch 304, Loss: 0.5724, Val Loss: 0.5305\n",
      "Saving model at epoch 304 with loss 0.5724\n",
      "Epoch 305, Loss: 0.5719, Val Loss: 0.5300\n",
      "Saving model at epoch 305 with loss 0.5719\n",
      "Epoch 306, Loss: 0.5713, Val Loss: 0.5295\n",
      "Saving model at epoch 306 with loss 0.5713\n",
      "Epoch 307, Loss: 0.5708, Val Loss: 0.5290\n",
      "Saving model at epoch 307 with loss 0.5708\n",
      "Epoch 308, Loss: 0.5703, Val Loss: 0.5285\n",
      "Saving model at epoch 308 with loss 0.5703\n",
      "Epoch 309, Loss: 0.5697, Val Loss: 0.5280\n",
      "Saving model at epoch 309 with loss 0.5697\n",
      "Epoch 310, Loss: 0.5692, Val Loss: 0.5275\n",
      "Saving model at epoch 310 with loss 0.5692\n",
      "Epoch 311, Loss: 0.5687, Val Loss: 0.5269\n",
      "Saving model at epoch 311 with loss 0.5687\n",
      "Epoch 312, Loss: 0.5681, Val Loss: 0.5264\n",
      "Saving model at epoch 312 with loss 0.5681\n",
      "Epoch 313, Loss: 0.5676, Val Loss: 0.5259\n",
      "Saving model at epoch 313 with loss 0.5676\n",
      "Epoch 314, Loss: 0.5671, Val Loss: 0.5254\n",
      "Saving model at epoch 314 with loss 0.5671\n",
      "Epoch 315, Loss: 0.5666, Val Loss: 0.5249\n",
      "Saving model at epoch 315 with loss 0.5666\n",
      "Epoch 316, Loss: 0.5660, Val Loss: 0.5244\n",
      "Saving model at epoch 316 with loss 0.5660\n",
      "Epoch 317, Loss: 0.5655, Val Loss: 0.5239\n",
      "Saving model at epoch 317 with loss 0.5655\n",
      "Epoch 318, Loss: 0.5650, Val Loss: 0.5234\n",
      "Saving model at epoch 318 with loss 0.5650\n",
      "Epoch 319, Loss: 0.5645, Val Loss: 0.5229\n",
      "Saving model at epoch 319 with loss 0.5645\n",
      "Epoch 320, Loss: 0.5640, Val Loss: 0.5224\n",
      "Saving model at epoch 320 with loss 0.5640\n",
      "Epoch 321, Loss: 0.5635, Val Loss: 0.5219\n",
      "Saving model at epoch 321 with loss 0.5635\n",
      "Epoch 322, Loss: 0.5630, Val Loss: 0.5214\n",
      "Saving model at epoch 322 with loss 0.5630\n",
      "Epoch 323, Loss: 0.5625, Val Loss: 0.5209\n",
      "Saving model at epoch 323 with loss 0.5625\n",
      "Epoch 324, Loss: 0.5620, Val Loss: 0.5204\n",
      "Saving model at epoch 324 with loss 0.5620\n",
      "Epoch 325, Loss: 0.5614, Val Loss: 0.5199\n",
      "Saving model at epoch 325 with loss 0.5614\n",
      "Epoch 326, Loss: 0.5609, Val Loss: 0.5195\n",
      "Saving model at epoch 326 with loss 0.5609\n",
      "Epoch 327, Loss: 0.5604, Val Loss: 0.5190\n",
      "Saving model at epoch 327 with loss 0.5604\n",
      "Epoch 328, Loss: 0.5599, Val Loss: 0.5185\n",
      "Saving model at epoch 328 with loss 0.5599\n",
      "Epoch 329, Loss: 0.5594, Val Loss: 0.5180\n",
      "Saving model at epoch 329 with loss 0.5594\n",
      "Epoch 330, Loss: 0.5589, Val Loss: 0.5175\n",
      "Saving model at epoch 330 with loss 0.5589\n",
      "Epoch 331, Loss: 0.5584, Val Loss: 0.5170\n",
      "Saving model at epoch 331 with loss 0.5584\n",
      "Epoch 332, Loss: 0.5579, Val Loss: 0.5166\n",
      "Saving model at epoch 332 with loss 0.5579\n",
      "Epoch 333, Loss: 0.5574, Val Loss: 0.5161\n",
      "Saving model at epoch 333 with loss 0.5574\n",
      "Epoch 334, Loss: 0.5569, Val Loss: 0.5156\n",
      "Saving model at epoch 334 with loss 0.5569\n",
      "Epoch 335, Loss: 0.5564, Val Loss: 0.5151\n",
      "Saving model at epoch 335 with loss 0.5564\n",
      "Epoch 336, Loss: 0.5560, Val Loss: 0.5146\n",
      "Saving model at epoch 336 with loss 0.5560\n",
      "Epoch 337, Loss: 0.5555, Val Loss: 0.5142\n",
      "Saving model at epoch 337 with loss 0.5555\n",
      "Epoch 338, Loss: 0.5550, Val Loss: 0.5137\n",
      "Saving model at epoch 338 with loss 0.5550\n",
      "Epoch 339, Loss: 0.5545, Val Loss: 0.5132\n",
      "Saving model at epoch 339 with loss 0.5545\n",
      "Epoch 340, Loss: 0.5540, Val Loss: 0.5128\n",
      "Saving model at epoch 340 with loss 0.5540\n",
      "Epoch 341, Loss: 0.5535, Val Loss: 0.5123\n",
      "Saving model at epoch 341 with loss 0.5535\n",
      "Epoch 342, Loss: 0.5531, Val Loss: 0.5118\n",
      "Saving model at epoch 342 with loss 0.5531\n",
      "Epoch 343, Loss: 0.5526, Val Loss: 0.5114\n",
      "Saving model at epoch 343 with loss 0.5526\n",
      "Epoch 344, Loss: 0.5521, Val Loss: 0.5109\n",
      "Saving model at epoch 344 with loss 0.5521\n",
      "Epoch 345, Loss: 0.5516, Val Loss: 0.5105\n",
      "Saving model at epoch 345 with loss 0.5516\n",
      "Epoch 346, Loss: 0.5512, Val Loss: 0.5100\n",
      "Saving model at epoch 346 with loss 0.5512\n",
      "Epoch 347, Loss: 0.5507, Val Loss: 0.5095\n",
      "Saving model at epoch 347 with loss 0.5507\n",
      "Epoch 348, Loss: 0.5502, Val Loss: 0.5091\n",
      "Saving model at epoch 348 with loss 0.5502\n",
      "Epoch 349, Loss: 0.5497, Val Loss: 0.5086\n",
      "Saving model at epoch 349 with loss 0.5497\n",
      "Epoch 350, Loss: 0.5493, Val Loss: 0.5082\n",
      "Saving model at epoch 350 with loss 0.5493\n",
      "Epoch 351, Loss: 0.5488, Val Loss: 0.5077\n",
      "Saving model at epoch 351 with loss 0.5488\n",
      "Epoch 352, Loss: 0.5484, Val Loss: 0.5073\n",
      "Saving model at epoch 352 with loss 0.5484\n",
      "Epoch 353, Loss: 0.5480, Val Loss: 0.5069\n",
      "Saving model at epoch 353 with loss 0.5480\n",
      "Epoch 354, Loss: 0.5475, Val Loss: 0.5064\n",
      "Saving model at epoch 354 with loss 0.5475\n",
      "Epoch 355, Loss: 0.5470, Val Loss: 0.5060\n",
      "Saving model at epoch 355 with loss 0.5470\n",
      "Epoch 356, Loss: 0.5466, Val Loss: 0.5056\n",
      "Saving model at epoch 356 with loss 0.5466\n",
      "Epoch 357, Loss: 0.5462, Val Loss: 0.5051\n",
      "Saving model at epoch 357 with loss 0.5462\n",
      "Epoch 358, Loss: 0.5457, Val Loss: 0.5047\n",
      "Saving model at epoch 358 with loss 0.5457\n",
      "Epoch 359, Loss: 0.5452, Val Loss: 0.5043\n",
      "Saving model at epoch 359 with loss 0.5452\n",
      "Epoch 360, Loss: 0.5448, Val Loss: 0.5038\n",
      "Saving model at epoch 360 with loss 0.5448\n",
      "Epoch 361, Loss: 0.5444, Val Loss: 0.5034\n",
      "Saving model at epoch 361 with loss 0.5444\n",
      "Epoch 362, Loss: 0.5440, Val Loss: 0.5030\n",
      "Saving model at epoch 362 with loss 0.5440\n",
      "Epoch 363, Loss: 0.5435, Val Loss: 0.5026\n",
      "Saving model at epoch 363 with loss 0.5435\n",
      "Epoch 364, Loss: 0.5431, Val Loss: 0.5022\n",
      "Saving model at epoch 364 with loss 0.5431\n",
      "Epoch 365, Loss: 0.5427, Val Loss: 0.5017\n",
      "Saving model at epoch 365 with loss 0.5427\n",
      "Epoch 366, Loss: 0.5422, Val Loss: 0.5013\n",
      "Saving model at epoch 366 with loss 0.5422\n",
      "Epoch 367, Loss: 0.5418, Val Loss: 0.5009\n",
      "Saving model at epoch 367 with loss 0.5418\n",
      "Epoch 368, Loss: 0.5414, Val Loss: 0.5005\n",
      "Saving model at epoch 368 with loss 0.5414\n",
      "Epoch 369, Loss: 0.5410, Val Loss: 0.5001\n",
      "Saving model at epoch 369 with loss 0.5410\n",
      "Epoch 370, Loss: 0.5406, Val Loss: 0.4997\n",
      "Saving model at epoch 370 with loss 0.5406\n",
      "Epoch 371, Loss: 0.5402, Val Loss: 0.4993\n",
      "Saving model at epoch 371 with loss 0.5402\n",
      "Epoch 372, Loss: 0.5398, Val Loss: 0.4989\n",
      "Saving model at epoch 372 with loss 0.5398\n",
      "Epoch 373, Loss: 0.5394, Val Loss: 0.4985\n",
      "Saving model at epoch 373 with loss 0.5394\n",
      "Epoch 374, Loss: 0.5390, Val Loss: 0.4981\n",
      "Saving model at epoch 374 with loss 0.5390\n",
      "Epoch 375, Loss: 0.5386, Val Loss: 0.4977\n",
      "Saving model at epoch 375 with loss 0.5386\n",
      "Epoch 376, Loss: 0.5382, Val Loss: 0.4974\n",
      "Saving model at epoch 376 with loss 0.5382\n",
      "Epoch 377, Loss: 0.5378, Val Loss: 0.4970\n",
      "Saving model at epoch 377 with loss 0.5378\n",
      "Epoch 378, Loss: 0.5374, Val Loss: 0.4966\n",
      "Saving model at epoch 378 with loss 0.5374\n",
      "Epoch 379, Loss: 0.5370, Val Loss: 0.4962\n",
      "Saving model at epoch 379 with loss 0.5370\n",
      "Epoch 380, Loss: 0.5366, Val Loss: 0.4958\n",
      "Saving model at epoch 380 with loss 0.5366\n",
      "Epoch 381, Loss: 0.5362, Val Loss: 0.4955\n",
      "Saving model at epoch 381 with loss 0.5362\n",
      "Epoch 382, Loss: 0.5359, Val Loss: 0.4951\n",
      "Saving model at epoch 382 with loss 0.5359\n",
      "Epoch 383, Loss: 0.5355, Val Loss: 0.4947\n",
      "Saving model at epoch 383 with loss 0.5355\n",
      "Epoch 384, Loss: 0.5351, Val Loss: 0.4944\n",
      "Saving model at epoch 384 with loss 0.5351\n",
      "Epoch 385, Loss: 0.5347, Val Loss: 0.4940\n",
      "Saving model at epoch 385 with loss 0.5347\n",
      "Epoch 386, Loss: 0.5343, Val Loss: 0.4936\n",
      "Saving model at epoch 386 with loss 0.5343\n",
      "Epoch 387, Loss: 0.5340, Val Loss: 0.4933\n",
      "Saving model at epoch 387 with loss 0.5340\n",
      "Epoch 388, Loss: 0.5336, Val Loss: 0.4929\n",
      "Saving model at epoch 388 with loss 0.5336\n",
      "Epoch 389, Loss: 0.5332, Val Loss: 0.4926\n",
      "Saving model at epoch 389 with loss 0.5332\n",
      "Epoch 390, Loss: 0.5329, Val Loss: 0.4922\n",
      "Saving model at epoch 390 with loss 0.5329\n",
      "Epoch 391, Loss: 0.5325, Val Loss: 0.4919\n",
      "Saving model at epoch 391 with loss 0.5325\n",
      "Epoch 392, Loss: 0.5322, Val Loss: 0.4915\n",
      "Saving model at epoch 392 with loss 0.5322\n",
      "Epoch 393, Loss: 0.5318, Val Loss: 0.4912\n",
      "Saving model at epoch 393 with loss 0.5318\n",
      "Epoch 394, Loss: 0.5314, Val Loss: 0.4908\n",
      "Saving model at epoch 394 with loss 0.5314\n",
      "Epoch 395, Loss: 0.5311, Val Loss: 0.4905\n",
      "Saving model at epoch 395 with loss 0.5311\n",
      "Epoch 396, Loss: 0.5307, Val Loss: 0.4901\n",
      "Saving model at epoch 396 with loss 0.5307\n",
      "Epoch 397, Loss: 0.5304, Val Loss: 0.4898\n",
      "Saving model at epoch 397 with loss 0.5304\n",
      "Epoch 398, Loss: 0.5300, Val Loss: 0.4895\n",
      "Saving model at epoch 398 with loss 0.5300\n",
      "Epoch 399, Loss: 0.5297, Val Loss: 0.4891\n",
      "Saving model at epoch 399 with loss 0.5297\n",
      "Epoch 400, Loss: 0.5293, Val Loss: 0.4888\n",
      "Saving model at epoch 400 with loss 0.5293\n",
      "Epoch 401, Loss: 0.5290, Val Loss: 0.4885\n",
      "Saving model at epoch 401 with loss 0.5290\n",
      "Epoch 402, Loss: 0.5287, Val Loss: 0.4881\n",
      "Saving model at epoch 402 with loss 0.5287\n",
      "Epoch 403, Loss: 0.5283, Val Loss: 0.4878\n",
      "Saving model at epoch 403 with loss 0.5283\n",
      "Epoch 404, Loss: 0.5280, Val Loss: 0.4875\n",
      "Saving model at epoch 404 with loss 0.5280\n",
      "Epoch 405, Loss: 0.5277, Val Loss: 0.4871\n",
      "Saving model at epoch 405 with loss 0.5277\n",
      "Epoch 406, Loss: 0.5273, Val Loss: 0.4868\n",
      "Saving model at epoch 406 with loss 0.5273\n",
      "Epoch 407, Loss: 0.5270, Val Loss: 0.4865\n",
      "Saving model at epoch 407 with loss 0.5270\n",
      "Epoch 408, Loss: 0.5267, Val Loss: 0.4862\n",
      "Saving model at epoch 408 with loss 0.5267\n",
      "Epoch 409, Loss: 0.5264, Val Loss: 0.4859\n",
      "Saving model at epoch 409 with loss 0.5264\n",
      "Epoch 410, Loss: 0.5260, Val Loss: 0.4856\n",
      "Saving model at epoch 410 with loss 0.5260\n",
      "Epoch 411, Loss: 0.5257, Val Loss: 0.4852\n",
      "Saving model at epoch 411 with loss 0.5257\n",
      "Epoch 412, Loss: 0.5254, Val Loss: 0.4849\n",
      "Saving model at epoch 412 with loss 0.5254\n",
      "Epoch 413, Loss: 0.5251, Val Loss: 0.4846\n",
      "Saving model at epoch 413 with loss 0.5251\n",
      "Epoch 414, Loss: 0.5248, Val Loss: 0.4843\n",
      "Saving model at epoch 414 with loss 0.5248\n",
      "Epoch 415, Loss: 0.5245, Val Loss: 0.4840\n",
      "Saving model at epoch 415 with loss 0.5245\n",
      "Epoch 416, Loss: 0.5242, Val Loss: 0.4837\n",
      "Saving model at epoch 416 with loss 0.5242\n",
      "Epoch 417, Loss: 0.5239, Val Loss: 0.4834\n",
      "Saving model at epoch 417 with loss 0.5239\n",
      "Epoch 418, Loss: 0.5236, Val Loss: 0.4831\n",
      "Saving model at epoch 418 with loss 0.5236\n",
      "Epoch 419, Loss: 0.5233, Val Loss: 0.4828\n",
      "Saving model at epoch 419 with loss 0.5233\n",
      "Epoch 420, Loss: 0.5230, Val Loss: 0.4825\n",
      "Saving model at epoch 420 with loss 0.5230\n",
      "Epoch 421, Loss: 0.5227, Val Loss: 0.4822\n",
      "Saving model at epoch 421 with loss 0.5227\n",
      "Epoch 422, Loss: 0.5224, Val Loss: 0.4819\n",
      "Saving model at epoch 422 with loss 0.5224\n",
      "Epoch 423, Loss: 0.5221, Val Loss: 0.4816\n",
      "Saving model at epoch 423 with loss 0.5221\n",
      "Epoch 424, Loss: 0.5218, Val Loss: 0.4813\n",
      "Saving model at epoch 424 with loss 0.5218\n",
      "Epoch 425, Loss: 0.5215, Val Loss: 0.4810\n",
      "Saving model at epoch 425 with loss 0.5215\n",
      "Epoch 426, Loss: 0.5212, Val Loss: 0.4807\n",
      "Saving model at epoch 426 with loss 0.5212\n",
      "Epoch 427, Loss: 0.5209, Val Loss: 0.4805\n",
      "Saving model at epoch 427 with loss 0.5209\n",
      "Epoch 428, Loss: 0.5206, Val Loss: 0.4802\n",
      "Saving model at epoch 428 with loss 0.5206\n",
      "Epoch 429, Loss: 0.5203, Val Loss: 0.4799\n",
      "Saving model at epoch 429 with loss 0.5203\n",
      "Epoch 430, Loss: 0.5201, Val Loss: 0.4796\n",
      "Saving model at epoch 430 with loss 0.5201\n",
      "Epoch 431, Loss: 0.5198, Val Loss: 0.4793\n",
      "Saving model at epoch 431 with loss 0.5198\n",
      "Epoch 432, Loss: 0.5195, Val Loss: 0.4790\n",
      "Saving model at epoch 432 with loss 0.5195\n",
      "Epoch 433, Loss: 0.5192, Val Loss: 0.4788\n",
      "Saving model at epoch 433 with loss 0.5192\n",
      "Epoch 434, Loss: 0.5190, Val Loss: 0.4785\n",
      "Saving model at epoch 434 with loss 0.5190\n",
      "Epoch 435, Loss: 0.5187, Val Loss: 0.4782\n",
      "Saving model at epoch 435 with loss 0.5187\n",
      "Epoch 436, Loss: 0.5184, Val Loss: 0.4779\n",
      "Saving model at epoch 436 with loss 0.5184\n",
      "Epoch 437, Loss: 0.5181, Val Loss: 0.4776\n",
      "Saving model at epoch 437 with loss 0.5181\n",
      "Epoch 438, Loss: 0.5179, Val Loss: 0.4774\n",
      "Saving model at epoch 438 with loss 0.5179\n",
      "Epoch 439, Loss: 0.5176, Val Loss: 0.4771\n",
      "Saving model at epoch 439 with loss 0.5176\n",
      "Epoch 440, Loss: 0.5173, Val Loss: 0.4768\n",
      "Saving model at epoch 440 with loss 0.5173\n",
      "Epoch 441, Loss: 0.5170, Val Loss: 0.4766\n",
      "Saving model at epoch 441 with loss 0.5170\n",
      "Epoch 442, Loss: 0.5168, Val Loss: 0.4763\n",
      "Saving model at epoch 442 with loss 0.5168\n",
      "Epoch 443, Loss: 0.5165, Val Loss: 0.4760\n",
      "Saving model at epoch 443 with loss 0.5165\n",
      "Epoch 444, Loss: 0.5163, Val Loss: 0.4758\n",
      "Saving model at epoch 444 with loss 0.5163\n",
      "Epoch 445, Loss: 0.5160, Val Loss: 0.4755\n",
      "Saving model at epoch 445 with loss 0.5160\n",
      "Epoch 446, Loss: 0.5158, Val Loss: 0.4752\n",
      "Saving model at epoch 446 with loss 0.5158\n",
      "Epoch 447, Loss: 0.5155, Val Loss: 0.4750\n",
      "Saving model at epoch 447 with loss 0.5155\n",
      "Epoch 448, Loss: 0.5152, Val Loss: 0.4747\n",
      "Saving model at epoch 448 with loss 0.5152\n",
      "Epoch 449, Loss: 0.5150, Val Loss: 0.4744\n",
      "Saving model at epoch 449 with loss 0.5150\n",
      "Epoch 450, Loss: 0.5147, Val Loss: 0.4742\n",
      "Saving model at epoch 450 with loss 0.5147\n",
      "Epoch 451, Loss: 0.5145, Val Loss: 0.4739\n",
      "Saving model at epoch 451 with loss 0.5145\n",
      "Epoch 452, Loss: 0.5142, Val Loss: 0.4737\n",
      "Saving model at epoch 452 with loss 0.5142\n",
      "Epoch 453, Loss: 0.5140, Val Loss: 0.4734\n",
      "Saving model at epoch 453 with loss 0.5140\n",
      "Epoch 454, Loss: 0.5137, Val Loss: 0.4732\n",
      "Saving model at epoch 454 with loss 0.5137\n",
      "Epoch 455, Loss: 0.5135, Val Loss: 0.4729\n",
      "Saving model at epoch 455 with loss 0.5135\n",
      "Epoch 456, Loss: 0.5132, Val Loss: 0.4727\n",
      "Saving model at epoch 456 with loss 0.5132\n",
      "Epoch 457, Loss: 0.5130, Val Loss: 0.4724\n",
      "Saving model at epoch 457 with loss 0.5130\n",
      "Epoch 458, Loss: 0.5128, Val Loss: 0.4722\n",
      "Saving model at epoch 458 with loss 0.5128\n",
      "Epoch 459, Loss: 0.5125, Val Loss: 0.4719\n",
      "Saving model at epoch 459 with loss 0.5125\n",
      "Epoch 460, Loss: 0.5123, Val Loss: 0.4717\n",
      "Saving model at epoch 460 with loss 0.5123\n",
      "Epoch 461, Loss: 0.5120, Val Loss: 0.4714\n",
      "Saving model at epoch 461 with loss 0.5120\n",
      "Epoch 462, Loss: 0.5118, Val Loss: 0.4712\n",
      "Saving model at epoch 462 with loss 0.5118\n",
      "Epoch 463, Loss: 0.5116, Val Loss: 0.4709\n",
      "Saving model at epoch 463 with loss 0.5116\n",
      "Epoch 464, Loss: 0.5113, Val Loss: 0.4707\n",
      "Saving model at epoch 464 with loss 0.5113\n",
      "Epoch 465, Loss: 0.5111, Val Loss: 0.4704\n",
      "Saving model at epoch 465 with loss 0.5111\n",
      "Epoch 466, Loss: 0.5108, Val Loss: 0.4702\n",
      "Saving model at epoch 466 with loss 0.5108\n",
      "Epoch 467, Loss: 0.5106, Val Loss: 0.4699\n",
      "Saving model at epoch 467 with loss 0.5106\n",
      "Epoch 468, Loss: 0.5104, Val Loss: 0.4697\n",
      "Saving model at epoch 468 with loss 0.5104\n",
      "Epoch 469, Loss: 0.5101, Val Loss: 0.4695\n",
      "Saving model at epoch 469 with loss 0.5101\n",
      "Epoch 470, Loss: 0.5099, Val Loss: 0.4692\n",
      "Saving model at epoch 470 with loss 0.5099\n",
      "Epoch 471, Loss: 0.5097, Val Loss: 0.4690\n",
      "Saving model at epoch 471 with loss 0.5097\n",
      "Epoch 472, Loss: 0.5095, Val Loss: 0.4687\n",
      "Saving model at epoch 472 with loss 0.5095\n",
      "Epoch 473, Loss: 0.5092, Val Loss: 0.4685\n",
      "Saving model at epoch 473 with loss 0.5092\n",
      "Epoch 474, Loss: 0.5090, Val Loss: 0.4683\n",
      "Saving model at epoch 474 with loss 0.5090\n",
      "Epoch 475, Loss: 0.5088, Val Loss: 0.4680\n",
      "Saving model at epoch 475 with loss 0.5088\n",
      "Epoch 476, Loss: 0.5086, Val Loss: 0.4678\n",
      "Saving model at epoch 476 with loss 0.5086\n",
      "Epoch 477, Loss: 0.5083, Val Loss: 0.4676\n",
      "Saving model at epoch 477 with loss 0.5083\n",
      "Epoch 478, Loss: 0.5081, Val Loss: 0.4673\n",
      "Saving model at epoch 478 with loss 0.5081\n",
      "Epoch 479, Loss: 0.5079, Val Loss: 0.4671\n",
      "Saving model at epoch 479 with loss 0.5079\n",
      "Epoch 480, Loss: 0.5077, Val Loss: 0.4669\n",
      "Saving model at epoch 480 with loss 0.5077\n",
      "Epoch 481, Loss: 0.5075, Val Loss: 0.4667\n",
      "Saving model at epoch 481 with loss 0.5075\n",
      "Epoch 482, Loss: 0.5073, Val Loss: 0.4664\n",
      "Saving model at epoch 482 with loss 0.5073\n",
      "Epoch 483, Loss: 0.5070, Val Loss: 0.4662\n",
      "Saving model at epoch 483 with loss 0.5070\n",
      "Epoch 484, Loss: 0.5068, Val Loss: 0.4660\n",
      "Saving model at epoch 484 with loss 0.5068\n",
      "Epoch 485, Loss: 0.5066, Val Loss: 0.4658\n",
      "Saving model at epoch 485 with loss 0.5066\n",
      "Epoch 486, Loss: 0.5064, Val Loss: 0.4656\n",
      "Saving model at epoch 486 with loss 0.5064\n",
      "Epoch 487, Loss: 0.5062, Val Loss: 0.4654\n",
      "Saving model at epoch 487 with loss 0.5062\n",
      "Epoch 488, Loss: 0.5060, Val Loss: 0.4651\n",
      "Saving model at epoch 488 with loss 0.5060\n",
      "Epoch 489, Loss: 0.5058, Val Loss: 0.4649\n",
      "Saving model at epoch 489 with loss 0.5058\n",
      "Epoch 490, Loss: 0.5056, Val Loss: 0.4647\n",
      "Saving model at epoch 490 with loss 0.5056\n",
      "Epoch 491, Loss: 0.5054, Val Loss: 0.4645\n",
      "Saving model at epoch 491 with loss 0.5054\n",
      "Epoch 492, Loss: 0.5052, Val Loss: 0.4643\n",
      "Saving model at epoch 492 with loss 0.5052\n",
      "Epoch 493, Loss: 0.5050, Val Loss: 0.4641\n",
      "Saving model at epoch 493 with loss 0.5050\n",
      "Epoch 494, Loss: 0.5048, Val Loss: 0.4639\n",
      "Saving model at epoch 494 with loss 0.5048\n",
      "Epoch 495, Loss: 0.5045, Val Loss: 0.4637\n",
      "Saving model at epoch 495 with loss 0.5045\n",
      "Epoch 496, Loss: 0.5044, Val Loss: 0.4635\n",
      "Saving model at epoch 496 with loss 0.5044\n",
      "Epoch 497, Loss: 0.5041, Val Loss: 0.4633\n",
      "Saving model at epoch 497 with loss 0.5041\n",
      "Epoch 498, Loss: 0.5040, Val Loss: 0.4631\n",
      "Saving model at epoch 498 with loss 0.5040\n",
      "Epoch 499, Loss: 0.5038, Val Loss: 0.4628\n",
      "Saving model at epoch 499 with loss 0.5038\n",
      "Epoch 500, Loss: 0.5036, Val Loss: 0.4626\n",
      "Saving model at epoch 500 with loss 0.5036\n",
      "Epoch 501, Loss: 0.5034, Val Loss: 0.4624\n",
      "Saving model at epoch 501 with loss 0.5034\n",
      "Epoch 502, Loss: 0.5032, Val Loss: 0.4622\n",
      "Saving model at epoch 502 with loss 0.5032\n",
      "Epoch 503, Loss: 0.5030, Val Loss: 0.4620\n",
      "Saving model at epoch 503 with loss 0.5030\n",
      "Epoch 504, Loss: 0.5028, Val Loss: 0.4618\n",
      "Saving model at epoch 504 with loss 0.5028\n",
      "Epoch 505, Loss: 0.5026, Val Loss: 0.4616\n",
      "Saving model at epoch 505 with loss 0.5026\n",
      "Epoch 506, Loss: 0.5024, Val Loss: 0.4614\n",
      "Saving model at epoch 506 with loss 0.5024\n",
      "Epoch 507, Loss: 0.5022, Val Loss: 0.4612\n",
      "Saving model at epoch 507 with loss 0.5022\n",
      "Epoch 508, Loss: 0.5020, Val Loss: 0.4610\n",
      "Saving model at epoch 508 with loss 0.5020\n",
      "Epoch 509, Loss: 0.5018, Val Loss: 0.4608\n",
      "Saving model at epoch 509 with loss 0.5018\n",
      "Epoch 510, Loss: 0.5017, Val Loss: 0.4607\n",
      "Saving model at epoch 510 with loss 0.5017\n",
      "Epoch 511, Loss: 0.5014, Val Loss: 0.4605\n",
      "Saving model at epoch 511 with loss 0.5014\n",
      "Epoch 512, Loss: 0.5013, Val Loss: 0.4603\n",
      "Saving model at epoch 512 with loss 0.5013\n",
      "Epoch 513, Loss: 0.5011, Val Loss: 0.4601\n",
      "Saving model at epoch 513 with loss 0.5011\n",
      "Epoch 514, Loss: 0.5009, Val Loss: 0.4599\n",
      "Saving model at epoch 514 with loss 0.5009\n",
      "Epoch 515, Loss: 0.5007, Val Loss: 0.4597\n",
      "Saving model at epoch 515 with loss 0.5007\n",
      "Epoch 516, Loss: 0.5005, Val Loss: 0.4595\n",
      "Saving model at epoch 516 with loss 0.5005\n",
      "Epoch 517, Loss: 0.5003, Val Loss: 0.4593\n",
      "Saving model at epoch 517 with loss 0.5003\n",
      "Epoch 518, Loss: 0.5002, Val Loss: 0.4591\n",
      "Saving model at epoch 518 with loss 0.5002\n",
      "Epoch 519, Loss: 0.5000, Val Loss: 0.4589\n",
      "Saving model at epoch 519 with loss 0.5000\n",
      "Epoch 520, Loss: 0.4998, Val Loss: 0.4587\n",
      "Saving model at epoch 520 with loss 0.4998\n",
      "Epoch 521, Loss: 0.4996, Val Loss: 0.4585\n",
      "Saving model at epoch 521 with loss 0.4996\n",
      "Epoch 522, Loss: 0.4995, Val Loss: 0.4583\n",
      "Saving model at epoch 522 with loss 0.4995\n",
      "Epoch 523, Loss: 0.4993, Val Loss: 0.4582\n",
      "Saving model at epoch 523 with loss 0.4993\n",
      "Epoch 524, Loss: 0.4991, Val Loss: 0.4580\n",
      "Saving model at epoch 524 with loss 0.4991\n",
      "Epoch 525, Loss: 0.4989, Val Loss: 0.4578\n",
      "Saving model at epoch 525 with loss 0.4989\n",
      "Epoch 526, Loss: 0.4988, Val Loss: 0.4576\n",
      "Saving model at epoch 526 with loss 0.4988\n",
      "Epoch 527, Loss: 0.4986, Val Loss: 0.4574\n",
      "Saving model at epoch 527 with loss 0.4986\n",
      "Epoch 528, Loss: 0.4984, Val Loss: 0.4572\n",
      "Saving model at epoch 528 with loss 0.4984\n",
      "Epoch 529, Loss: 0.4982, Val Loss: 0.4570\n",
      "Saving model at epoch 529 with loss 0.4982\n",
      "Epoch 530, Loss: 0.4981, Val Loss: 0.4569\n",
      "Saving model at epoch 530 with loss 0.4981\n",
      "Epoch 531, Loss: 0.4979, Val Loss: 0.4567\n",
      "Saving model at epoch 531 with loss 0.4979\n",
      "Epoch 532, Loss: 0.4977, Val Loss: 0.4565\n",
      "Saving model at epoch 532 with loss 0.4977\n",
      "Epoch 533, Loss: 0.4976, Val Loss: 0.4563\n",
      "Saving model at epoch 533 with loss 0.4976\n",
      "Epoch 534, Loss: 0.4974, Val Loss: 0.4561\n",
      "Saving model at epoch 534 with loss 0.4974\n",
      "Epoch 535, Loss: 0.4972, Val Loss: 0.4560\n",
      "Saving model at epoch 535 with loss 0.4972\n",
      "Epoch 536, Loss: 0.4971, Val Loss: 0.4558\n",
      "Saving model at epoch 536 with loss 0.4971\n",
      "Epoch 537, Loss: 0.4969, Val Loss: 0.4556\n",
      "Saving model at epoch 537 with loss 0.4969\n",
      "Epoch 538, Loss: 0.4968, Val Loss: 0.4554\n",
      "Saving model at epoch 538 with loss 0.4968\n",
      "Epoch 539, Loss: 0.4966, Val Loss: 0.4552\n",
      "Saving model at epoch 539 with loss 0.4966\n",
      "Epoch 540, Loss: 0.4964, Val Loss: 0.4551\n",
      "Saving model at epoch 540 with loss 0.4964\n",
      "Epoch 541, Loss: 0.4963, Val Loss: 0.4549\n",
      "Saving model at epoch 541 with loss 0.4963\n",
      "Epoch 542, Loss: 0.4961, Val Loss: 0.4547\n",
      "Saving model at epoch 542 with loss 0.4961\n",
      "Epoch 543, Loss: 0.4959, Val Loss: 0.4545\n",
      "Saving model at epoch 543 with loss 0.4959\n",
      "Epoch 544, Loss: 0.4958, Val Loss: 0.4544\n",
      "Saving model at epoch 544 with loss 0.4958\n",
      "Epoch 545, Loss: 0.4956, Val Loss: 0.4542\n",
      "Saving model at epoch 545 with loss 0.4956\n",
      "Epoch 546, Loss: 0.4955, Val Loss: 0.4540\n",
      "Saving model at epoch 546 with loss 0.4955\n",
      "Epoch 547, Loss: 0.4953, Val Loss: 0.4538\n",
      "Saving model at epoch 547 with loss 0.4953\n",
      "Epoch 548, Loss: 0.4952, Val Loss: 0.4537\n",
      "Saving model at epoch 548 with loss 0.4952\n",
      "Epoch 549, Loss: 0.4950, Val Loss: 0.4535\n",
      "Saving model at epoch 549 with loss 0.4950\n",
      "Epoch 550, Loss: 0.4948, Val Loss: 0.4533\n",
      "Saving model at epoch 550 with loss 0.4948\n",
      "Epoch 551, Loss: 0.4947, Val Loss: 0.4532\n",
      "Saving model at epoch 551 with loss 0.4947\n",
      "Epoch 552, Loss: 0.4945, Val Loss: 0.4530\n",
      "Saving model at epoch 552 with loss 0.4945\n",
      "Epoch 553, Loss: 0.4944, Val Loss: 0.4528\n",
      "Saving model at epoch 553 with loss 0.4944\n",
      "Epoch 554, Loss: 0.4942, Val Loss: 0.4526\n",
      "Saving model at epoch 554 with loss 0.4942\n",
      "Epoch 555, Loss: 0.4941, Val Loss: 0.4525\n",
      "Saving model at epoch 555 with loss 0.4941\n",
      "Epoch 556, Loss: 0.4939, Val Loss: 0.4523\n",
      "Saving model at epoch 556 with loss 0.4939\n",
      "Epoch 557, Loss: 0.4938, Val Loss: 0.4521\n",
      "Saving model at epoch 557 with loss 0.4938\n",
      "Epoch 558, Loss: 0.4936, Val Loss: 0.4520\n",
      "Saving model at epoch 558 with loss 0.4936\n",
      "Epoch 559, Loss: 0.4935, Val Loss: 0.4518\n",
      "Saving model at epoch 559 with loss 0.4935\n",
      "Epoch 560, Loss: 0.4933, Val Loss: 0.4516\n",
      "Saving model at epoch 560 with loss 0.4933\n",
      "Epoch 561, Loss: 0.4932, Val Loss: 0.4515\n",
      "Saving model at epoch 561 with loss 0.4932\n",
      "Epoch 562, Loss: 0.4930, Val Loss: 0.4513\n",
      "Saving model at epoch 562 with loss 0.4930\n",
      "Epoch 563, Loss: 0.4929, Val Loss: 0.4511\n",
      "Saving model at epoch 563 with loss 0.4929\n",
      "Epoch 564, Loss: 0.4927, Val Loss: 0.4510\n",
      "Saving model at epoch 564 with loss 0.4927\n",
      "Epoch 565, Loss: 0.4926, Val Loss: 0.4508\n",
      "Saving model at epoch 565 with loss 0.4926\n",
      "Epoch 566, Loss: 0.4924, Val Loss: 0.4506\n",
      "Saving model at epoch 566 with loss 0.4924\n",
      "Epoch 567, Loss: 0.4923, Val Loss: 0.4505\n",
      "Saving model at epoch 567 with loss 0.4923\n",
      "Epoch 568, Loss: 0.4921, Val Loss: 0.4503\n",
      "Saving model at epoch 568 with loss 0.4921\n",
      "Epoch 569, Loss: 0.4920, Val Loss: 0.4501\n",
      "Saving model at epoch 569 with loss 0.4920\n",
      "Epoch 570, Loss: 0.4918, Val Loss: 0.4500\n",
      "Saving model at epoch 570 with loss 0.4918\n",
      "Epoch 571, Loss: 0.4917, Val Loss: 0.4498\n",
      "Saving model at epoch 571 with loss 0.4917\n",
      "Epoch 572, Loss: 0.4916, Val Loss: 0.4497\n",
      "Saving model at epoch 572 with loss 0.4916\n",
      "Epoch 573, Loss: 0.4914, Val Loss: 0.4495\n",
      "Saving model at epoch 573 with loss 0.4914\n",
      "Epoch 574, Loss: 0.4913, Val Loss: 0.4493\n",
      "Saving model at epoch 574 with loss 0.4913\n",
      "Epoch 575, Loss: 0.4911, Val Loss: 0.4492\n",
      "Saving model at epoch 575 with loss 0.4911\n",
      "Epoch 576, Loss: 0.4910, Val Loss: 0.4490\n",
      "Saving model at epoch 576 with loss 0.4910\n",
      "Epoch 577, Loss: 0.4909, Val Loss: 0.4489\n",
      "Saving model at epoch 577 with loss 0.4909\n",
      "Epoch 578, Loss: 0.4907, Val Loss: 0.4487\n",
      "Saving model at epoch 578 with loss 0.4907\n",
      "Epoch 579, Loss: 0.4906, Val Loss: 0.4485\n",
      "Saving model at epoch 579 with loss 0.4906\n",
      "Epoch 580, Loss: 0.4905, Val Loss: 0.4484\n",
      "Saving model at epoch 580 with loss 0.4905\n",
      "Epoch 581, Loss: 0.4903, Val Loss: 0.4482\n",
      "Saving model at epoch 581 with loss 0.4903\n",
      "Epoch 582, Loss: 0.4902, Val Loss: 0.4481\n",
      "Saving model at epoch 582 with loss 0.4902\n",
      "Epoch 583, Loss: 0.4900, Val Loss: 0.4479\n",
      "Saving model at epoch 583 with loss 0.4900\n",
      "Epoch 584, Loss: 0.4899, Val Loss: 0.4478\n",
      "Saving model at epoch 584 with loss 0.4899\n",
      "Epoch 585, Loss: 0.4898, Val Loss: 0.4476\n",
      "Saving model at epoch 585 with loss 0.4898\n",
      "Epoch 586, Loss: 0.4897, Val Loss: 0.4475\n",
      "Saving model at epoch 586 with loss 0.4897\n",
      "Epoch 587, Loss: 0.4895, Val Loss: 0.4473\n",
      "Saving model at epoch 587 with loss 0.4895\n",
      "Epoch 588, Loss: 0.4894, Val Loss: 0.4471\n",
      "Saving model at epoch 588 with loss 0.4894\n",
      "Epoch 589, Loss: 0.4893, Val Loss: 0.4470\n",
      "Saving model at epoch 589 with loss 0.4893\n",
      "Epoch 590, Loss: 0.4891, Val Loss: 0.4468\n",
      "Saving model at epoch 590 with loss 0.4891\n",
      "Epoch 591, Loss: 0.4890, Val Loss: 0.4467\n",
      "Saving model at epoch 591 with loss 0.4890\n",
      "Epoch 592, Loss: 0.4889, Val Loss: 0.4465\n",
      "Saving model at epoch 592 with loss 0.4889\n",
      "Epoch 593, Loss: 0.4887, Val Loss: 0.4464\n",
      "Saving model at epoch 593 with loss 0.4887\n",
      "Epoch 594, Loss: 0.4886, Val Loss: 0.4462\n",
      "Saving model at epoch 594 with loss 0.4886\n",
      "Epoch 595, Loss: 0.4885, Val Loss: 0.4461\n",
      "Saving model at epoch 595 with loss 0.4885\n",
      "Epoch 596, Loss: 0.4884, Val Loss: 0.4459\n",
      "Saving model at epoch 596 with loss 0.4884\n",
      "Epoch 597, Loss: 0.4882, Val Loss: 0.4458\n",
      "Saving model at epoch 597 with loss 0.4882\n",
      "Epoch 598, Loss: 0.4881, Val Loss: 0.4456\n",
      "Saving model at epoch 598 with loss 0.4881\n",
      "Epoch 599, Loss: 0.4880, Val Loss: 0.4455\n",
      "Saving model at epoch 599 with loss 0.4880\n",
      "Epoch 600, Loss: 0.4878, Val Loss: 0.4453\n",
      "Saving model at epoch 600 with loss 0.4878\n",
      "Epoch 601, Loss: 0.4877, Val Loss: 0.4452\n",
      "Saving model at epoch 601 with loss 0.4877\n",
      "Epoch 602, Loss: 0.4876, Val Loss: 0.4450\n",
      "Saving model at epoch 602 with loss 0.4876\n",
      "Epoch 603, Loss: 0.4875, Val Loss: 0.4449\n",
      "Saving model at epoch 603 with loss 0.4875\n",
      "Epoch 604, Loss: 0.4874, Val Loss: 0.4447\n",
      "Saving model at epoch 604 with loss 0.4874\n",
      "Epoch 605, Loss: 0.4872, Val Loss: 0.4446\n",
      "Saving model at epoch 605 with loss 0.4872\n",
      "Epoch 606, Loss: 0.4871, Val Loss: 0.4444\n",
      "Saving model at epoch 606 with loss 0.4871\n",
      "Epoch 607, Loss: 0.4870, Val Loss: 0.4443\n",
      "Saving model at epoch 607 with loss 0.4870\n",
      "Epoch 608, Loss: 0.4869, Val Loss: 0.4442\n",
      "Saving model at epoch 608 with loss 0.4869\n",
      "Epoch 609, Loss: 0.4868, Val Loss: 0.4440\n",
      "Saving model at epoch 609 with loss 0.4868\n",
      "Epoch 610, Loss: 0.4866, Val Loss: 0.4439\n",
      "Saving model at epoch 610 with loss 0.4866\n",
      "Epoch 611, Loss: 0.4865, Val Loss: 0.4437\n",
      "Saving model at epoch 611 with loss 0.4865\n",
      "Epoch 612, Loss: 0.4864, Val Loss: 0.4436\n",
      "Saving model at epoch 612 with loss 0.4864\n",
      "Epoch 613, Loss: 0.4863, Val Loss: 0.4434\n",
      "Saving model at epoch 613 with loss 0.4863\n",
      "Epoch 614, Loss: 0.4862, Val Loss: 0.4433\n",
      "Saving model at epoch 614 with loss 0.4862\n",
      "Epoch 615, Loss: 0.4860, Val Loss: 0.4431\n",
      "Saving model at epoch 615 with loss 0.4860\n",
      "Epoch 616, Loss: 0.4859, Val Loss: 0.4430\n",
      "Saving model at epoch 616 with loss 0.4859\n",
      "Epoch 617, Loss: 0.4858, Val Loss: 0.4429\n",
      "Saving model at epoch 617 with loss 0.4858\n",
      "Epoch 618, Loss: 0.4857, Val Loss: 0.4427\n",
      "Saving model at epoch 618 with loss 0.4857\n",
      "Epoch 619, Loss: 0.4856, Val Loss: 0.4426\n",
      "Saving model at epoch 619 with loss 0.4856\n",
      "Epoch 620, Loss: 0.4855, Val Loss: 0.4424\n",
      "Saving model at epoch 620 with loss 0.4855\n",
      "Epoch 621, Loss: 0.4854, Val Loss: 0.4423\n",
      "Saving model at epoch 621 with loss 0.4854\n",
      "Epoch 622, Loss: 0.4852, Val Loss: 0.4422\n",
      "Saving model at epoch 622 with loss 0.4852\n",
      "Epoch 623, Loss: 0.4851, Val Loss: 0.4420\n",
      "Saving model at epoch 623 with loss 0.4851\n",
      "Epoch 624, Loss: 0.4850, Val Loss: 0.4419\n",
      "Saving model at epoch 624 with loss 0.4850\n",
      "Epoch 625, Loss: 0.4849, Val Loss: 0.4417\n",
      "Saving model at epoch 625 with loss 0.4849\n",
      "Epoch 626, Loss: 0.4848, Val Loss: 0.4416\n",
      "Saving model at epoch 626 with loss 0.4848\n",
      "Epoch 627, Loss: 0.4847, Val Loss: 0.4415\n",
      "Saving model at epoch 627 with loss 0.4847\n",
      "Epoch 628, Loss: 0.4846, Val Loss: 0.4413\n",
      "Saving model at epoch 628 with loss 0.4846\n",
      "Epoch 629, Loss: 0.4845, Val Loss: 0.4412\n",
      "Saving model at epoch 629 with loss 0.4845\n",
      "Epoch 630, Loss: 0.4844, Val Loss: 0.4410\n",
      "Saving model at epoch 630 with loss 0.4844\n",
      "Epoch 631, Loss: 0.4842, Val Loss: 0.4409\n",
      "Saving model at epoch 631 with loss 0.4842\n",
      "Epoch 632, Loss: 0.4841, Val Loss: 0.4408\n",
      "Saving model at epoch 632 with loss 0.4841\n",
      "Epoch 633, Loss: 0.4840, Val Loss: 0.4406\n",
      "Saving model at epoch 633 with loss 0.4840\n",
      "Epoch 634, Loss: 0.4839, Val Loss: 0.4405\n",
      "Saving model at epoch 634 with loss 0.4839\n",
      "Epoch 635, Loss: 0.4838, Val Loss: 0.4404\n",
      "Saving model at epoch 635 with loss 0.4838\n",
      "Epoch 636, Loss: 0.4837, Val Loss: 0.4402\n",
      "Saving model at epoch 636 with loss 0.4837\n",
      "Epoch 637, Loss: 0.4836, Val Loss: 0.4401\n",
      "Saving model at epoch 637 with loss 0.4836\n",
      "Epoch 638, Loss: 0.4835, Val Loss: 0.4400\n",
      "Saving model at epoch 638 with loss 0.4835\n",
      "Epoch 639, Loss: 0.4834, Val Loss: 0.4398\n",
      "Saving model at epoch 639 with loss 0.4834\n",
      "Epoch 640, Loss: 0.4833, Val Loss: 0.4397\n",
      "Saving model at epoch 640 with loss 0.4833\n",
      "Epoch 641, Loss: 0.4832, Val Loss: 0.4396\n",
      "Saving model at epoch 641 with loss 0.4832\n",
      "Epoch 642, Loss: 0.4831, Val Loss: 0.4394\n",
      "Saving model at epoch 642 with loss 0.4831\n",
      "Epoch 643, Loss: 0.4830, Val Loss: 0.4393\n",
      "Saving model at epoch 643 with loss 0.4830\n",
      "Epoch 644, Loss: 0.4829, Val Loss: 0.4392\n",
      "Saving model at epoch 644 with loss 0.4829\n",
      "Epoch 645, Loss: 0.4828, Val Loss: 0.4391\n",
      "Saving model at epoch 645 with loss 0.4828\n",
      "Epoch 646, Loss: 0.4827, Val Loss: 0.4389\n",
      "Saving model at epoch 646 with loss 0.4827\n",
      "Epoch 647, Loss: 0.4826, Val Loss: 0.4388\n",
      "Saving model at epoch 647 with loss 0.4826\n",
      "Epoch 648, Loss: 0.4825, Val Loss: 0.4387\n",
      "Saving model at epoch 648 with loss 0.4825\n",
      "Epoch 649, Loss: 0.4824, Val Loss: 0.4385\n",
      "Saving model at epoch 649 with loss 0.4824\n",
      "Epoch 650, Loss: 0.4823, Val Loss: 0.4384\n",
      "Saving model at epoch 650 with loss 0.4823\n",
      "Epoch 651, Loss: 0.4822, Val Loss: 0.4383\n",
      "Saving model at epoch 651 with loss 0.4822\n",
      "Epoch 652, Loss: 0.4821, Val Loss: 0.4381\n",
      "Saving model at epoch 652 with loss 0.4821\n",
      "Epoch 653, Loss: 0.4820, Val Loss: 0.4380\n",
      "Saving model at epoch 653 with loss 0.4820\n",
      "Epoch 654, Loss: 0.4819, Val Loss: 0.4379\n",
      "Saving model at epoch 654 with loss 0.4819\n",
      "Epoch 655, Loss: 0.4818, Val Loss: 0.4378\n",
      "Saving model at epoch 655 with loss 0.4818\n",
      "Epoch 656, Loss: 0.4817, Val Loss: 0.4376\n",
      "Saving model at epoch 656 with loss 0.4817\n",
      "Epoch 657, Loss: 0.4816, Val Loss: 0.4375\n",
      "Saving model at epoch 657 with loss 0.4816\n",
      "Epoch 658, Loss: 0.4815, Val Loss: 0.4374\n",
      "Saving model at epoch 658 with loss 0.4815\n",
      "Epoch 659, Loss: 0.4814, Val Loss: 0.4373\n",
      "Saving model at epoch 659 with loss 0.4814\n",
      "Epoch 660, Loss: 0.4813, Val Loss: 0.4371\n",
      "Saving model at epoch 660 with loss 0.4813\n",
      "Epoch 661, Loss: 0.4812, Val Loss: 0.4370\n",
      "Saving model at epoch 661 with loss 0.4812\n",
      "Epoch 662, Loss: 0.4811, Val Loss: 0.4369\n",
      "Saving model at epoch 662 with loss 0.4811\n",
      "Epoch 663, Loss: 0.4810, Val Loss: 0.4368\n",
      "Saving model at epoch 663 with loss 0.4810\n",
      "Epoch 664, Loss: 0.4809, Val Loss: 0.4366\n",
      "Saving model at epoch 664 with loss 0.4809\n",
      "Epoch 665, Loss: 0.4808, Val Loss: 0.4365\n",
      "Saving model at epoch 665 with loss 0.4808\n",
      "Epoch 666, Loss: 0.4808, Val Loss: 0.4364\n",
      "Saving model at epoch 666 with loss 0.4808\n",
      "Epoch 667, Loss: 0.4806, Val Loss: 0.4363\n",
      "Saving model at epoch 667 with loss 0.4806\n",
      "Epoch 668, Loss: 0.4806, Val Loss: 0.4361\n",
      "Saving model at epoch 668 with loss 0.4806\n",
      "Epoch 669, Loss: 0.4805, Val Loss: 0.4360\n",
      "Saving model at epoch 669 with loss 0.4805\n",
      "Epoch 670, Loss: 0.4804, Val Loss: 0.4359\n",
      "Saving model at epoch 670 with loss 0.4804\n",
      "Epoch 671, Loss: 0.4803, Val Loss: 0.4358\n",
      "Saving model at epoch 671 with loss 0.4803\n",
      "Epoch 672, Loss: 0.4802, Val Loss: 0.4356\n",
      "Saving model at epoch 672 with loss 0.4802\n",
      "Epoch 673, Loss: 0.4801, Val Loss: 0.4355\n",
      "Saving model at epoch 673 with loss 0.4801\n",
      "Epoch 674, Loss: 0.4800, Val Loss: 0.4354\n",
      "Saving model at epoch 674 with loss 0.4800\n",
      "Epoch 675, Loss: 0.4799, Val Loss: 0.4353\n",
      "Saving model at epoch 675 with loss 0.4799\n",
      "Epoch 676, Loss: 0.4798, Val Loss: 0.4352\n",
      "Saving model at epoch 676 with loss 0.4798\n",
      "Epoch 677, Loss: 0.4797, Val Loss: 0.4350\n",
      "Saving model at epoch 677 with loss 0.4797\n",
      "Epoch 678, Loss: 0.4797, Val Loss: 0.4349\n",
      "Saving model at epoch 678 with loss 0.4797\n",
      "Epoch 679, Loss: 0.4796, Val Loss: 0.4348\n",
      "Saving model at epoch 679 with loss 0.4796\n",
      "Epoch 680, Loss: 0.4795, Val Loss: 0.4347\n",
      "Saving model at epoch 680 with loss 0.4795\n",
      "Epoch 681, Loss: 0.4794, Val Loss: 0.4346\n",
      "Saving model at epoch 681 with loss 0.4794\n",
      "Epoch 682, Loss: 0.4793, Val Loss: 0.4344\n",
      "Saving model at epoch 682 with loss 0.4793\n",
      "Epoch 683, Loss: 0.4792, Val Loss: 0.4343\n",
      "Saving model at epoch 683 with loss 0.4792\n",
      "Epoch 684, Loss: 0.4791, Val Loss: 0.4342\n",
      "Saving model at epoch 684 with loss 0.4791\n",
      "Epoch 685, Loss: 0.4790, Val Loss: 0.4341\n",
      "Saving model at epoch 685 with loss 0.4790\n",
      "Epoch 686, Loss: 0.4790, Val Loss: 0.4340\n",
      "Saving model at epoch 686 with loss 0.4790\n",
      "Epoch 687, Loss: 0.4789, Val Loss: 0.4339\n",
      "Saving model at epoch 687 with loss 0.4789\n",
      "Epoch 688, Loss: 0.4788, Val Loss: 0.4337\n",
      "Saving model at epoch 688 with loss 0.4788\n",
      "Epoch 689, Loss: 0.4787, Val Loss: 0.4336\n",
      "Saving model at epoch 689 with loss 0.4787\n",
      "Epoch 690, Loss: 0.4786, Val Loss: 0.4335\n",
      "Saving model at epoch 690 with loss 0.4786\n",
      "Epoch 691, Loss: 0.4785, Val Loss: 0.4334\n",
      "Saving model at epoch 691 with loss 0.4785\n",
      "Epoch 692, Loss: 0.4785, Val Loss: 0.4333\n",
      "Saving model at epoch 692 with loss 0.4785\n",
      "Epoch 693, Loss: 0.4784, Val Loss: 0.4332\n",
      "Saving model at epoch 693 with loss 0.4784\n",
      "Epoch 694, Loss: 0.4783, Val Loss: 0.4330\n",
      "Saving model at epoch 694 with loss 0.4783\n",
      "Epoch 695, Loss: 0.4782, Val Loss: 0.4329\n",
      "Saving model at epoch 695 with loss 0.4782\n",
      "Epoch 696, Loss: 0.4781, Val Loss: 0.4328\n",
      "Saving model at epoch 696 with loss 0.4781\n",
      "Epoch 697, Loss: 0.4781, Val Loss: 0.4327\n",
      "Saving model at epoch 697 with loss 0.4781\n",
      "Epoch 698, Loss: 0.4780, Val Loss: 0.4326\n",
      "Saving model at epoch 698 with loss 0.4780\n",
      "Epoch 699, Loss: 0.4779, Val Loss: 0.4325\n",
      "Saving model at epoch 699 with loss 0.4779\n",
      "Epoch 700, Loss: 0.4778, Val Loss: 0.4324\n",
      "Saving model at epoch 700 with loss 0.4778\n",
      "Epoch 701, Loss: 0.4777, Val Loss: 0.4323\n",
      "Saving model at epoch 701 with loss 0.4777\n",
      "Epoch 702, Loss: 0.4776, Val Loss: 0.4321\n",
      "Saving model at epoch 702 with loss 0.4776\n",
      "Epoch 703, Loss: 0.4776, Val Loss: 0.4320\n",
      "Saving model at epoch 703 with loss 0.4776\n",
      "Epoch 704, Loss: 0.4775, Val Loss: 0.4319\n",
      "Saving model at epoch 704 with loss 0.4775\n",
      "Epoch 705, Loss: 0.4774, Val Loss: 0.4318\n",
      "Saving model at epoch 705 with loss 0.4774\n",
      "Epoch 706, Loss: 0.4773, Val Loss: 0.4317\n",
      "Saving model at epoch 706 with loss 0.4773\n",
      "Epoch 707, Loss: 0.4772, Val Loss: 0.4316\n",
      "Saving model at epoch 707 with loss 0.4772\n",
      "Epoch 708, Loss: 0.4772, Val Loss: 0.4315\n",
      "Saving model at epoch 708 with loss 0.4772\n",
      "Epoch 709, Loss: 0.4771, Val Loss: 0.4314\n",
      "Saving model at epoch 709 with loss 0.4771\n",
      "Epoch 710, Loss: 0.4770, Val Loss: 0.4313\n",
      "Saving model at epoch 710 with loss 0.4770\n",
      "Epoch 711, Loss: 0.4769, Val Loss: 0.4312\n",
      "Saving model at epoch 711 with loss 0.4769\n",
      "Epoch 712, Loss: 0.4769, Val Loss: 0.4311\n",
      "Saving model at epoch 712 with loss 0.4769\n",
      "Epoch 713, Loss: 0.4768, Val Loss: 0.4309\n",
      "Saving model at epoch 713 with loss 0.4768\n",
      "Epoch 714, Loss: 0.4767, Val Loss: 0.4308\n",
      "Saving model at epoch 714 with loss 0.4767\n",
      "Epoch 715, Loss: 0.4766, Val Loss: 0.4307\n",
      "Saving model at epoch 715 with loss 0.4766\n",
      "Epoch 716, Loss: 0.4766, Val Loss: 0.4306\n",
      "Saving model at epoch 716 with loss 0.4766\n",
      "Epoch 717, Loss: 0.4765, Val Loss: 0.4305\n",
      "Saving model at epoch 717 with loss 0.4765\n",
      "Epoch 718, Loss: 0.4764, Val Loss: 0.4304\n",
      "Saving model at epoch 718 with loss 0.4764\n",
      "Epoch 719, Loss: 0.4763, Val Loss: 0.4303\n",
      "Saving model at epoch 719 with loss 0.4763\n",
      "Epoch 720, Loss: 0.4763, Val Loss: 0.4302\n",
      "Saving model at epoch 720 with loss 0.4763\n",
      "Epoch 721, Loss: 0.4762, Val Loss: 0.4301\n",
      "Saving model at epoch 721 with loss 0.4762\n",
      "Epoch 722, Loss: 0.4761, Val Loss: 0.4300\n",
      "Saving model at epoch 722 with loss 0.4761\n",
      "Epoch 723, Loss: 0.4761, Val Loss: 0.4299\n",
      "Saving model at epoch 723 with loss 0.4761\n",
      "Epoch 724, Loss: 0.4760, Val Loss: 0.4298\n",
      "Saving model at epoch 724 with loss 0.4760\n",
      "Epoch 725, Loss: 0.4759, Val Loss: 0.4297\n",
      "Saving model at epoch 725 with loss 0.4759\n",
      "Epoch 726, Loss: 0.4758, Val Loss: 0.4296\n",
      "Saving model at epoch 726 with loss 0.4758\n",
      "Epoch 727, Loss: 0.4757, Val Loss: 0.4295\n",
      "Saving model at epoch 727 with loss 0.4757\n",
      "Epoch 728, Loss: 0.4757, Val Loss: 0.4294\n",
      "Saving model at epoch 728 with loss 0.4757\n",
      "Epoch 729, Loss: 0.4756, Val Loss: 0.4293\n",
      "Saving model at epoch 729 with loss 0.4756\n",
      "Epoch 730, Loss: 0.4755, Val Loss: 0.4291\n",
      "Saving model at epoch 730 with loss 0.4755\n",
      "Epoch 731, Loss: 0.4755, Val Loss: 0.4290\n",
      "Saving model at epoch 731 with loss 0.4755\n",
      "Epoch 732, Loss: 0.4754, Val Loss: 0.4289\n",
      "Saving model at epoch 732 with loss 0.4754\n",
      "Epoch 733, Loss: 0.4753, Val Loss: 0.4288\n",
      "Saving model at epoch 733 with loss 0.4753\n",
      "Epoch 734, Loss: 0.4753, Val Loss: 0.4287\n",
      "Saving model at epoch 734 with loss 0.4753\n",
      "Epoch 735, Loss: 0.4752, Val Loss: 0.4286\n",
      "Saving model at epoch 735 with loss 0.4752\n",
      "Epoch 736, Loss: 0.4751, Val Loss: 0.4285\n",
      "Saving model at epoch 736 with loss 0.4751\n",
      "Epoch 737, Loss: 0.4750, Val Loss: 0.4284\n",
      "Saving model at epoch 737 with loss 0.4750\n",
      "Epoch 738, Loss: 0.4750, Val Loss: 0.4283\n",
      "Saving model at epoch 738 with loss 0.4750\n",
      "Epoch 739, Loss: 0.4749, Val Loss: 0.4282\n",
      "Saving model at epoch 739 with loss 0.4749\n",
      "Epoch 740, Loss: 0.4748, Val Loss: 0.4281\n",
      "Saving model at epoch 740 with loss 0.4748\n",
      "Epoch 741, Loss: 0.4748, Val Loss: 0.4280\n",
      "Saving model at epoch 741 with loss 0.4748\n",
      "Epoch 742, Loss: 0.4747, Val Loss: 0.4279\n",
      "Saving model at epoch 742 with loss 0.4747\n",
      "Epoch 743, Loss: 0.4746, Val Loss: 0.4278\n",
      "Saving model at epoch 743 with loss 0.4746\n",
      "Epoch 744, Loss: 0.4746, Val Loss: 0.4277\n",
      "Saving model at epoch 744 with loss 0.4746\n",
      "Epoch 745, Loss: 0.4745, Val Loss: 0.4276\n",
      "Saving model at epoch 745 with loss 0.4745\n",
      "Epoch 746, Loss: 0.4745, Val Loss: 0.4275\n",
      "Saving model at epoch 746 with loss 0.4745\n",
      "Epoch 747, Loss: 0.4744, Val Loss: 0.4274\n",
      "Saving model at epoch 747 with loss 0.4744\n",
      "Epoch 748, Loss: 0.4743, Val Loss: 0.4273\n",
      "Saving model at epoch 748 with loss 0.4743\n",
      "Epoch 749, Loss: 0.4742, Val Loss: 0.4272\n",
      "Saving model at epoch 749 with loss 0.4742\n",
      "Epoch 750, Loss: 0.4742, Val Loss: 0.4271\n",
      "Saving model at epoch 750 with loss 0.4742\n",
      "Epoch 751, Loss: 0.4741, Val Loss: 0.4270\n",
      "Saving model at epoch 751 with loss 0.4741\n",
      "Epoch 752, Loss: 0.4740, Val Loss: 0.4269\n",
      "Saving model at epoch 752 with loss 0.4740\n",
      "Epoch 753, Loss: 0.4740, Val Loss: 0.4268\n",
      "Saving model at epoch 753 with loss 0.4740\n",
      "Epoch 754, Loss: 0.4739, Val Loss: 0.4267\n",
      "Saving model at epoch 754 with loss 0.4739\n",
      "Epoch 755, Loss: 0.4739, Val Loss: 0.4267\n",
      "Saving model at epoch 755 with loss 0.4739\n",
      "Epoch 756, Loss: 0.4738, Val Loss: 0.4266\n",
      "Saving model at epoch 756 with loss 0.4738\n",
      "Epoch 757, Loss: 0.4737, Val Loss: 0.4265\n",
      "Saving model at epoch 757 with loss 0.4737\n",
      "Epoch 758, Loss: 0.4737, Val Loss: 0.4264\n",
      "Saving model at epoch 758 with loss 0.4737\n",
      "Epoch 759, Loss: 0.4736, Val Loss: 0.4263\n",
      "Saving model at epoch 759 with loss 0.4736\n",
      "Epoch 760, Loss: 0.4736, Val Loss: 0.4262\n",
      "Saving model at epoch 760 with loss 0.4736\n",
      "Epoch 761, Loss: 0.4735, Val Loss: 0.4261\n",
      "Saving model at epoch 761 with loss 0.4735\n",
      "Epoch 762, Loss: 0.4735, Val Loss: 0.4260\n",
      "Saving model at epoch 762 with loss 0.4735\n",
      "Epoch 763, Loss: 0.4734, Val Loss: 0.4259\n",
      "Saving model at epoch 763 with loss 0.4734\n",
      "Epoch 764, Loss: 0.4733, Val Loss: 0.4258\n",
      "Saving model at epoch 764 with loss 0.4733\n",
      "Epoch 765, Loss: 0.4733, Val Loss: 0.4257\n",
      "Saving model at epoch 765 with loss 0.4733\n",
      "Epoch 766, Loss: 0.4732, Val Loss: 0.4256\n",
      "Saving model at epoch 766 with loss 0.4732\n",
      "Epoch 767, Loss: 0.4731, Val Loss: 0.4255\n",
      "Saving model at epoch 767 with loss 0.4731\n",
      "Epoch 768, Loss: 0.4731, Val Loss: 0.4254\n",
      "Saving model at epoch 768 with loss 0.4731\n",
      "Epoch 769, Loss: 0.4730, Val Loss: 0.4253\n",
      "Saving model at epoch 769 with loss 0.4730\n",
      "Epoch 770, Loss: 0.4730, Val Loss: 0.4252\n",
      "Saving model at epoch 770 with loss 0.4730\n",
      "Epoch 771, Loss: 0.4729, Val Loss: 0.4251\n",
      "Saving model at epoch 771 with loss 0.4729\n",
      "Epoch 772, Loss: 0.4728, Val Loss: 0.4250\n",
      "Saving model at epoch 772 with loss 0.4728\n",
      "Epoch 773, Loss: 0.4728, Val Loss: 0.4250\n",
      "Saving model at epoch 773 with loss 0.4728\n",
      "Epoch 774, Loss: 0.4727, Val Loss: 0.4249\n",
      "Saving model at epoch 774 with loss 0.4727\n",
      "Epoch 775, Loss: 0.4727, Val Loss: 0.4248\n",
      "Saving model at epoch 775 with loss 0.4727\n",
      "Epoch 776, Loss: 0.4726, Val Loss: 0.4247\n",
      "Saving model at epoch 776 with loss 0.4726\n",
      "Epoch 777, Loss: 0.4726, Val Loss: 0.4246\n",
      "Saving model at epoch 777 with loss 0.4726\n",
      "Epoch 778, Loss: 0.4725, Val Loss: 0.4245\n",
      "Saving model at epoch 778 with loss 0.4725\n",
      "Epoch 779, Loss: 0.4725, Val Loss: 0.4244\n",
      "Saving model at epoch 779 with loss 0.4725\n",
      "Epoch 780, Loss: 0.4724, Val Loss: 0.4243\n",
      "Saving model at epoch 780 with loss 0.4724\n",
      "Epoch 781, Loss: 0.4723, Val Loss: 0.4242\n",
      "Saving model at epoch 781 with loss 0.4723\n",
      "Epoch 782, Loss: 0.4723, Val Loss: 0.4241\n",
      "Saving model at epoch 782 with loss 0.4723\n",
      "Epoch 783, Loss: 0.4722, Val Loss: 0.4240\n",
      "Saving model at epoch 783 with loss 0.4722\n",
      "Epoch 784, Loss: 0.4722, Val Loss: 0.4240\n",
      "Saving model at epoch 784 with loss 0.4722\n",
      "Epoch 785, Loss: 0.4721, Val Loss: 0.4239\n",
      "Saving model at epoch 785 with loss 0.4721\n",
      "Epoch 786, Loss: 0.4720, Val Loss: 0.4238\n",
      "Saving model at epoch 786 with loss 0.4720\n",
      "Epoch 787, Loss: 0.4720, Val Loss: 0.4237\n",
      "Saving model at epoch 787 with loss 0.4720\n",
      "Epoch 788, Loss: 0.4719, Val Loss: 0.4236\n",
      "Saving model at epoch 788 with loss 0.4719\n",
      "Epoch 789, Loss: 0.4719, Val Loss: 0.4235\n",
      "Saving model at epoch 789 with loss 0.4719\n",
      "Epoch 790, Loss: 0.4718, Val Loss: 0.4234\n",
      "Saving model at epoch 790 with loss 0.4718\n",
      "Epoch 791, Loss: 0.4718, Val Loss: 0.4233\n",
      "Saving model at epoch 791 with loss 0.4718\n",
      "Epoch 792, Loss: 0.4717, Val Loss: 0.4233\n",
      "Saving model at epoch 792 with loss 0.4717\n",
      "Epoch 793, Loss: 0.4717, Val Loss: 0.4232\n",
      "Saving model at epoch 793 with loss 0.4717\n",
      "Epoch 794, Loss: 0.4716, Val Loss: 0.4231\n",
      "Saving model at epoch 794 with loss 0.4716\n",
      "Epoch 795, Loss: 0.4716, Val Loss: 0.4230\n",
      "Saving model at epoch 795 with loss 0.4716\n",
      "Epoch 796, Loss: 0.4715, Val Loss: 0.4229\n",
      "Saving model at epoch 796 with loss 0.4715\n",
      "Epoch 797, Loss: 0.4715, Val Loss: 0.4228\n",
      "Saving model at epoch 797 with loss 0.4715\n",
      "Epoch 798, Loss: 0.4714, Val Loss: 0.4227\n",
      "Saving model at epoch 798 with loss 0.4714\n",
      "Epoch 799, Loss: 0.4714, Val Loss: 0.4226\n",
      "Saving model at epoch 799 with loss 0.4714\n",
      "Epoch 800, Loss: 0.4713, Val Loss: 0.4226\n",
      "Saving model at epoch 800 with loss 0.4713\n",
      "Epoch 801, Loss: 0.4713, Val Loss: 0.4225\n",
      "Saving model at epoch 801 with loss 0.4713\n",
      "Epoch 802, Loss: 0.4712, Val Loss: 0.4224\n",
      "Saving model at epoch 802 with loss 0.4712\n",
      "Epoch 803, Loss: 0.4712, Val Loss: 0.4223\n",
      "Saving model at epoch 803 with loss 0.4712\n",
      "Epoch 804, Loss: 0.4711, Val Loss: 0.4222\n",
      "Saving model at epoch 804 with loss 0.4711\n",
      "Epoch 805, Loss: 0.4711, Val Loss: 0.4221\n",
      "Saving model at epoch 805 with loss 0.4711\n",
      "Epoch 806, Loss: 0.4710, Val Loss: 0.4221\n",
      "Saving model at epoch 806 with loss 0.4710\n",
      "Epoch 807, Loss: 0.4710, Val Loss: 0.4220\n",
      "Saving model at epoch 807 with loss 0.4710\n",
      "Epoch 808, Loss: 0.4709, Val Loss: 0.4219\n",
      "Saving model at epoch 808 with loss 0.4709\n",
      "Epoch 809, Loss: 0.4708, Val Loss: 0.4218\n",
      "Saving model at epoch 809 with loss 0.4708\n",
      "Epoch 810, Loss: 0.4708, Val Loss: 0.4217\n",
      "Saving model at epoch 810 with loss 0.4708\n",
      "Epoch 811, Loss: 0.4707, Val Loss: 0.4216\n",
      "Saving model at epoch 811 with loss 0.4707\n",
      "Epoch 812, Loss: 0.4707, Val Loss: 0.4215\n",
      "Saving model at epoch 812 with loss 0.4707\n",
      "Epoch 813, Loss: 0.4707, Val Loss: 0.4215\n",
      "Saving model at epoch 813 with loss 0.4707\n",
      "Epoch 814, Loss: 0.4706, Val Loss: 0.4214\n",
      "Saving model at epoch 814 with loss 0.4706\n",
      "Epoch 815, Loss: 0.4706, Val Loss: 0.4213\n",
      "Saving model at epoch 815 with loss 0.4706\n",
      "Epoch 816, Loss: 0.4705, Val Loss: 0.4212\n",
      "Saving model at epoch 816 with loss 0.4705\n",
      "Epoch 817, Loss: 0.4705, Val Loss: 0.4211\n",
      "Saving model at epoch 817 with loss 0.4705\n",
      "Epoch 818, Loss: 0.4704, Val Loss: 0.4211\n",
      "Saving model at epoch 818 with loss 0.4704\n",
      "Epoch 819, Loss: 0.4703, Val Loss: 0.4210\n",
      "Saving model at epoch 819 with loss 0.4703\n",
      "Epoch 820, Loss: 0.4703, Val Loss: 0.4209\n",
      "Saving model at epoch 820 with loss 0.4703\n",
      "Epoch 821, Loss: 0.4703, Val Loss: 0.4208\n",
      "Saving model at epoch 821 with loss 0.4703\n",
      "Epoch 822, Loss: 0.4702, Val Loss: 0.4207\n",
      "Saving model at epoch 822 with loss 0.4702\n",
      "Epoch 823, Loss: 0.4702, Val Loss: 0.4206\n",
      "Saving model at epoch 823 with loss 0.4702\n",
      "Epoch 824, Loss: 0.4701, Val Loss: 0.4206\n",
      "Saving model at epoch 824 with loss 0.4701\n",
      "Epoch 825, Loss: 0.4701, Val Loss: 0.4205\n",
      "Saving model at epoch 825 with loss 0.4701\n",
      "Epoch 826, Loss: 0.4700, Val Loss: 0.4204\n",
      "Saving model at epoch 826 with loss 0.4700\n",
      "Epoch 827, Loss: 0.4700, Val Loss: 0.4203\n",
      "Saving model at epoch 827 with loss 0.4700\n",
      "Epoch 828, Loss: 0.4699, Val Loss: 0.4202\n",
      "Saving model at epoch 828 with loss 0.4699\n",
      "Epoch 829, Loss: 0.4699, Val Loss: 0.4201\n",
      "Saving model at epoch 829 with loss 0.4699\n",
      "Epoch 830, Loss: 0.4698, Val Loss: 0.4201\n",
      "Saving model at epoch 830 with loss 0.4698\n",
      "Epoch 831, Loss: 0.4698, Val Loss: 0.4200\n",
      "Saving model at epoch 831 with loss 0.4698\n",
      "Epoch 832, Loss: 0.4698, Val Loss: 0.4199\n",
      "Saving model at epoch 832 with loss 0.4698\n",
      "Epoch 833, Loss: 0.4697, Val Loss: 0.4198\n",
      "Saving model at epoch 833 with loss 0.4697\n",
      "Epoch 834, Loss: 0.4697, Val Loss: 0.4197\n",
      "Saving model at epoch 834 with loss 0.4697\n",
      "Epoch 835, Loss: 0.4696, Val Loss: 0.4197\n",
      "Saving model at epoch 835 with loss 0.4696\n",
      "Epoch 836, Loss: 0.4696, Val Loss: 0.4196\n",
      "Saving model at epoch 836 with loss 0.4696\n",
      "Epoch 837, Loss: 0.4695, Val Loss: 0.4195\n",
      "Saving model at epoch 837 with loss 0.4695\n",
      "Epoch 838, Loss: 0.4695, Val Loss: 0.4194\n",
      "Saving model at epoch 838 with loss 0.4695\n",
      "Epoch 839, Loss: 0.4694, Val Loss: 0.4194\n",
      "Saving model at epoch 839 with loss 0.4694\n",
      "Epoch 840, Loss: 0.4694, Val Loss: 0.4193\n",
      "Saving model at epoch 840 with loss 0.4694\n",
      "Epoch 841, Loss: 0.4693, Val Loss: 0.4192\n",
      "Saving model at epoch 841 with loss 0.4693\n",
      "Epoch 842, Loss: 0.4693, Val Loss: 0.4191\n",
      "Saving model at epoch 842 with loss 0.4693\n",
      "Epoch 843, Loss: 0.4692, Val Loss: 0.4190\n",
      "Saving model at epoch 843 with loss 0.4692\n",
      "Epoch 844, Loss: 0.4692, Val Loss: 0.4190\n",
      "Saving model at epoch 844 with loss 0.4692\n",
      "Epoch 845, Loss: 0.4691, Val Loss: 0.4189\n",
      "Saving model at epoch 845 with loss 0.4691\n",
      "Epoch 846, Loss: 0.4691, Val Loss: 0.4188\n",
      "Saving model at epoch 846 with loss 0.4691\n",
      "Epoch 847, Loss: 0.4690, Val Loss: 0.4187\n",
      "Saving model at epoch 847 with loss 0.4690\n",
      "Epoch 848, Loss: 0.4690, Val Loss: 0.4187\n",
      "Saving model at epoch 848 with loss 0.4690\n",
      "Epoch 849, Loss: 0.4689, Val Loss: 0.4186\n",
      "Saving model at epoch 849 with loss 0.4689\n",
      "Epoch 850, Loss: 0.4689, Val Loss: 0.4185\n",
      "Saving model at epoch 850 with loss 0.4689\n",
      "Epoch 851, Loss: 0.4689, Val Loss: 0.4184\n",
      "Saving model at epoch 851 with loss 0.4689\n",
      "Epoch 852, Loss: 0.4688, Val Loss: 0.4184\n",
      "Saving model at epoch 852 with loss 0.4688\n",
      "Epoch 853, Loss: 0.4688, Val Loss: 0.4183\n",
      "Saving model at epoch 853 with loss 0.4688\n",
      "Epoch 854, Loss: 0.4687, Val Loss: 0.4182\n",
      "Saving model at epoch 854 with loss 0.4687\n",
      "Epoch 855, Loss: 0.4687, Val Loss: 0.4181\n",
      "Saving model at epoch 855 with loss 0.4687\n",
      "Epoch 856, Loss: 0.4686, Val Loss: 0.4181\n",
      "Saving model at epoch 856 with loss 0.4686\n",
      "Epoch 857, Loss: 0.4686, Val Loss: 0.4180\n",
      "Saving model at epoch 857 with loss 0.4686\n",
      "Epoch 858, Loss: 0.4686, Val Loss: 0.4179\n",
      "Saving model at epoch 858 with loss 0.4686\n",
      "Epoch 859, Loss: 0.4685, Val Loss: 0.4178\n",
      "Saving model at epoch 859 with loss 0.4685\n",
      "Epoch 860, Loss: 0.4685, Val Loss: 0.4178\n",
      "Saving model at epoch 860 with loss 0.4685\n",
      "Epoch 861, Loss: 0.4684, Val Loss: 0.4177\n",
      "Saving model at epoch 861 with loss 0.4684\n",
      "Epoch 862, Loss: 0.4684, Val Loss: 0.4176\n",
      "Saving model at epoch 862 with loss 0.4684\n",
      "Epoch 863, Loss: 0.4683, Val Loss: 0.4175\n",
      "Saving model at epoch 863 with loss 0.4683\n",
      "Epoch 864, Loss: 0.4683, Val Loss: 0.4175\n",
      "Saving model at epoch 864 with loss 0.4683\n",
      "Epoch 865, Loss: 0.4683, Val Loss: 0.4174\n",
      "Saving model at epoch 865 with loss 0.4683\n",
      "Epoch 866, Loss: 0.4682, Val Loss: 0.4173\n",
      "Saving model at epoch 866 with loss 0.4682\n",
      "Epoch 867, Loss: 0.4682, Val Loss: 0.4173\n",
      "Saving model at epoch 867 with loss 0.4682\n",
      "Epoch 868, Loss: 0.4681, Val Loss: 0.4172\n",
      "Saving model at epoch 868 with loss 0.4681\n",
      "Epoch 869, Loss: 0.4681, Val Loss: 0.4171\n",
      "Saving model at epoch 869 with loss 0.4681\n",
      "Epoch 870, Loss: 0.4680, Val Loss: 0.4170\n",
      "Saving model at epoch 870 with loss 0.4680\n",
      "Epoch 871, Loss: 0.4680, Val Loss: 0.4170\n",
      "Saving model at epoch 871 with loss 0.4680\n",
      "Epoch 872, Loss: 0.4680, Val Loss: 0.4169\n",
      "Saving model at epoch 872 with loss 0.4680\n",
      "Epoch 873, Loss: 0.4679, Val Loss: 0.4168\n",
      "Saving model at epoch 873 with loss 0.4679\n",
      "Epoch 874, Loss: 0.4679, Val Loss: 0.4168\n",
      "Saving model at epoch 874 with loss 0.4679\n",
      "Epoch 875, Loss: 0.4678, Val Loss: 0.4167\n",
      "Saving model at epoch 875 with loss 0.4678\n",
      "Epoch 876, Loss: 0.4678, Val Loss: 0.4166\n",
      "Saving model at epoch 876 with loss 0.4678\n",
      "Epoch 877, Loss: 0.4677, Val Loss: 0.4165\n",
      "Saving model at epoch 877 with loss 0.4677\n",
      "Epoch 878, Loss: 0.4677, Val Loss: 0.4165\n",
      "Saving model at epoch 878 with loss 0.4677\n",
      "Epoch 879, Loss: 0.4677, Val Loss: 0.4164\n",
      "Saving model at epoch 879 with loss 0.4677\n",
      "Epoch 880, Loss: 0.4676, Val Loss: 0.4163\n",
      "Saving model at epoch 880 with loss 0.4676\n",
      "Epoch 881, Loss: 0.4676, Val Loss: 0.4163\n",
      "Saving model at epoch 881 with loss 0.4676\n",
      "Epoch 882, Loss: 0.4676, Val Loss: 0.4162\n",
      "Saving model at epoch 882 with loss 0.4676\n",
      "Epoch 883, Loss: 0.4675, Val Loss: 0.4161\n",
      "Saving model at epoch 883 with loss 0.4675\n",
      "Epoch 884, Loss: 0.4675, Val Loss: 0.4161\n",
      "Saving model at epoch 884 with loss 0.4675\n",
      "Epoch 885, Loss: 0.4674, Val Loss: 0.4160\n",
      "Saving model at epoch 885 with loss 0.4674\n",
      "Epoch 886, Loss: 0.4674, Val Loss: 0.4159\n",
      "Saving model at epoch 886 with loss 0.4674\n",
      "Epoch 887, Loss: 0.4674, Val Loss: 0.4159\n",
      "Saving model at epoch 887 with loss 0.4674\n",
      "Epoch 888, Loss: 0.4673, Val Loss: 0.4158\n",
      "Saving model at epoch 888 with loss 0.4673\n",
      "Epoch 889, Loss: 0.4673, Val Loss: 0.4157\n",
      "Saving model at epoch 889 with loss 0.4673\n",
      "Epoch 890, Loss: 0.4673, Val Loss: 0.4156\n",
      "Saving model at epoch 890 with loss 0.4673\n",
      "Epoch 891, Loss: 0.4672, Val Loss: 0.4156\n",
      "Saving model at epoch 891 with loss 0.4672\n",
      "Epoch 892, Loss: 0.4672, Val Loss: 0.4155\n",
      "Saving model at epoch 892 with loss 0.4672\n",
      "Epoch 893, Loss: 0.4672, Val Loss: 0.4154\n",
      "Saving model at epoch 893 with loss 0.4672\n",
      "Epoch 894, Loss: 0.4671, Val Loss: 0.4154\n",
      "Saving model at epoch 894 with loss 0.4671\n",
      "Epoch 895, Loss: 0.4671, Val Loss: 0.4153\n",
      "Saving model at epoch 895 with loss 0.4671\n",
      "Epoch 896, Loss: 0.4671, Val Loss: 0.4152\n",
      "Saving model at epoch 896 with loss 0.4671\n",
      "Epoch 897, Loss: 0.4670, Val Loss: 0.4152\n",
      "Saving model at epoch 897 with loss 0.4670\n",
      "Epoch 898, Loss: 0.4670, Val Loss: 0.4151\n",
      "Saving model at epoch 898 with loss 0.4670\n",
      "Epoch 899, Loss: 0.4669, Val Loss: 0.4150\n",
      "Saving model at epoch 899 with loss 0.4669\n",
      "Epoch 900, Loss: 0.4669, Val Loss: 0.4150\n",
      "Saving model at epoch 900 with loss 0.4669\n",
      "Epoch 901, Loss: 0.4669, Val Loss: 0.4149\n",
      "Saving model at epoch 901 with loss 0.4669\n",
      "Epoch 902, Loss: 0.4668, Val Loss: 0.4148\n",
      "Saving model at epoch 902 with loss 0.4668\n",
      "Epoch 903, Loss: 0.4668, Val Loss: 0.4148\n",
      "Saving model at epoch 903 with loss 0.4668\n",
      "Epoch 904, Loss: 0.4668, Val Loss: 0.4147\n",
      "Saving model at epoch 904 with loss 0.4668\n",
      "Epoch 905, Loss: 0.4667, Val Loss: 0.4146\n",
      "Saving model at epoch 905 with loss 0.4667\n",
      "Epoch 906, Loss: 0.4667, Val Loss: 0.4146\n",
      "Saving model at epoch 906 with loss 0.4667\n",
      "Epoch 907, Loss: 0.4667, Val Loss: 0.4145\n",
      "Saving model at epoch 907 with loss 0.4667\n",
      "Epoch 908, Loss: 0.4666, Val Loss: 0.4145\n",
      "Saving model at epoch 908 with loss 0.4666\n",
      "Epoch 909, Loss: 0.4666, Val Loss: 0.4144\n",
      "Saving model at epoch 909 with loss 0.4666\n",
      "Epoch 910, Loss: 0.4665, Val Loss: 0.4143\n",
      "Saving model at epoch 910 with loss 0.4665\n",
      "Epoch 911, Loss: 0.4665, Val Loss: 0.4143\n",
      "Saving model at epoch 911 with loss 0.4665\n",
      "Epoch 912, Loss: 0.4665, Val Loss: 0.4142\n",
      "Saving model at epoch 912 with loss 0.4665\n",
      "Epoch 913, Loss: 0.4665, Val Loss: 0.4141\n",
      "Saving model at epoch 913 with loss 0.4665\n",
      "Epoch 914, Loss: 0.4664, Val Loss: 0.4141\n",
      "Saving model at epoch 914 with loss 0.4664\n",
      "Epoch 915, Loss: 0.4664, Val Loss: 0.4140\n",
      "Saving model at epoch 915 with loss 0.4664\n",
      "Epoch 916, Loss: 0.4664, Val Loss: 0.4139\n",
      "Saving model at epoch 916 with loss 0.4664\n",
      "Epoch 917, Loss: 0.4663, Val Loss: 0.4139\n",
      "Saving model at epoch 917 with loss 0.4663\n",
      "Epoch 918, Loss: 0.4663, Val Loss: 0.4138\n",
      "Saving model at epoch 918 with loss 0.4663\n",
      "Epoch 919, Loss: 0.4662, Val Loss: 0.4137\n",
      "Saving model at epoch 919 with loss 0.4662\n",
      "Epoch 920, Loss: 0.4662, Val Loss: 0.4137\n",
      "Saving model at epoch 920 with loss 0.4662\n",
      "Epoch 921, Loss: 0.4662, Val Loss: 0.4136\n",
      "Saving model at epoch 921 with loss 0.4662\n",
      "Epoch 922, Loss: 0.4662, Val Loss: 0.4136\n",
      "Saving model at epoch 922 with loss 0.4662\n",
      "Epoch 923, Loss: 0.4661, Val Loss: 0.4135\n",
      "Saving model at epoch 923 with loss 0.4661\n",
      "Epoch 924, Loss: 0.4661, Val Loss: 0.4134\n",
      "Saving model at epoch 924 with loss 0.4661\n",
      "Epoch 925, Loss: 0.4661, Val Loss: 0.4134\n",
      "Saving model at epoch 925 with loss 0.4661\n",
      "Epoch 926, Loss: 0.4660, Val Loss: 0.4133\n",
      "Saving model at epoch 926 with loss 0.4660\n",
      "Epoch 927, Loss: 0.4660, Val Loss: 0.4132\n",
      "Saving model at epoch 927 with loss 0.4660\n",
      "Epoch 928, Loss: 0.4659, Val Loss: 0.4132\n",
      "Saving model at epoch 928 with loss 0.4659\n",
      "Epoch 929, Loss: 0.4659, Val Loss: 0.4131\n",
      "Saving model at epoch 929 with loss 0.4659\n",
      "Epoch 930, Loss: 0.4659, Val Loss: 0.4131\n",
      "Saving model at epoch 930 with loss 0.4659\n",
      "Epoch 931, Loss: 0.4659, Val Loss: 0.4130\n",
      "Saving model at epoch 931 with loss 0.4659\n",
      "Epoch 932, Loss: 0.4658, Val Loss: 0.4129\n",
      "Saving model at epoch 932 with loss 0.4658\n",
      "Epoch 933, Loss: 0.4658, Val Loss: 0.4129\n",
      "Saving model at epoch 933 with loss 0.4658\n",
      "Epoch 934, Loss: 0.4658, Val Loss: 0.4128\n",
      "Saving model at epoch 934 with loss 0.4658\n",
      "Epoch 935, Loss: 0.4657, Val Loss: 0.4127\n",
      "Saving model at epoch 935 with loss 0.4657\n",
      "Epoch 936, Loss: 0.4657, Val Loss: 0.4127\n",
      "Saving model at epoch 936 with loss 0.4657\n",
      "Epoch 937, Loss: 0.4657, Val Loss: 0.4126\n",
      "Saving model at epoch 937 with loss 0.4657\n",
      "Epoch 938, Loss: 0.4656, Val Loss: 0.4126\n",
      "Saving model at epoch 938 with loss 0.4656\n",
      "Epoch 939, Loss: 0.4656, Val Loss: 0.4125\n",
      "Saving model at epoch 939 with loss 0.4656\n",
      "Epoch 940, Loss: 0.4656, Val Loss: 0.4124\n",
      "Saving model at epoch 940 with loss 0.4656\n",
      "Epoch 941, Loss: 0.4655, Val Loss: 0.4124\n",
      "Saving model at epoch 941 with loss 0.4655\n",
      "Epoch 942, Loss: 0.4655, Val Loss: 0.4123\n",
      "Saving model at epoch 942 with loss 0.4655\n",
      "Epoch 943, Loss: 0.4655, Val Loss: 0.4123\n",
      "Saving model at epoch 943 with loss 0.4655\n",
      "Epoch 944, Loss: 0.4654, Val Loss: 0.4122\n",
      "Saving model at epoch 944 with loss 0.4654\n",
      "Epoch 945, Loss: 0.4654, Val Loss: 0.4121\n",
      "Saving model at epoch 945 with loss 0.4654\n",
      "Epoch 946, Loss: 0.4654, Val Loss: 0.4121\n",
      "Saving model at epoch 946 with loss 0.4654\n",
      "Epoch 947, Loss: 0.4653, Val Loss: 0.4120\n",
      "Saving model at epoch 947 with loss 0.4653\n",
      "Epoch 948, Loss: 0.4653, Val Loss: 0.4119\n",
      "Saving model at epoch 948 with loss 0.4653\n",
      "Epoch 949, Loss: 0.4653, Val Loss: 0.4119\n",
      "Saving model at epoch 949 with loss 0.4653\n",
      "Epoch 950, Loss: 0.4652, Val Loss: 0.4118\n",
      "Saving model at epoch 950 with loss 0.4652\n",
      "Epoch 951, Loss: 0.4652, Val Loss: 0.4118\n",
      "Saving model at epoch 951 with loss 0.4652\n",
      "Epoch 952, Loss: 0.4652, Val Loss: 0.4117\n",
      "Saving model at epoch 952 with loss 0.4652\n",
      "Epoch 953, Loss: 0.4651, Val Loss: 0.4116\n",
      "Saving model at epoch 953 with loss 0.4651\n",
      "Epoch 954, Loss: 0.4651, Val Loss: 0.4116\n",
      "Saving model at epoch 954 with loss 0.4651\n",
      "Epoch 955, Loss: 0.4651, Val Loss: 0.4115\n",
      "Saving model at epoch 955 with loss 0.4651\n",
      "Epoch 956, Loss: 0.4650, Val Loss: 0.4115\n",
      "Saving model at epoch 956 with loss 0.4650\n",
      "Epoch 957, Loss: 0.4650, Val Loss: 0.4114\n",
      "Saving model at epoch 957 with loss 0.4650\n",
      "Epoch 958, Loss: 0.4650, Val Loss: 0.4113\n",
      "Saving model at epoch 958 with loss 0.4650\n",
      "Epoch 959, Loss: 0.4650, Val Loss: 0.4113\n",
      "Saving model at epoch 959 with loss 0.4650\n",
      "Epoch 960, Loss: 0.4649, Val Loss: 0.4112\n",
      "Saving model at epoch 960 with loss 0.4649\n",
      "Epoch 961, Loss: 0.4649, Val Loss: 0.4112\n",
      "Saving model at epoch 961 with loss 0.4649\n",
      "Epoch 962, Loss: 0.4649, Val Loss: 0.4111\n",
      "Saving model at epoch 962 with loss 0.4649\n",
      "Epoch 963, Loss: 0.4648, Val Loss: 0.4111\n",
      "Saving model at epoch 963 with loss 0.4648\n",
      "Epoch 964, Loss: 0.4648, Val Loss: 0.4110\n",
      "Saving model at epoch 964 with loss 0.4648\n",
      "Epoch 965, Loss: 0.4648, Val Loss: 0.4109\n",
      "Saving model at epoch 965 with loss 0.4648\n",
      "Epoch 966, Loss: 0.4647, Val Loss: 0.4109\n",
      "Saving model at epoch 966 with loss 0.4647\n",
      "Epoch 967, Loss: 0.4647, Val Loss: 0.4108\n",
      "Saving model at epoch 967 with loss 0.4647\n",
      "Epoch 968, Loss: 0.4647, Val Loss: 0.4108\n",
      "Saving model at epoch 968 with loss 0.4647\n",
      "Epoch 969, Loss: 0.4646, Val Loss: 0.4107\n",
      "Saving model at epoch 969 with loss 0.4646\n",
      "Epoch 970, Loss: 0.4646, Val Loss: 0.4106\n",
      "Saving model at epoch 970 with loss 0.4646\n",
      "Epoch 971, Loss: 0.4646, Val Loss: 0.4106\n",
      "Saving model at epoch 971 with loss 0.4646\n",
      "Epoch 972, Loss: 0.4646, Val Loss: 0.4105\n",
      "Saving model at epoch 972 with loss 0.4646\n",
      "Epoch 973, Loss: 0.4645, Val Loss: 0.4105\n",
      "Saving model at epoch 973 with loss 0.4645\n",
      "Epoch 974, Loss: 0.4645, Val Loss: 0.4104\n",
      "Saving model at epoch 974 with loss 0.4645\n",
      "Epoch 975, Loss: 0.4645, Val Loss: 0.4104\n",
      "Saving model at epoch 975 with loss 0.4645\n",
      "Epoch 976, Loss: 0.4644, Val Loss: 0.4103\n",
      "Saving model at epoch 976 with loss 0.4644\n",
      "Epoch 977, Loss: 0.4644, Val Loss: 0.4102\n",
      "Saving model at epoch 977 with loss 0.4644\n",
      "Epoch 978, Loss: 0.4644, Val Loss: 0.4102\n",
      "Saving model at epoch 978 with loss 0.4644\n",
      "Epoch 979, Loss: 0.4643, Val Loss: 0.4101\n",
      "Saving model at epoch 979 with loss 0.4643\n",
      "Epoch 980, Loss: 0.4643, Val Loss: 0.4101\n",
      "Saving model at epoch 980 with loss 0.4643\n",
      "Epoch 981, Loss: 0.4643, Val Loss: 0.4100\n",
      "Saving model at epoch 981 with loss 0.4643\n",
      "Epoch 982, Loss: 0.4643, Val Loss: 0.4100\n",
      "Saving model at epoch 982 with loss 0.4643\n",
      "Epoch 983, Loss: 0.4642, Val Loss: 0.4099\n",
      "Saving model at epoch 983 with loss 0.4642\n",
      "Epoch 984, Loss: 0.4642, Val Loss: 0.4098\n",
      "Saving model at epoch 984 with loss 0.4642\n",
      "Epoch 985, Loss: 0.4642, Val Loss: 0.4098\n",
      "Saving model at epoch 985 with loss 0.4642\n",
      "Epoch 986, Loss: 0.4641, Val Loss: 0.4097\n",
      "Saving model at epoch 986 with loss 0.4641\n",
      "Epoch 987, Loss: 0.4641, Val Loss: 0.4097\n",
      "Saving model at epoch 987 with loss 0.4641\n",
      "Epoch 988, Loss: 0.4641, Val Loss: 0.4096\n",
      "Saving model at epoch 988 with loss 0.4641\n",
      "Epoch 989, Loss: 0.4641, Val Loss: 0.4096\n",
      "Saving model at epoch 989 with loss 0.4641\n",
      "Epoch 990, Loss: 0.4640, Val Loss: 0.4095\n",
      "Saving model at epoch 990 with loss 0.4640\n",
      "Epoch 991, Loss: 0.4640, Val Loss: 0.4094\n",
      "Saving model at epoch 991 with loss 0.4640\n",
      "Epoch 992, Loss: 0.4640, Val Loss: 0.4094\n",
      "Saving model at epoch 992 with loss 0.4640\n",
      "Epoch 993, Loss: 0.4639, Val Loss: 0.4093\n",
      "Saving model at epoch 993 with loss 0.4639\n",
      "Epoch 994, Loss: 0.4639, Val Loss: 0.4093\n",
      "Saving model at epoch 994 with loss 0.4639\n",
      "Epoch 995, Loss: 0.4639, Val Loss: 0.4092\n",
      "Saving model at epoch 995 with loss 0.4639\n",
      "Epoch 996, Loss: 0.4638, Val Loss: 0.4092\n",
      "Saving model at epoch 996 with loss 0.4638\n",
      "Epoch 997, Loss: 0.4638, Val Loss: 0.4091\n",
      "Saving model at epoch 997 with loss 0.4638\n",
      "Epoch 998, Loss: 0.4638, Val Loss: 0.4091\n",
      "Saving model at epoch 998 with loss 0.4638\n",
      "Epoch 999, Loss: 0.4638, Val Loss: 0.4090\n",
      "Saving model at epoch 999 with loss 0.4638\n",
      "Test Loss (MSE): 0.4648\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the features we want to keep\n",
    "# Keep only pickup and drop off positions, and add pickup and dropoff month, day, hour\n",
    "features_model2 = [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\",\n",
    "                   \"pickup_month\", \"pickup_day\", \"pickup_hour\", \"dropoff_month\", \"dropoff_day\", \"dropoff_hour\"]\n",
    "\n",
    "# Split 10% of the training data as validation set\n",
    "model2_x = select_features(X_train, features_model2)\n",
    "model2_y = y_train\n",
    "model2_x_train, model2_x_val, model2_y_train, model2_y_val = train_test_split(model2_x, model2_y, test_size=0.1, random_state=0)\n",
    "model2_x_test = select_features(X_test, features_model2)\n",
    "\n",
    "# Define Model\n",
    "model2 = Sequential()\n",
    "model2.add(LinearLayer(model2_x_train.shape[1], 10))\n",
    "model2.add(ReLU())\n",
    "model2.add(LinearLayer(10, 10))\n",
    "model2.add(ReLU())\n",
    "model2.add(LinearLayer(10, 10))\n",
    "model2.add(ReLU())\n",
    "model2.add(LinearLayer(10, 1))\n",
    "\n",
    "# Training Setup\n",
    "epochs_model2 = 1000\n",
    "learning_rate_model2 = 0.01\n",
    "loss_function_model2 = MeanSquaredErrorLoss()\n",
    "\n",
    "best_loss_model2 = float(\"inf\")\n",
    "patience_model2 = 3  # Early stopping patience\n",
    "stagnation_model2 = 0\n",
    "losses_model2 = []\n",
    "val_losses_model2 = []\n",
    "\n",
    "for epoch_model2 in range(epochs_model2):\n",
    "    predictions_model2 = model2.forward(model2_x_train)\n",
    "    loss_model2 = loss_function_model2.forward(predictions_model2, model2_y_train)\n",
    "    losses_model2.append(loss_model2)\n",
    "    \n",
    "    val_predictions_model2 = model2.forward(model2_x_val)\n",
    "    val_loss_model2 = loss_function_model2.forward(val_predictions_model2, model2_y_val)\n",
    "    val_losses_model2.append(val_loss_model2)\n",
    "    \n",
    "    print(f\"Epoch {epoch_model2}, Loss: {loss_model2:.4f}, Val Loss: {val_loss_model2:.4f}\")\n",
    "    \n",
    "    grad_output_model2 = loss_function_model2.backward()\n",
    "    model2.backward(grad_output_model2, learning_rate_model2)\n",
    "\n",
    "    # Early stopping\n",
    "    if loss_model2 < best_loss_model2:\n",
    "        best_loss_model2 = loss_model2\n",
    "        stagnation_model2 = 0\n",
    "        print(f\"Saving model at epoch {epoch_model2} with loss {loss_model2:.4f}\")\n",
    "        model2.save_weights(\"best_model2\")\n",
    "    else:\n",
    "        stagnation_model2 += 1\n",
    "        if stagnation_model2 >= patience_model2:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load best model and evaluate on test set\n",
    "test_predictions_model2 = model2.forward(model2_x_test)\n",
    "test_loss_model2 = loss_function_model2.forward(test_predictions_model2, y_test)\n",
    "print(f\"Test Loss (MSE): {test_loss_model2:.4f}\")\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses_model2, label='Training Loss')\n",
    "plt.plot(val_losses_model2, label='Validation Loss', linestyle='dashed')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickup Time, Position and Dropoff Time, Position and day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 43.7380, Val Loss: 43.7221\n",
      "Saving model at epoch 0 with loss 43.7380\n",
      "Epoch 1, Loss: 39.2889, Val Loss: 39.2865\n",
      "Saving model at epoch 1 with loss 39.2889\n",
      "Epoch 2, Loss: 35.2051, Val Loss: 35.2328\n",
      "Saving model at epoch 2 with loss 35.2051\n",
      "Epoch 3, Loss: 30.7171, Val Loss: 30.8145\n",
      "Saving model at epoch 3 with loss 30.7171\n",
      "Epoch 4, Loss: 25.5001, Val Loss: 25.7202\n",
      "Saving model at epoch 4 with loss 25.5001\n",
      "Epoch 5, Loss: 19.5195, Val Loss: 19.8946\n",
      "Saving model at epoch 5 with loss 19.5195\n",
      "Epoch 6, Loss: 13.6094, Val Loss: 14.1609\n",
      "Saving model at epoch 6 with loss 13.6094\n",
      "Epoch 7, Loss: 9.5762, Val Loss: 9.8554\n",
      "Saving model at epoch 7 with loss 9.5762\n",
      "Epoch 8, Loss: 7.5425, Val Loss: 7.6136\n",
      "Saving model at epoch 8 with loss 7.5425\n",
      "Epoch 9, Loss: 6.6684, Val Loss: 6.3473\n",
      "Saving model at epoch 9 with loss 6.6684\n",
      "Epoch 10, Loss: 5.9225, Val Loss: 5.5232\n",
      "Saving model at epoch 10 with loss 5.9225\n",
      "Epoch 11, Loss: 5.2693, Val Loss: 4.8915\n",
      "Saving model at epoch 11 with loss 5.2693\n",
      "Epoch 12, Loss: 4.7815, Val Loss: 4.3623\n",
      "Saving model at epoch 12 with loss 4.7815\n",
      "Epoch 13, Loss: 4.3464, Val Loss: 3.9293\n",
      "Saving model at epoch 13 with loss 4.3464\n",
      "Epoch 14, Loss: 4.0029, Val Loss: 3.5817\n",
      "Saving model at epoch 14 with loss 4.0029\n",
      "Epoch 15, Loss: 3.7013, Val Loss: 3.2879\n",
      "Saving model at epoch 15 with loss 3.7013\n",
      "Epoch 16, Loss: 3.4536, Val Loss: 3.0506\n",
      "Saving model at epoch 16 with loss 3.4536\n",
      "Epoch 17, Loss: 3.2362, Val Loss: 2.8460\n",
      "Saving model at epoch 17 with loss 3.2362\n",
      "Epoch 18, Loss: 3.0477, Val Loss: 2.6741\n",
      "Saving model at epoch 18 with loss 3.0477\n",
      "Epoch 19, Loss: 2.8863, Val Loss: 2.5272\n",
      "Saving model at epoch 19 with loss 2.8863\n",
      "Epoch 20, Loss: 2.7465, Val Loss: 2.4000\n",
      "Saving model at epoch 20 with loss 2.7465\n",
      "Epoch 21, Loss: 2.6235, Val Loss: 2.2889\n",
      "Saving model at epoch 21 with loss 2.6235\n",
      "Epoch 22, Loss: 2.5151, Val Loss: 2.1912\n",
      "Saving model at epoch 22 with loss 2.5151\n",
      "Epoch 23, Loss: 2.4186, Val Loss: 2.1050\n",
      "Saving model at epoch 23 with loss 2.4186\n",
      "Epoch 24, Loss: 2.3324, Val Loss: 2.0286\n",
      "Saving model at epoch 24 with loss 2.3324\n",
      "Epoch 25, Loss: 2.2545, Val Loss: 1.9604\n",
      "Saving model at epoch 25 with loss 2.2545\n",
      "Epoch 26, Loss: 2.1842, Val Loss: 1.8994\n",
      "Saving model at epoch 26 with loss 2.1842\n",
      "Epoch 27, Loss: 2.1204, Val Loss: 1.8444\n",
      "Saving model at epoch 27 with loss 2.1204\n",
      "Epoch 28, Loss: 2.0621, Val Loss: 1.7947\n",
      "Saving model at epoch 28 with loss 2.0621\n",
      "Epoch 29, Loss: 2.0091, Val Loss: 1.7495\n",
      "Saving model at epoch 29 with loss 2.0091\n",
      "Epoch 30, Loss: 1.9604, Val Loss: 1.7082\n",
      "Saving model at epoch 30 with loss 1.9604\n",
      "Epoch 31, Loss: 1.9153, Val Loss: 1.6703\n",
      "Saving model at epoch 31 with loss 1.9153\n",
      "Epoch 32, Loss: 1.8737, Val Loss: 1.6354\n",
      "Saving model at epoch 32 with loss 1.8737\n",
      "Epoch 33, Loss: 1.8350, Val Loss: 1.6031\n",
      "Saving model at epoch 33 with loss 1.8350\n",
      "Epoch 34, Loss: 1.7991, Val Loss: 1.5732\n",
      "Saving model at epoch 34 with loss 1.7991\n",
      "Epoch 35, Loss: 1.7654, Val Loss: 1.5453\n",
      "Saving model at epoch 35 with loss 1.7654\n",
      "Epoch 36, Loss: 1.7339, Val Loss: 1.5192\n",
      "Saving model at epoch 36 with loss 1.7339\n",
      "Epoch 37, Loss: 1.7043, Val Loss: 1.4948\n",
      "Saving model at epoch 37 with loss 1.7043\n",
      "Epoch 38, Loss: 1.6766, Val Loss: 1.4718\n",
      "Saving model at epoch 38 with loss 1.6766\n",
      "Epoch 39, Loss: 1.6505, Val Loss: 1.4502\n",
      "Saving model at epoch 39 with loss 1.6505\n",
      "Epoch 40, Loss: 1.6258, Val Loss: 1.4297\n",
      "Saving model at epoch 40 with loss 1.6258\n",
      "Epoch 41, Loss: 1.6024, Val Loss: 1.4103\n",
      "Saving model at epoch 41 with loss 1.6024\n",
      "Epoch 42, Loss: 1.5801, Val Loss: 1.3920\n",
      "Saving model at epoch 42 with loss 1.5801\n",
      "Epoch 43, Loss: 1.5589, Val Loss: 1.3746\n",
      "Saving model at epoch 43 with loss 1.5589\n",
      "Epoch 44, Loss: 1.5387, Val Loss: 1.3580\n",
      "Saving model at epoch 44 with loss 1.5387\n",
      "Epoch 45, Loss: 1.5194, Val Loss: 1.3421\n",
      "Saving model at epoch 45 with loss 1.5194\n",
      "Epoch 46, Loss: 1.5009, Val Loss: 1.3270\n",
      "Saving model at epoch 46 with loss 1.5009\n",
      "Epoch 47, Loss: 1.4832, Val Loss: 1.3124\n",
      "Saving model at epoch 47 with loss 1.4832\n",
      "Epoch 48, Loss: 1.4662, Val Loss: 1.2985\n",
      "Saving model at epoch 48 with loss 1.4662\n",
      "Epoch 49, Loss: 1.4499, Val Loss: 1.2851\n",
      "Saving model at epoch 49 with loss 1.4499\n",
      "Epoch 50, Loss: 1.4341, Val Loss: 1.2722\n",
      "Saving model at epoch 50 with loss 1.4341\n",
      "Epoch 51, Loss: 1.4190, Val Loss: 1.2597\n",
      "Saving model at epoch 51 with loss 1.4190\n",
      "Epoch 52, Loss: 1.4043, Val Loss: 1.2477\n",
      "Saving model at epoch 52 with loss 1.4043\n",
      "Epoch 53, Loss: 1.3901, Val Loss: 1.2361\n",
      "Saving model at epoch 53 with loss 1.3901\n",
      "Epoch 54, Loss: 1.3764, Val Loss: 1.2249\n",
      "Saving model at epoch 54 with loss 1.3764\n",
      "Epoch 55, Loss: 1.3632, Val Loss: 1.2140\n",
      "Saving model at epoch 55 with loss 1.3632\n",
      "Epoch 56, Loss: 1.3503, Val Loss: 1.2035\n",
      "Saving model at epoch 56 with loss 1.3503\n",
      "Epoch 57, Loss: 1.3378, Val Loss: 1.1932\n",
      "Saving model at epoch 57 with loss 1.3378\n",
      "Epoch 58, Loss: 1.3257, Val Loss: 1.1833\n",
      "Saving model at epoch 58 with loss 1.3257\n",
      "Epoch 59, Loss: 1.3140, Val Loss: 1.1737\n",
      "Saving model at epoch 59 with loss 1.3140\n",
      "Epoch 60, Loss: 1.3026, Val Loss: 1.1643\n",
      "Saving model at epoch 60 with loss 1.3026\n",
      "Epoch 61, Loss: 1.2915, Val Loss: 1.1552\n",
      "Saving model at epoch 61 with loss 1.2915\n",
      "Epoch 62, Loss: 1.2807, Val Loss: 1.1463\n",
      "Saving model at epoch 62 with loss 1.2807\n",
      "Epoch 63, Loss: 1.2703, Val Loss: 1.1377\n",
      "Saving model at epoch 63 with loss 1.2703\n",
      "Epoch 64, Loss: 1.2600, Val Loss: 1.1292\n",
      "Saving model at epoch 64 with loss 1.2600\n",
      "Epoch 65, Loss: 1.2501, Val Loss: 1.1210\n",
      "Saving model at epoch 65 with loss 1.2501\n",
      "Epoch 66, Loss: 1.2404, Val Loss: 1.1130\n",
      "Saving model at epoch 66 with loss 1.2404\n",
      "Epoch 67, Loss: 1.2309, Val Loss: 1.1052\n",
      "Saving model at epoch 67 with loss 1.2309\n",
      "Epoch 68, Loss: 1.2217, Val Loss: 1.0976\n",
      "Saving model at epoch 68 with loss 1.2217\n",
      "Epoch 69, Loss: 1.2127, Val Loss: 1.0901\n",
      "Saving model at epoch 69 with loss 1.2127\n",
      "Epoch 70, Loss: 1.2039, Val Loss: 1.0828\n",
      "Saving model at epoch 70 with loss 1.2039\n",
      "Epoch 71, Loss: 1.1953, Val Loss: 1.0757\n",
      "Saving model at epoch 71 with loss 1.1953\n",
      "Epoch 72, Loss: 1.1869, Val Loss: 1.0688\n",
      "Saving model at epoch 72 with loss 1.1869\n",
      "Epoch 73, Loss: 1.1788, Val Loss: 1.0620\n",
      "Saving model at epoch 73 with loss 1.1788\n",
      "Epoch 74, Loss: 1.1708, Val Loss: 1.0553\n",
      "Saving model at epoch 74 with loss 1.1708\n",
      "Epoch 75, Loss: 1.1631, Val Loss: 1.0488\n",
      "Saving model at epoch 75 with loss 1.1631\n",
      "Epoch 76, Loss: 1.1555, Val Loss: 1.0425\n",
      "Saving model at epoch 76 with loss 1.1555\n",
      "Epoch 77, Loss: 1.1481, Val Loss: 1.0362\n",
      "Saving model at epoch 77 with loss 1.1481\n",
      "Epoch 78, Loss: 1.1408, Val Loss: 1.0301\n",
      "Saving model at epoch 78 with loss 1.1408\n",
      "Epoch 79, Loss: 1.1337, Val Loss: 1.0241\n",
      "Saving model at epoch 79 with loss 1.1337\n",
      "Epoch 80, Loss: 1.1268, Val Loss: 1.0183\n",
      "Saving model at epoch 80 with loss 1.1268\n",
      "Epoch 81, Loss: 1.1200, Val Loss: 1.0126\n",
      "Saving model at epoch 81 with loss 1.1200\n",
      "Epoch 82, Loss: 1.1133, Val Loss: 1.0070\n",
      "Saving model at epoch 82 with loss 1.1133\n",
      "Epoch 83, Loss: 1.1068, Val Loss: 1.0014\n",
      "Saving model at epoch 83 with loss 1.1068\n",
      "Epoch 84, Loss: 1.1003, Val Loss: 0.9960\n",
      "Saving model at epoch 84 with loss 1.1003\n",
      "Epoch 85, Loss: 1.0940, Val Loss: 0.9907\n",
      "Saving model at epoch 85 with loss 1.0940\n",
      "Epoch 86, Loss: 1.0879, Val Loss: 0.9855\n",
      "Saving model at epoch 86 with loss 1.0879\n",
      "Epoch 87, Loss: 1.0818, Val Loss: 0.9804\n",
      "Saving model at epoch 87 with loss 1.0818\n",
      "Epoch 88, Loss: 1.0758, Val Loss: 0.9754\n",
      "Saving model at epoch 88 with loss 1.0758\n",
      "Epoch 89, Loss: 1.0700, Val Loss: 0.9704\n",
      "Saving model at epoch 89 with loss 1.0700\n",
      "Epoch 90, Loss: 1.0642, Val Loss: 0.9656\n",
      "Saving model at epoch 90 with loss 1.0642\n",
      "Epoch 91, Loss: 1.0586, Val Loss: 0.9608\n",
      "Saving model at epoch 91 with loss 1.0586\n",
      "Epoch 92, Loss: 1.0531, Val Loss: 0.9562\n",
      "Saving model at epoch 92 with loss 1.0531\n",
      "Epoch 93, Loss: 1.0476, Val Loss: 0.9516\n",
      "Saving model at epoch 93 with loss 1.0476\n",
      "Epoch 94, Loss: 1.0423, Val Loss: 0.9470\n",
      "Saving model at epoch 94 with loss 1.0423\n",
      "Epoch 95, Loss: 1.0370, Val Loss: 0.9426\n",
      "Saving model at epoch 95 with loss 1.0370\n",
      "Epoch 96, Loss: 1.0318, Val Loss: 0.9382\n",
      "Saving model at epoch 96 with loss 1.0318\n",
      "Epoch 97, Loss: 1.0268, Val Loss: 0.9339\n",
      "Saving model at epoch 97 with loss 1.0268\n",
      "Epoch 98, Loss: 1.0218, Val Loss: 0.9297\n",
      "Saving model at epoch 98 with loss 1.0218\n",
      "Epoch 99, Loss: 1.0168, Val Loss: 0.9255\n",
      "Saving model at epoch 99 with loss 1.0168\n",
      "Epoch 100, Loss: 1.0120, Val Loss: 0.9214\n",
      "Saving model at epoch 100 with loss 1.0120\n",
      "Epoch 101, Loss: 1.0073, Val Loss: 0.9174\n",
      "Saving model at epoch 101 with loss 1.0073\n",
      "Epoch 102, Loss: 1.0026, Val Loss: 0.9135\n",
      "Saving model at epoch 102 with loss 1.0026\n",
      "Epoch 103, Loss: 0.9980, Val Loss: 0.9096\n",
      "Saving model at epoch 103 with loss 0.9980\n",
      "Epoch 104, Loss: 0.9935, Val Loss: 0.9057\n",
      "Saving model at epoch 104 with loss 0.9935\n",
      "Epoch 105, Loss: 0.9890, Val Loss: 0.9020\n",
      "Saving model at epoch 105 with loss 0.9890\n",
      "Epoch 106, Loss: 0.9846, Val Loss: 0.8982\n",
      "Saving model at epoch 106 with loss 0.9846\n",
      "Epoch 107, Loss: 0.9803, Val Loss: 0.8946\n",
      "Saving model at epoch 107 with loss 0.9803\n",
      "Epoch 108, Loss: 0.9760, Val Loss: 0.8910\n",
      "Saving model at epoch 108 with loss 0.9760\n",
      "Epoch 109, Loss: 0.9718, Val Loss: 0.8874\n",
      "Saving model at epoch 109 with loss 0.9718\n",
      "Epoch 110, Loss: 0.9677, Val Loss: 0.8839\n",
      "Saving model at epoch 110 with loss 0.9677\n",
      "Epoch 111, Loss: 0.9636, Val Loss: 0.8804\n",
      "Saving model at epoch 111 with loss 0.9636\n",
      "Epoch 112, Loss: 0.9596, Val Loss: 0.8770\n",
      "Saving model at epoch 112 with loss 0.9596\n",
      "Epoch 113, Loss: 0.9557, Val Loss: 0.8737\n",
      "Saving model at epoch 113 with loss 0.9557\n",
      "Epoch 114, Loss: 0.9518, Val Loss: 0.8704\n",
      "Saving model at epoch 114 with loss 0.9518\n",
      "Epoch 115, Loss: 0.9480, Val Loss: 0.8671\n",
      "Saving model at epoch 115 with loss 0.9480\n",
      "Epoch 116, Loss: 0.9442, Val Loss: 0.8639\n",
      "Saving model at epoch 116 with loss 0.9442\n",
      "Epoch 117, Loss: 0.9405, Val Loss: 0.8607\n",
      "Saving model at epoch 117 with loss 0.9405\n",
      "Epoch 118, Loss: 0.9368, Val Loss: 0.8576\n",
      "Saving model at epoch 118 with loss 0.9368\n",
      "Epoch 119, Loss: 0.9333, Val Loss: 0.8546\n",
      "Saving model at epoch 119 with loss 0.9333\n",
      "Epoch 120, Loss: 0.9297, Val Loss: 0.8515\n",
      "Saving model at epoch 120 with loss 0.9297\n",
      "Epoch 121, Loss: 0.9262, Val Loss: 0.8485\n",
      "Saving model at epoch 121 with loss 0.9262\n",
      "Epoch 122, Loss: 0.9228, Val Loss: 0.8456\n",
      "Saving model at epoch 122 with loss 0.9228\n",
      "Epoch 123, Loss: 0.9194, Val Loss: 0.8427\n",
      "Saving model at epoch 123 with loss 0.9194\n",
      "Epoch 124, Loss: 0.9160, Val Loss: 0.8398\n",
      "Saving model at epoch 124 with loss 0.9160\n",
      "Epoch 125, Loss: 0.9127, Val Loss: 0.8370\n",
      "Saving model at epoch 125 with loss 0.9127\n",
      "Epoch 126, Loss: 0.9095, Val Loss: 0.8342\n",
      "Saving model at epoch 126 with loss 0.9095\n",
      "Epoch 127, Loss: 0.9063, Val Loss: 0.8314\n",
      "Saving model at epoch 127 with loss 0.9063\n",
      "Epoch 128, Loss: 0.9031, Val Loss: 0.8287\n",
      "Saving model at epoch 128 with loss 0.9031\n",
      "Epoch 129, Loss: 0.9000, Val Loss: 0.8260\n",
      "Saving model at epoch 129 with loss 0.9000\n",
      "Epoch 130, Loss: 0.8969, Val Loss: 0.8234\n",
      "Saving model at epoch 130 with loss 0.8969\n",
      "Epoch 131, Loss: 0.8938, Val Loss: 0.8208\n",
      "Saving model at epoch 131 with loss 0.8938\n",
      "Epoch 132, Loss: 0.8908, Val Loss: 0.8182\n",
      "Saving model at epoch 132 with loss 0.8908\n",
      "Epoch 133, Loss: 0.8879, Val Loss: 0.8157\n",
      "Saving model at epoch 133 with loss 0.8879\n",
      "Epoch 134, Loss: 0.8849, Val Loss: 0.8132\n",
      "Saving model at epoch 134 with loss 0.8849\n",
      "Epoch 135, Loss: 0.8820, Val Loss: 0.8107\n",
      "Saving model at epoch 135 with loss 0.8820\n",
      "Epoch 136, Loss: 0.8792, Val Loss: 0.8082\n",
      "Saving model at epoch 136 with loss 0.8792\n",
      "Epoch 137, Loss: 0.8764, Val Loss: 0.8058\n",
      "Saving model at epoch 137 with loss 0.8764\n",
      "Epoch 138, Loss: 0.8736, Val Loss: 0.8035\n",
      "Saving model at epoch 138 with loss 0.8736\n",
      "Epoch 139, Loss: 0.8709, Val Loss: 0.8011\n",
      "Saving model at epoch 139 with loss 0.8709\n",
      "Epoch 140, Loss: 0.8682, Val Loss: 0.7988\n",
      "Saving model at epoch 140 with loss 0.8682\n",
      "Epoch 141, Loss: 0.8655, Val Loss: 0.7966\n",
      "Saving model at epoch 141 with loss 0.8655\n",
      "Epoch 142, Loss: 0.8629, Val Loss: 0.7943\n",
      "Saving model at epoch 142 with loss 0.8629\n",
      "Epoch 143, Loss: 0.8603, Val Loss: 0.7921\n",
      "Saving model at epoch 143 with loss 0.8603\n",
      "Epoch 144, Loss: 0.8577, Val Loss: 0.7899\n",
      "Saving model at epoch 144 with loss 0.8577\n",
      "Epoch 145, Loss: 0.8551, Val Loss: 0.7877\n",
      "Saving model at epoch 145 with loss 0.8551\n",
      "Epoch 146, Loss: 0.8526, Val Loss: 0.7855\n",
      "Saving model at epoch 146 with loss 0.8526\n",
      "Epoch 147, Loss: 0.8502, Val Loss: 0.7834\n",
      "Saving model at epoch 147 with loss 0.8502\n",
      "Epoch 148, Loss: 0.8477, Val Loss: 0.7813\n",
      "Saving model at epoch 148 with loss 0.8477\n",
      "Epoch 149, Loss: 0.8453, Val Loss: 0.7792\n",
      "Saving model at epoch 149 with loss 0.8453\n",
      "Epoch 150, Loss: 0.8429, Val Loss: 0.7772\n",
      "Saving model at epoch 150 with loss 0.8429\n",
      "Epoch 151, Loss: 0.8405, Val Loss: 0.7752\n",
      "Saving model at epoch 151 with loss 0.8405\n",
      "Epoch 152, Loss: 0.8382, Val Loss: 0.7732\n",
      "Saving model at epoch 152 with loss 0.8382\n",
      "Epoch 153, Loss: 0.8359, Val Loss: 0.7712\n",
      "Saving model at epoch 153 with loss 0.8359\n",
      "Epoch 154, Loss: 0.8336, Val Loss: 0.7692\n",
      "Saving model at epoch 154 with loss 0.8336\n",
      "Epoch 155, Loss: 0.8314, Val Loss: 0.7673\n",
      "Saving model at epoch 155 with loss 0.8314\n",
      "Epoch 156, Loss: 0.8292, Val Loss: 0.7654\n",
      "Saving model at epoch 156 with loss 0.8292\n",
      "Epoch 157, Loss: 0.8270, Val Loss: 0.7635\n",
      "Saving model at epoch 157 with loss 0.8270\n",
      "Epoch 158, Loss: 0.8248, Val Loss: 0.7616\n",
      "Saving model at epoch 158 with loss 0.8248\n",
      "Epoch 159, Loss: 0.8227, Val Loss: 0.7598\n",
      "Saving model at epoch 159 with loss 0.8227\n",
      "Epoch 160, Loss: 0.8205, Val Loss: 0.7579\n",
      "Saving model at epoch 160 with loss 0.8205\n",
      "Epoch 161, Loss: 0.8184, Val Loss: 0.7561\n",
      "Saving model at epoch 161 with loss 0.8184\n",
      "Epoch 162, Loss: 0.8164, Val Loss: 0.7543\n",
      "Saving model at epoch 162 with loss 0.8164\n",
      "Epoch 163, Loss: 0.8143, Val Loss: 0.7526\n",
      "Saving model at epoch 163 with loss 0.8143\n",
      "Epoch 164, Loss: 0.8123, Val Loss: 0.7508\n",
      "Saving model at epoch 164 with loss 0.8123\n",
      "Epoch 165, Loss: 0.8103, Val Loss: 0.7491\n",
      "Saving model at epoch 165 with loss 0.8103\n",
      "Epoch 166, Loss: 0.8083, Val Loss: 0.7474\n",
      "Saving model at epoch 166 with loss 0.8083\n",
      "Epoch 167, Loss: 0.8063, Val Loss: 0.7457\n",
      "Saving model at epoch 167 with loss 0.8063\n",
      "Epoch 168, Loss: 0.8044, Val Loss: 0.7440\n",
      "Saving model at epoch 168 with loss 0.8044\n",
      "Epoch 169, Loss: 0.8025, Val Loss: 0.7424\n",
      "Saving model at epoch 169 with loss 0.8025\n",
      "Epoch 170, Loss: 0.8006, Val Loss: 0.7407\n",
      "Saving model at epoch 170 with loss 0.8006\n",
      "Epoch 171, Loss: 0.7987, Val Loss: 0.7391\n",
      "Saving model at epoch 171 with loss 0.7987\n",
      "Epoch 172, Loss: 0.7969, Val Loss: 0.7375\n",
      "Saving model at epoch 172 with loss 0.7969\n",
      "Epoch 173, Loss: 0.7950, Val Loss: 0.7359\n",
      "Saving model at epoch 173 with loss 0.7950\n",
      "Epoch 174, Loss: 0.7932, Val Loss: 0.7343\n",
      "Saving model at epoch 174 with loss 0.7932\n",
      "Epoch 175, Loss: 0.7914, Val Loss: 0.7328\n",
      "Saving model at epoch 175 with loss 0.7914\n",
      "Epoch 176, Loss: 0.7896, Val Loss: 0.7312\n",
      "Saving model at epoch 176 with loss 0.7896\n",
      "Epoch 177, Loss: 0.7878, Val Loss: 0.7297\n",
      "Saving model at epoch 177 with loss 0.7878\n",
      "Epoch 178, Loss: 0.7861, Val Loss: 0.7282\n",
      "Saving model at epoch 178 with loss 0.7861\n",
      "Epoch 179, Loss: 0.7843, Val Loss: 0.7267\n",
      "Saving model at epoch 179 with loss 0.7843\n",
      "Epoch 180, Loss: 0.7826, Val Loss: 0.7252\n",
      "Saving model at epoch 180 with loss 0.7826\n",
      "Epoch 181, Loss: 0.7809, Val Loss: 0.7238\n",
      "Saving model at epoch 181 with loss 0.7809\n",
      "Epoch 182, Loss: 0.7793, Val Loss: 0.7223\n",
      "Saving model at epoch 182 with loss 0.7793\n",
      "Epoch 183, Loss: 0.7776, Val Loss: 0.7209\n",
      "Saving model at epoch 183 with loss 0.7776\n",
      "Epoch 184, Loss: 0.7760, Val Loss: 0.7195\n",
      "Saving model at epoch 184 with loss 0.7760\n",
      "Epoch 185, Loss: 0.7744, Val Loss: 0.7181\n",
      "Saving model at epoch 185 with loss 0.7744\n",
      "Epoch 186, Loss: 0.7727, Val Loss: 0.7167\n",
      "Saving model at epoch 186 with loss 0.7727\n",
      "Epoch 187, Loss: 0.7711, Val Loss: 0.7153\n",
      "Saving model at epoch 187 with loss 0.7711\n",
      "Epoch 188, Loss: 0.7696, Val Loss: 0.7140\n",
      "Saving model at epoch 188 with loss 0.7696\n",
      "Epoch 189, Loss: 0.7680, Val Loss: 0.7126\n",
      "Saving model at epoch 189 with loss 0.7680\n",
      "Epoch 190, Loss: 0.7665, Val Loss: 0.7113\n",
      "Saving model at epoch 190 with loss 0.7665\n",
      "Epoch 191, Loss: 0.7649, Val Loss: 0.7099\n",
      "Saving model at epoch 191 with loss 0.7649\n",
      "Epoch 192, Loss: 0.7634, Val Loss: 0.7086\n",
      "Saving model at epoch 192 with loss 0.7634\n",
      "Epoch 193, Loss: 0.7619, Val Loss: 0.7073\n",
      "Saving model at epoch 193 with loss 0.7619\n",
      "Epoch 194, Loss: 0.7605, Val Loss: 0.7061\n",
      "Saving model at epoch 194 with loss 0.7605\n",
      "Epoch 195, Loss: 0.7590, Val Loss: 0.7048\n",
      "Saving model at epoch 195 with loss 0.7590\n",
      "Epoch 196, Loss: 0.7575, Val Loss: 0.7035\n",
      "Saving model at epoch 196 with loss 0.7575\n",
      "Epoch 197, Loss: 0.7561, Val Loss: 0.7023\n",
      "Saving model at epoch 197 with loss 0.7561\n",
      "Epoch 198, Loss: 0.7547, Val Loss: 0.7010\n",
      "Saving model at epoch 198 with loss 0.7547\n",
      "Epoch 199, Loss: 0.7533, Val Loss: 0.6998\n",
      "Saving model at epoch 199 with loss 0.7533\n",
      "Epoch 200, Loss: 0.7519, Val Loss: 0.6986\n",
      "Saving model at epoch 200 with loss 0.7519\n",
      "Epoch 201, Loss: 0.7505, Val Loss: 0.6974\n",
      "Saving model at epoch 201 with loss 0.7505\n",
      "Epoch 202, Loss: 0.7491, Val Loss: 0.6962\n",
      "Saving model at epoch 202 with loss 0.7491\n",
      "Epoch 203, Loss: 0.7477, Val Loss: 0.6950\n",
      "Saving model at epoch 203 with loss 0.7477\n",
      "Epoch 204, Loss: 0.7464, Val Loss: 0.6939\n",
      "Saving model at epoch 204 with loss 0.7464\n",
      "Epoch 205, Loss: 0.7451, Val Loss: 0.6927\n",
      "Saving model at epoch 205 with loss 0.7451\n",
      "Epoch 206, Loss: 0.7437, Val Loss: 0.6915\n",
      "Saving model at epoch 206 with loss 0.7437\n",
      "Epoch 207, Loss: 0.7424, Val Loss: 0.6904\n",
      "Saving model at epoch 207 with loss 0.7424\n",
      "Epoch 208, Loss: 0.7411, Val Loss: 0.6893\n",
      "Saving model at epoch 208 with loss 0.7411\n",
      "Epoch 209, Loss: 0.7398, Val Loss: 0.6882\n",
      "Saving model at epoch 209 with loss 0.7398\n",
      "Epoch 210, Loss: 0.7386, Val Loss: 0.6871\n",
      "Saving model at epoch 210 with loss 0.7386\n",
      "Epoch 211, Loss: 0.7373, Val Loss: 0.6860\n",
      "Saving model at epoch 211 with loss 0.7373\n",
      "Epoch 212, Loss: 0.7361, Val Loss: 0.6849\n",
      "Saving model at epoch 212 with loss 0.7361\n",
      "Epoch 213, Loss: 0.7348, Val Loss: 0.6838\n",
      "Saving model at epoch 213 with loss 0.7348\n",
      "Epoch 214, Loss: 0.7336, Val Loss: 0.6827\n",
      "Saving model at epoch 214 with loss 0.7336\n",
      "Epoch 215, Loss: 0.7324, Val Loss: 0.6817\n",
      "Saving model at epoch 215 with loss 0.7324\n",
      "Epoch 216, Loss: 0.7312, Val Loss: 0.6806\n",
      "Saving model at epoch 216 with loss 0.7312\n",
      "Epoch 217, Loss: 0.7300, Val Loss: 0.6796\n",
      "Saving model at epoch 217 with loss 0.7300\n",
      "Epoch 218, Loss: 0.7288, Val Loss: 0.6785\n",
      "Saving model at epoch 218 with loss 0.7288\n",
      "Epoch 219, Loss: 0.7276, Val Loss: 0.6775\n",
      "Saving model at epoch 219 with loss 0.7276\n",
      "Epoch 220, Loss: 0.7265, Val Loss: 0.6765\n",
      "Saving model at epoch 220 with loss 0.7265\n",
      "Epoch 221, Loss: 0.7253, Val Loss: 0.6755\n",
      "Saving model at epoch 221 with loss 0.7253\n",
      "Epoch 222, Loss: 0.7242, Val Loss: 0.6745\n",
      "Saving model at epoch 222 with loss 0.7242\n",
      "Epoch 223, Loss: 0.7230, Val Loss: 0.6735\n",
      "Saving model at epoch 223 with loss 0.7230\n",
      "Epoch 224, Loss: 0.7219, Val Loss: 0.6725\n",
      "Saving model at epoch 224 with loss 0.7219\n",
      "Epoch 225, Loss: 0.7208, Val Loss: 0.6716\n",
      "Saving model at epoch 225 with loss 0.7208\n",
      "Epoch 226, Loss: 0.7197, Val Loss: 0.6706\n",
      "Saving model at epoch 226 with loss 0.7197\n",
      "Epoch 227, Loss: 0.7186, Val Loss: 0.6697\n",
      "Saving model at epoch 227 with loss 0.7186\n",
      "Epoch 228, Loss: 0.7175, Val Loss: 0.6687\n",
      "Saving model at epoch 228 with loss 0.7175\n",
      "Epoch 229, Loss: 0.7165, Val Loss: 0.6678\n",
      "Saving model at epoch 229 with loss 0.7165\n",
      "Epoch 230, Loss: 0.7154, Val Loss: 0.6669\n",
      "Saving model at epoch 230 with loss 0.7154\n",
      "Epoch 231, Loss: 0.7144, Val Loss: 0.6659\n",
      "Saving model at epoch 231 with loss 0.7144\n",
      "Epoch 232, Loss: 0.7133, Val Loss: 0.6650\n",
      "Saving model at epoch 232 with loss 0.7133\n",
      "Epoch 233, Loss: 0.7123, Val Loss: 0.6641\n",
      "Saving model at epoch 233 with loss 0.7123\n",
      "Epoch 234, Loss: 0.7112, Val Loss: 0.6632\n",
      "Saving model at epoch 234 with loss 0.7112\n",
      "Epoch 235, Loss: 0.7102, Val Loss: 0.6623\n",
      "Saving model at epoch 235 with loss 0.7102\n",
      "Epoch 236, Loss: 0.7092, Val Loss: 0.6614\n",
      "Saving model at epoch 236 with loss 0.7092\n",
      "Epoch 237, Loss: 0.7082, Val Loss: 0.6606\n",
      "Saving model at epoch 237 with loss 0.7082\n",
      "Epoch 238, Loss: 0.7072, Val Loss: 0.6597\n",
      "Saving model at epoch 238 with loss 0.7072\n",
      "Epoch 239, Loss: 0.7062, Val Loss: 0.6588\n",
      "Saving model at epoch 239 with loss 0.7062\n",
      "Epoch 240, Loss: 0.7052, Val Loss: 0.6580\n",
      "Saving model at epoch 240 with loss 0.7052\n",
      "Epoch 241, Loss: 0.7043, Val Loss: 0.6571\n",
      "Saving model at epoch 241 with loss 0.7043\n",
      "Epoch 242, Loss: 0.7033, Val Loss: 0.6563\n",
      "Saving model at epoch 242 with loss 0.7033\n",
      "Epoch 243, Loss: 0.7024, Val Loss: 0.6555\n",
      "Saving model at epoch 243 with loss 0.7024\n",
      "Epoch 244, Loss: 0.7014, Val Loss: 0.6546\n",
      "Saving model at epoch 244 with loss 0.7014\n",
      "Epoch 245, Loss: 0.7005, Val Loss: 0.6538\n",
      "Saving model at epoch 245 with loss 0.7005\n",
      "Epoch 246, Loss: 0.6995, Val Loss: 0.6530\n",
      "Saving model at epoch 246 with loss 0.6995\n",
      "Epoch 247, Loss: 0.6986, Val Loss: 0.6522\n",
      "Saving model at epoch 247 with loss 0.6986\n",
      "Epoch 248, Loss: 0.6977, Val Loss: 0.6514\n",
      "Saving model at epoch 248 with loss 0.6977\n",
      "Epoch 249, Loss: 0.6968, Val Loss: 0.6506\n",
      "Saving model at epoch 249 with loss 0.6968\n",
      "Epoch 250, Loss: 0.6959, Val Loss: 0.6498\n",
      "Saving model at epoch 250 with loss 0.6959\n",
      "Epoch 251, Loss: 0.6950, Val Loss: 0.6490\n",
      "Saving model at epoch 251 with loss 0.6950\n",
      "Epoch 252, Loss: 0.6941, Val Loss: 0.6483\n",
      "Saving model at epoch 252 with loss 0.6941\n",
      "Epoch 253, Loss: 0.6932, Val Loss: 0.6475\n",
      "Saving model at epoch 253 with loss 0.6932\n",
      "Epoch 254, Loss: 0.6923, Val Loss: 0.6467\n",
      "Saving model at epoch 254 with loss 0.6923\n",
      "Epoch 255, Loss: 0.6915, Val Loss: 0.6460\n",
      "Saving model at epoch 255 with loss 0.6915\n",
      "Epoch 256, Loss: 0.6906, Val Loss: 0.6452\n",
      "Saving model at epoch 256 with loss 0.6906\n",
      "Epoch 257, Loss: 0.6898, Val Loss: 0.6445\n",
      "Saving model at epoch 257 with loss 0.6898\n",
      "Epoch 258, Loss: 0.6889, Val Loss: 0.6437\n",
      "Saving model at epoch 258 with loss 0.6889\n",
      "Epoch 259, Loss: 0.6881, Val Loss: 0.6430\n",
      "Saving model at epoch 259 with loss 0.6881\n",
      "Epoch 260, Loss: 0.6872, Val Loss: 0.6422\n",
      "Saving model at epoch 260 with loss 0.6872\n",
      "Epoch 261, Loss: 0.6864, Val Loss: 0.6415\n",
      "Saving model at epoch 261 with loss 0.6864\n",
      "Epoch 262, Loss: 0.6856, Val Loss: 0.6408\n",
      "Saving model at epoch 262 with loss 0.6856\n",
      "Epoch 263, Loss: 0.6848, Val Loss: 0.6401\n",
      "Saving model at epoch 263 with loss 0.6848\n",
      "Epoch 264, Loss: 0.6839, Val Loss: 0.6394\n",
      "Saving model at epoch 264 with loss 0.6839\n",
      "Epoch 265, Loss: 0.6831, Val Loss: 0.6387\n",
      "Saving model at epoch 265 with loss 0.6831\n",
      "Epoch 266, Loss: 0.6823, Val Loss: 0.6380\n",
      "Saving model at epoch 266 with loss 0.6823\n",
      "Epoch 267, Loss: 0.6815, Val Loss: 0.6373\n",
      "Saving model at epoch 267 with loss 0.6815\n",
      "Epoch 268, Loss: 0.6808, Val Loss: 0.6366\n",
      "Saving model at epoch 268 with loss 0.6808\n",
      "Epoch 269, Loss: 0.6800, Val Loss: 0.6359\n",
      "Saving model at epoch 269 with loss 0.6800\n",
      "Epoch 270, Loss: 0.6792, Val Loss: 0.6352\n",
      "Saving model at epoch 270 with loss 0.6792\n",
      "Epoch 271, Loss: 0.6784, Val Loss: 0.6345\n",
      "Saving model at epoch 271 with loss 0.6784\n",
      "Epoch 272, Loss: 0.6776, Val Loss: 0.6339\n",
      "Saving model at epoch 272 with loss 0.6776\n",
      "Epoch 273, Loss: 0.6769, Val Loss: 0.6332\n",
      "Saving model at epoch 273 with loss 0.6769\n",
      "Epoch 274, Loss: 0.6761, Val Loss: 0.6325\n",
      "Saving model at epoch 274 with loss 0.6761\n",
      "Epoch 275, Loss: 0.6754, Val Loss: 0.6319\n",
      "Saving model at epoch 275 with loss 0.6754\n",
      "Epoch 276, Loss: 0.6746, Val Loss: 0.6312\n",
      "Saving model at epoch 276 with loss 0.6746\n",
      "Epoch 277, Loss: 0.6739, Val Loss: 0.6306\n",
      "Saving model at epoch 277 with loss 0.6739\n",
      "Epoch 278, Loss: 0.6732, Val Loss: 0.6299\n",
      "Saving model at epoch 278 with loss 0.6732\n",
      "Epoch 279, Loss: 0.6724, Val Loss: 0.6293\n",
      "Saving model at epoch 279 with loss 0.6724\n",
      "Epoch 280, Loss: 0.6717, Val Loss: 0.6287\n",
      "Saving model at epoch 280 with loss 0.6717\n",
      "Epoch 281, Loss: 0.6710, Val Loss: 0.6280\n",
      "Saving model at epoch 281 with loss 0.6710\n",
      "Epoch 282, Loss: 0.6703, Val Loss: 0.6274\n",
      "Saving model at epoch 282 with loss 0.6703\n",
      "Epoch 283, Loss: 0.6695, Val Loss: 0.6268\n",
      "Saving model at epoch 283 with loss 0.6695\n",
      "Epoch 284, Loss: 0.6688, Val Loss: 0.6262\n",
      "Saving model at epoch 284 with loss 0.6688\n",
      "Epoch 285, Loss: 0.6681, Val Loss: 0.6255\n",
      "Saving model at epoch 285 with loss 0.6681\n",
      "Epoch 286, Loss: 0.6674, Val Loss: 0.6249\n",
      "Saving model at epoch 286 with loss 0.6674\n",
      "Epoch 287, Loss: 0.6668, Val Loss: 0.6243\n",
      "Saving model at epoch 287 with loss 0.6668\n",
      "Epoch 288, Loss: 0.6661, Val Loss: 0.6237\n",
      "Saving model at epoch 288 with loss 0.6661\n",
      "Epoch 289, Loss: 0.6654, Val Loss: 0.6231\n",
      "Saving model at epoch 289 with loss 0.6654\n",
      "Epoch 290, Loss: 0.6647, Val Loss: 0.6225\n",
      "Saving model at epoch 290 with loss 0.6647\n",
      "Epoch 291, Loss: 0.6640, Val Loss: 0.6220\n",
      "Saving model at epoch 291 with loss 0.6640\n",
      "Epoch 292, Loss: 0.6634, Val Loss: 0.6214\n",
      "Saving model at epoch 292 with loss 0.6634\n",
      "Epoch 293, Loss: 0.6627, Val Loss: 0.6208\n",
      "Saving model at epoch 293 with loss 0.6627\n",
      "Epoch 294, Loss: 0.6620, Val Loss: 0.6202\n",
      "Saving model at epoch 294 with loss 0.6620\n",
      "Epoch 295, Loss: 0.6614, Val Loss: 0.6196\n",
      "Saving model at epoch 295 with loss 0.6614\n",
      "Epoch 296, Loss: 0.6607, Val Loss: 0.6191\n",
      "Saving model at epoch 296 with loss 0.6607\n",
      "Epoch 297, Loss: 0.6601, Val Loss: 0.6185\n",
      "Saving model at epoch 297 with loss 0.6601\n",
      "Epoch 298, Loss: 0.6594, Val Loss: 0.6179\n",
      "Saving model at epoch 298 with loss 0.6594\n",
      "Epoch 299, Loss: 0.6588, Val Loss: 0.6174\n",
      "Saving model at epoch 299 with loss 0.6588\n",
      "Epoch 300, Loss: 0.6582, Val Loss: 0.6168\n",
      "Saving model at epoch 300 with loss 0.6582\n",
      "Epoch 301, Loss: 0.6575, Val Loss: 0.6163\n",
      "Saving model at epoch 301 with loss 0.6575\n",
      "Epoch 302, Loss: 0.6569, Val Loss: 0.6157\n",
      "Saving model at epoch 302 with loss 0.6569\n",
      "Epoch 303, Loss: 0.6563, Val Loss: 0.6152\n",
      "Saving model at epoch 303 with loss 0.6563\n",
      "Epoch 304, Loss: 0.6557, Val Loss: 0.6146\n",
      "Saving model at epoch 304 with loss 0.6557\n",
      "Epoch 305, Loss: 0.6550, Val Loss: 0.6141\n",
      "Saving model at epoch 305 with loss 0.6550\n",
      "Epoch 306, Loss: 0.6544, Val Loss: 0.6135\n",
      "Saving model at epoch 306 with loss 0.6544\n",
      "Epoch 307, Loss: 0.6538, Val Loss: 0.6130\n",
      "Saving model at epoch 307 with loss 0.6538\n",
      "Epoch 308, Loss: 0.6532, Val Loss: 0.6125\n",
      "Saving model at epoch 308 with loss 0.6532\n",
      "Epoch 309, Loss: 0.6526, Val Loss: 0.6119\n",
      "Saving model at epoch 309 with loss 0.6526\n",
      "Epoch 310, Loss: 0.6520, Val Loss: 0.6114\n",
      "Saving model at epoch 310 with loss 0.6520\n",
      "Epoch 311, Loss: 0.6514, Val Loss: 0.6109\n",
      "Saving model at epoch 311 with loss 0.6514\n",
      "Epoch 312, Loss: 0.6508, Val Loss: 0.6104\n",
      "Saving model at epoch 312 with loss 0.6508\n",
      "Epoch 313, Loss: 0.6502, Val Loss: 0.6099\n",
      "Saving model at epoch 313 with loss 0.6502\n",
      "Epoch 314, Loss: 0.6497, Val Loss: 0.6093\n",
      "Saving model at epoch 314 with loss 0.6497\n",
      "Epoch 315, Loss: 0.6491, Val Loss: 0.6088\n",
      "Saving model at epoch 315 with loss 0.6491\n",
      "Epoch 316, Loss: 0.6485, Val Loss: 0.6083\n",
      "Saving model at epoch 316 with loss 0.6485\n",
      "Epoch 317, Loss: 0.6479, Val Loss: 0.6078\n",
      "Saving model at epoch 317 with loss 0.6479\n",
      "Epoch 318, Loss: 0.6474, Val Loss: 0.6073\n",
      "Saving model at epoch 318 with loss 0.6474\n",
      "Epoch 319, Loss: 0.6468, Val Loss: 0.6068\n",
      "Saving model at epoch 319 with loss 0.6468\n",
      "Epoch 320, Loss: 0.6462, Val Loss: 0.6063\n",
      "Saving model at epoch 320 with loss 0.6462\n",
      "Epoch 321, Loss: 0.6457, Val Loss: 0.6058\n",
      "Saving model at epoch 321 with loss 0.6457\n",
      "Epoch 322, Loss: 0.6451, Val Loss: 0.6053\n",
      "Saving model at epoch 322 with loss 0.6451\n",
      "Epoch 323, Loss: 0.6446, Val Loss: 0.6049\n",
      "Saving model at epoch 323 with loss 0.6446\n",
      "Epoch 324, Loss: 0.6440, Val Loss: 0.6044\n",
      "Saving model at epoch 324 with loss 0.6440\n",
      "Epoch 325, Loss: 0.6435, Val Loss: 0.6039\n",
      "Saving model at epoch 325 with loss 0.6435\n",
      "Epoch 326, Loss: 0.6429, Val Loss: 0.6034\n",
      "Saving model at epoch 326 with loss 0.6429\n",
      "Epoch 327, Loss: 0.6424, Val Loss: 0.6029\n",
      "Saving model at epoch 327 with loss 0.6424\n",
      "Epoch 328, Loss: 0.6418, Val Loss: 0.6025\n",
      "Saving model at epoch 328 with loss 0.6418\n",
      "Epoch 329, Loss: 0.6413, Val Loss: 0.6020\n",
      "Saving model at epoch 329 with loss 0.6413\n",
      "Epoch 330, Loss: 0.6408, Val Loss: 0.6015\n",
      "Saving model at epoch 330 with loss 0.6408\n",
      "Epoch 331, Loss: 0.6402, Val Loss: 0.6010\n",
      "Saving model at epoch 331 with loss 0.6402\n",
      "Epoch 332, Loss: 0.6397, Val Loss: 0.6006\n",
      "Saving model at epoch 332 with loss 0.6397\n",
      "Epoch 333, Loss: 0.6392, Val Loss: 0.6001\n",
      "Saving model at epoch 333 with loss 0.6392\n",
      "Epoch 334, Loss: 0.6387, Val Loss: 0.5997\n",
      "Saving model at epoch 334 with loss 0.6387\n",
      "Epoch 335, Loss: 0.6381, Val Loss: 0.5992\n",
      "Saving model at epoch 335 with loss 0.6381\n",
      "Epoch 336, Loss: 0.6376, Val Loss: 0.5987\n",
      "Saving model at epoch 336 with loss 0.6376\n",
      "Epoch 337, Loss: 0.6371, Val Loss: 0.5983\n",
      "Saving model at epoch 337 with loss 0.6371\n",
      "Epoch 338, Loss: 0.6366, Val Loss: 0.5978\n",
      "Saving model at epoch 338 with loss 0.6366\n",
      "Epoch 339, Loss: 0.6361, Val Loss: 0.5974\n",
      "Saving model at epoch 339 with loss 0.6361\n",
      "Epoch 340, Loss: 0.6356, Val Loss: 0.5969\n",
      "Saving model at epoch 340 with loss 0.6356\n",
      "Epoch 341, Loss: 0.6351, Val Loss: 0.5965\n",
      "Saving model at epoch 341 with loss 0.6351\n",
      "Epoch 342, Loss: 0.6346, Val Loss: 0.5960\n",
      "Saving model at epoch 342 with loss 0.6346\n",
      "Epoch 343, Loss: 0.6341, Val Loss: 0.5956\n",
      "Saving model at epoch 343 with loss 0.6341\n",
      "Epoch 344, Loss: 0.6336, Val Loss: 0.5952\n",
      "Saving model at epoch 344 with loss 0.6336\n",
      "Epoch 345, Loss: 0.6331, Val Loss: 0.5947\n",
      "Saving model at epoch 345 with loss 0.6331\n",
      "Epoch 346, Loss: 0.6326, Val Loss: 0.5943\n",
      "Saving model at epoch 346 with loss 0.6326\n",
      "Epoch 347, Loss: 0.6321, Val Loss: 0.5939\n",
      "Saving model at epoch 347 with loss 0.6321\n",
      "Epoch 348, Loss: 0.6316, Val Loss: 0.5934\n",
      "Saving model at epoch 348 with loss 0.6316\n",
      "Epoch 349, Loss: 0.6311, Val Loss: 0.5930\n",
      "Saving model at epoch 349 with loss 0.6311\n",
      "Epoch 350, Loss: 0.6306, Val Loss: 0.5926\n",
      "Saving model at epoch 350 with loss 0.6306\n",
      "Epoch 351, Loss: 0.6302, Val Loss: 0.5922\n",
      "Saving model at epoch 351 with loss 0.6302\n",
      "Epoch 352, Loss: 0.6297, Val Loss: 0.5918\n",
      "Saving model at epoch 352 with loss 0.6297\n",
      "Epoch 353, Loss: 0.6292, Val Loss: 0.5913\n",
      "Saving model at epoch 353 with loss 0.6292\n",
      "Epoch 354, Loss: 0.6287, Val Loss: 0.5909\n",
      "Saving model at epoch 354 with loss 0.6287\n",
      "Epoch 355, Loss: 0.6283, Val Loss: 0.5905\n",
      "Saving model at epoch 355 with loss 0.6283\n",
      "Epoch 356, Loss: 0.6278, Val Loss: 0.5901\n",
      "Saving model at epoch 356 with loss 0.6278\n",
      "Epoch 357, Loss: 0.6273, Val Loss: 0.5897\n",
      "Saving model at epoch 357 with loss 0.6273\n",
      "Epoch 358, Loss: 0.6269, Val Loss: 0.5893\n",
      "Saving model at epoch 358 with loss 0.6269\n",
      "Epoch 359, Loss: 0.6264, Val Loss: 0.5889\n",
      "Saving model at epoch 359 with loss 0.6264\n",
      "Epoch 360, Loss: 0.6260, Val Loss: 0.5885\n",
      "Saving model at epoch 360 with loss 0.6260\n",
      "Epoch 361, Loss: 0.6255, Val Loss: 0.5881\n",
      "Saving model at epoch 361 with loss 0.6255\n",
      "Epoch 362, Loss: 0.6250, Val Loss: 0.5877\n",
      "Saving model at epoch 362 with loss 0.6250\n",
      "Epoch 363, Loss: 0.6246, Val Loss: 0.5873\n",
      "Saving model at epoch 363 with loss 0.6246\n",
      "Epoch 364, Loss: 0.6241, Val Loss: 0.5869\n",
      "Saving model at epoch 364 with loss 0.6241\n",
      "Epoch 365, Loss: 0.6237, Val Loss: 0.5865\n",
      "Saving model at epoch 365 with loss 0.6237\n",
      "Epoch 366, Loss: 0.6232, Val Loss: 0.5861\n",
      "Saving model at epoch 366 with loss 0.6232\n",
      "Epoch 367, Loss: 0.6228, Val Loss: 0.5857\n",
      "Saving model at epoch 367 with loss 0.6228\n",
      "Epoch 368, Loss: 0.6224, Val Loss: 0.5853\n",
      "Saving model at epoch 368 with loss 0.6224\n",
      "Epoch 369, Loss: 0.6219, Val Loss: 0.5849\n",
      "Saving model at epoch 369 with loss 0.6219\n",
      "Epoch 370, Loss: 0.6215, Val Loss: 0.5845\n",
      "Saving model at epoch 370 with loss 0.6215\n",
      "Epoch 371, Loss: 0.6210, Val Loss: 0.5841\n",
      "Saving model at epoch 371 with loss 0.6210\n",
      "Epoch 372, Loss: 0.6206, Val Loss: 0.5838\n",
      "Saving model at epoch 372 with loss 0.6206\n",
      "Epoch 373, Loss: 0.6202, Val Loss: 0.5834\n",
      "Saving model at epoch 373 with loss 0.6202\n",
      "Epoch 374, Loss: 0.6197, Val Loss: 0.5830\n",
      "Saving model at epoch 374 with loss 0.6197\n",
      "Epoch 375, Loss: 0.6193, Val Loss: 0.5826\n",
      "Saving model at epoch 375 with loss 0.6193\n",
      "Epoch 376, Loss: 0.6189, Val Loss: 0.5822\n",
      "Saving model at epoch 376 with loss 0.6189\n",
      "Epoch 377, Loss: 0.6185, Val Loss: 0.5819\n",
      "Saving model at epoch 377 with loss 0.6185\n",
      "Epoch 378, Loss: 0.6181, Val Loss: 0.5815\n",
      "Saving model at epoch 378 with loss 0.6181\n",
      "Epoch 379, Loss: 0.6176, Val Loss: 0.5811\n",
      "Saving model at epoch 379 with loss 0.6176\n",
      "Epoch 380, Loss: 0.6172, Val Loss: 0.5808\n",
      "Saving model at epoch 380 with loss 0.6172\n",
      "Epoch 381, Loss: 0.6168, Val Loss: 0.5804\n",
      "Saving model at epoch 381 with loss 0.6168\n",
      "Epoch 382, Loss: 0.6164, Val Loss: 0.5800\n",
      "Saving model at epoch 382 with loss 0.6164\n",
      "Epoch 383, Loss: 0.6160, Val Loss: 0.5797\n",
      "Saving model at epoch 383 with loss 0.6160\n",
      "Epoch 384, Loss: 0.6156, Val Loss: 0.5793\n",
      "Saving model at epoch 384 with loss 0.6156\n",
      "Epoch 385, Loss: 0.6152, Val Loss: 0.5789\n",
      "Saving model at epoch 385 with loss 0.6152\n",
      "Epoch 386, Loss: 0.6147, Val Loss: 0.5786\n",
      "Saving model at epoch 386 with loss 0.6147\n",
      "Epoch 387, Loss: 0.6143, Val Loss: 0.5782\n",
      "Saving model at epoch 387 with loss 0.6143\n",
      "Epoch 388, Loss: 0.6139, Val Loss: 0.5779\n",
      "Saving model at epoch 388 with loss 0.6139\n",
      "Epoch 389, Loss: 0.6135, Val Loss: 0.5775\n",
      "Saving model at epoch 389 with loss 0.6135\n",
      "Epoch 390, Loss: 0.6131, Val Loss: 0.5771\n",
      "Saving model at epoch 390 with loss 0.6131\n",
      "Epoch 391, Loss: 0.6127, Val Loss: 0.5768\n",
      "Saving model at epoch 391 with loss 0.6127\n",
      "Epoch 392, Loss: 0.6123, Val Loss: 0.5764\n",
      "Saving model at epoch 392 with loss 0.6123\n",
      "Epoch 393, Loss: 0.6119, Val Loss: 0.5761\n",
      "Saving model at epoch 393 with loss 0.6119\n",
      "Epoch 394, Loss: 0.6115, Val Loss: 0.5757\n",
      "Saving model at epoch 394 with loss 0.6115\n",
      "Epoch 395, Loss: 0.6111, Val Loss: 0.5754\n",
      "Saving model at epoch 395 with loss 0.6111\n",
      "Epoch 396, Loss: 0.6108, Val Loss: 0.5750\n",
      "Saving model at epoch 396 with loss 0.6108\n",
      "Epoch 397, Loss: 0.6104, Val Loss: 0.5747\n",
      "Saving model at epoch 397 with loss 0.6104\n",
      "Epoch 398, Loss: 0.6100, Val Loss: 0.5743\n",
      "Saving model at epoch 398 with loss 0.6100\n",
      "Epoch 399, Loss: 0.6096, Val Loss: 0.5740\n",
      "Saving model at epoch 399 with loss 0.6096\n",
      "Epoch 400, Loss: 0.6092, Val Loss: 0.5736\n",
      "Saving model at epoch 400 with loss 0.6092\n",
      "Epoch 401, Loss: 0.6088, Val Loss: 0.5733\n",
      "Saving model at epoch 401 with loss 0.6088\n",
      "Epoch 402, Loss: 0.6084, Val Loss: 0.5730\n",
      "Saving model at epoch 402 with loss 0.6084\n",
      "Epoch 403, Loss: 0.6080, Val Loss: 0.5726\n",
      "Saving model at epoch 403 with loss 0.6080\n",
      "Epoch 404, Loss: 0.6077, Val Loss: 0.5723\n",
      "Saving model at epoch 404 with loss 0.6077\n",
      "Epoch 405, Loss: 0.6073, Val Loss: 0.5719\n",
      "Saving model at epoch 405 with loss 0.6073\n",
      "Epoch 406, Loss: 0.6069, Val Loss: 0.5716\n",
      "Saving model at epoch 406 with loss 0.6069\n",
      "Epoch 407, Loss: 0.6065, Val Loss: 0.5713\n",
      "Saving model at epoch 407 with loss 0.6065\n",
      "Epoch 408, Loss: 0.6062, Val Loss: 0.5709\n",
      "Saving model at epoch 408 with loss 0.6062\n",
      "Epoch 409, Loss: 0.6058, Val Loss: 0.5706\n",
      "Saving model at epoch 409 with loss 0.6058\n",
      "Epoch 410, Loss: 0.6054, Val Loss: 0.5703\n",
      "Saving model at epoch 410 with loss 0.6054\n",
      "Epoch 411, Loss: 0.6050, Val Loss: 0.5699\n",
      "Saving model at epoch 411 with loss 0.6050\n",
      "Epoch 412, Loss: 0.6047, Val Loss: 0.5696\n",
      "Saving model at epoch 412 with loss 0.6047\n",
      "Epoch 413, Loss: 0.6043, Val Loss: 0.5693\n",
      "Saving model at epoch 413 with loss 0.6043\n",
      "Epoch 414, Loss: 0.6039, Val Loss: 0.5689\n",
      "Saving model at epoch 414 with loss 0.6039\n",
      "Epoch 415, Loss: 0.6036, Val Loss: 0.5686\n",
      "Saving model at epoch 415 with loss 0.6036\n",
      "Epoch 416, Loss: 0.6032, Val Loss: 0.5683\n",
      "Saving model at epoch 416 with loss 0.6032\n",
      "Epoch 417, Loss: 0.6028, Val Loss: 0.5679\n",
      "Saving model at epoch 417 with loss 0.6028\n",
      "Epoch 418, Loss: 0.6025, Val Loss: 0.5676\n",
      "Saving model at epoch 418 with loss 0.6025\n",
      "Epoch 419, Loss: 0.6021, Val Loss: 0.5673\n",
      "Saving model at epoch 419 with loss 0.6021\n",
      "Epoch 420, Loss: 0.6017, Val Loss: 0.5670\n",
      "Saving model at epoch 420 with loss 0.6017\n",
      "Epoch 421, Loss: 0.6014, Val Loss: 0.5666\n",
      "Saving model at epoch 421 with loss 0.6014\n",
      "Epoch 422, Loss: 0.6010, Val Loss: 0.5663\n",
      "Saving model at epoch 422 with loss 0.6010\n",
      "Epoch 423, Loss: 0.6007, Val Loss: 0.5660\n",
      "Saving model at epoch 423 with loss 0.6007\n",
      "Epoch 424, Loss: 0.6003, Val Loss: 0.5657\n",
      "Saving model at epoch 424 with loss 0.6003\n",
      "Epoch 425, Loss: 0.5999, Val Loss: 0.5654\n",
      "Saving model at epoch 425 with loss 0.5999\n",
      "Epoch 426, Loss: 0.5996, Val Loss: 0.5650\n",
      "Saving model at epoch 426 with loss 0.5996\n",
      "Epoch 427, Loss: 0.5992, Val Loss: 0.5647\n",
      "Saving model at epoch 427 with loss 0.5992\n",
      "Epoch 428, Loss: 0.5989, Val Loss: 0.5644\n",
      "Saving model at epoch 428 with loss 0.5989\n",
      "Epoch 429, Loss: 0.5985, Val Loss: 0.5641\n",
      "Saving model at epoch 429 with loss 0.5985\n",
      "Epoch 430, Loss: 0.5982, Val Loss: 0.5638\n",
      "Saving model at epoch 430 with loss 0.5982\n",
      "Epoch 431, Loss: 0.5978, Val Loss: 0.5634\n",
      "Saving model at epoch 431 with loss 0.5978\n",
      "Epoch 432, Loss: 0.5975, Val Loss: 0.5631\n",
      "Saving model at epoch 432 with loss 0.5975\n",
      "Epoch 433, Loss: 0.5971, Val Loss: 0.5628\n",
      "Saving model at epoch 433 with loss 0.5971\n",
      "Epoch 434, Loss: 0.5968, Val Loss: 0.5625\n",
      "Saving model at epoch 434 with loss 0.5968\n",
      "Epoch 435, Loss: 0.5964, Val Loss: 0.5622\n",
      "Saving model at epoch 435 with loss 0.5964\n",
      "Epoch 436, Loss: 0.5961, Val Loss: 0.5619\n",
      "Saving model at epoch 436 with loss 0.5961\n",
      "Epoch 437, Loss: 0.5957, Val Loss: 0.5616\n",
      "Saving model at epoch 437 with loss 0.5957\n",
      "Epoch 438, Loss: 0.5954, Val Loss: 0.5613\n",
      "Saving model at epoch 438 with loss 0.5954\n",
      "Epoch 439, Loss: 0.5950, Val Loss: 0.5609\n",
      "Saving model at epoch 439 with loss 0.5950\n",
      "Epoch 440, Loss: 0.5947, Val Loss: 0.5606\n",
      "Saving model at epoch 440 with loss 0.5947\n",
      "Epoch 441, Loss: 0.5944, Val Loss: 0.5603\n",
      "Saving model at epoch 441 with loss 0.5944\n",
      "Epoch 442, Loss: 0.5940, Val Loss: 0.5600\n",
      "Saving model at epoch 442 with loss 0.5940\n",
      "Epoch 443, Loss: 0.5937, Val Loss: 0.5597\n",
      "Saving model at epoch 443 with loss 0.5937\n",
      "Epoch 444, Loss: 0.5933, Val Loss: 0.5594\n",
      "Saving model at epoch 444 with loss 0.5933\n",
      "Epoch 445, Loss: 0.5930, Val Loss: 0.5591\n",
      "Saving model at epoch 445 with loss 0.5930\n",
      "Epoch 446, Loss: 0.5926, Val Loss: 0.5588\n",
      "Saving model at epoch 446 with loss 0.5926\n",
      "Epoch 447, Loss: 0.5923, Val Loss: 0.5585\n",
      "Saving model at epoch 447 with loss 0.5923\n",
      "Epoch 448, Loss: 0.5920, Val Loss: 0.5582\n",
      "Saving model at epoch 448 with loss 0.5920\n",
      "Epoch 449, Loss: 0.5916, Val Loss: 0.5579\n",
      "Saving model at epoch 449 with loss 0.5916\n",
      "Epoch 450, Loss: 0.5913, Val Loss: 0.5576\n",
      "Saving model at epoch 450 with loss 0.5913\n",
      "Epoch 451, Loss: 0.5910, Val Loss: 0.5573\n",
      "Saving model at epoch 451 with loss 0.5910\n",
      "Epoch 452, Loss: 0.5906, Val Loss: 0.5569\n",
      "Saving model at epoch 452 with loss 0.5906\n",
      "Epoch 453, Loss: 0.5903, Val Loss: 0.5566\n",
      "Saving model at epoch 453 with loss 0.5903\n",
      "Epoch 454, Loss: 0.5900, Val Loss: 0.5563\n",
      "Saving model at epoch 454 with loss 0.5900\n",
      "Epoch 455, Loss: 0.5896, Val Loss: 0.5560\n",
      "Saving model at epoch 455 with loss 0.5896\n",
      "Epoch 456, Loss: 0.5893, Val Loss: 0.5557\n",
      "Saving model at epoch 456 with loss 0.5893\n",
      "Epoch 457, Loss: 0.5890, Val Loss: 0.5554\n",
      "Saving model at epoch 457 with loss 0.5890\n",
      "Epoch 458, Loss: 0.5886, Val Loss: 0.5551\n",
      "Saving model at epoch 458 with loss 0.5886\n",
      "Epoch 459, Loss: 0.5883, Val Loss: 0.5548\n",
      "Saving model at epoch 459 with loss 0.5883\n",
      "Epoch 460, Loss: 0.5880, Val Loss: 0.5545\n",
      "Saving model at epoch 460 with loss 0.5880\n",
      "Epoch 461, Loss: 0.5876, Val Loss: 0.5542\n",
      "Saving model at epoch 461 with loss 0.5876\n",
      "Epoch 462, Loss: 0.5873, Val Loss: 0.5539\n",
      "Saving model at epoch 462 with loss 0.5873\n",
      "Epoch 463, Loss: 0.5870, Val Loss: 0.5536\n",
      "Saving model at epoch 463 with loss 0.5870\n",
      "Epoch 464, Loss: 0.5867, Val Loss: 0.5533\n",
      "Saving model at epoch 464 with loss 0.5867\n",
      "Epoch 465, Loss: 0.5863, Val Loss: 0.5531\n",
      "Saving model at epoch 465 with loss 0.5863\n",
      "Epoch 466, Loss: 0.5860, Val Loss: 0.5528\n",
      "Saving model at epoch 466 with loss 0.5860\n",
      "Epoch 467, Loss: 0.5857, Val Loss: 0.5525\n",
      "Saving model at epoch 467 with loss 0.5857\n",
      "Epoch 468, Loss: 0.5854, Val Loss: 0.5522\n",
      "Saving model at epoch 468 with loss 0.5854\n",
      "Epoch 469, Loss: 0.5850, Val Loss: 0.5519\n",
      "Saving model at epoch 469 with loss 0.5850\n",
      "Epoch 470, Loss: 0.5847, Val Loss: 0.5516\n",
      "Saving model at epoch 470 with loss 0.5847\n",
      "Epoch 471, Loss: 0.5844, Val Loss: 0.5513\n",
      "Saving model at epoch 471 with loss 0.5844\n",
      "Epoch 472, Loss: 0.5841, Val Loss: 0.5510\n",
      "Saving model at epoch 472 with loss 0.5841\n",
      "Epoch 473, Loss: 0.5837, Val Loss: 0.5507\n",
      "Saving model at epoch 473 with loss 0.5837\n",
      "Epoch 474, Loss: 0.5834, Val Loss: 0.5504\n",
      "Saving model at epoch 474 with loss 0.5834\n",
      "Epoch 475, Loss: 0.5831, Val Loss: 0.5501\n",
      "Saving model at epoch 475 with loss 0.5831\n",
      "Epoch 476, Loss: 0.5828, Val Loss: 0.5498\n",
      "Saving model at epoch 476 with loss 0.5828\n",
      "Epoch 477, Loss: 0.5825, Val Loss: 0.5495\n",
      "Saving model at epoch 477 with loss 0.5825\n",
      "Epoch 478, Loss: 0.5821, Val Loss: 0.5492\n",
      "Saving model at epoch 478 with loss 0.5821\n",
      "Epoch 479, Loss: 0.5818, Val Loss: 0.5489\n",
      "Saving model at epoch 479 with loss 0.5818\n",
      "Epoch 480, Loss: 0.5815, Val Loss: 0.5486\n",
      "Saving model at epoch 480 with loss 0.5815\n",
      "Epoch 481, Loss: 0.5812, Val Loss: 0.5484\n",
      "Saving model at epoch 481 with loss 0.5812\n",
      "Epoch 482, Loss: 0.5809, Val Loss: 0.5481\n",
      "Saving model at epoch 482 with loss 0.5809\n",
      "Epoch 483, Loss: 0.5805, Val Loss: 0.5478\n",
      "Saving model at epoch 483 with loss 0.5805\n",
      "Epoch 484, Loss: 0.5802, Val Loss: 0.5475\n",
      "Saving model at epoch 484 with loss 0.5802\n",
      "Epoch 485, Loss: 0.5799, Val Loss: 0.5472\n",
      "Saving model at epoch 485 with loss 0.5799\n",
      "Epoch 486, Loss: 0.5796, Val Loss: 0.5469\n",
      "Saving model at epoch 486 with loss 0.5796\n",
      "Epoch 487, Loss: 0.5793, Val Loss: 0.5466\n",
      "Saving model at epoch 487 with loss 0.5793\n",
      "Epoch 488, Loss: 0.5790, Val Loss: 0.5463\n",
      "Saving model at epoch 488 with loss 0.5790\n",
      "Epoch 489, Loss: 0.5786, Val Loss: 0.5460\n",
      "Saving model at epoch 489 with loss 0.5786\n",
      "Epoch 490, Loss: 0.5783, Val Loss: 0.5458\n",
      "Saving model at epoch 490 with loss 0.5783\n",
      "Epoch 491, Loss: 0.5780, Val Loss: 0.5455\n",
      "Saving model at epoch 491 with loss 0.5780\n",
      "Epoch 492, Loss: 0.5777, Val Loss: 0.5452\n",
      "Saving model at epoch 492 with loss 0.5777\n",
      "Epoch 493, Loss: 0.5774, Val Loss: 0.5449\n",
      "Saving model at epoch 493 with loss 0.5774\n",
      "Epoch 494, Loss: 0.5771, Val Loss: 0.5446\n",
      "Saving model at epoch 494 with loss 0.5771\n",
      "Epoch 495, Loss: 0.5768, Val Loss: 0.5443\n",
      "Saving model at epoch 495 with loss 0.5768\n",
      "Epoch 496, Loss: 0.5764, Val Loss: 0.5440\n",
      "Saving model at epoch 496 with loss 0.5764\n",
      "Epoch 497, Loss: 0.5761, Val Loss: 0.5437\n",
      "Saving model at epoch 497 with loss 0.5761\n",
      "Epoch 498, Loss: 0.5758, Val Loss: 0.5435\n",
      "Saving model at epoch 498 with loss 0.5758\n",
      "Epoch 499, Loss: 0.5755, Val Loss: 0.5432\n",
      "Saving model at epoch 499 with loss 0.5755\n",
      "Epoch 500, Loss: 0.5752, Val Loss: 0.5429\n",
      "Saving model at epoch 500 with loss 0.5752\n",
      "Epoch 501, Loss: 0.5749, Val Loss: 0.5426\n",
      "Saving model at epoch 501 with loss 0.5749\n",
      "Epoch 502, Loss: 0.5746, Val Loss: 0.5423\n",
      "Saving model at epoch 502 with loss 0.5746\n",
      "Epoch 503, Loss: 0.5743, Val Loss: 0.5420\n",
      "Saving model at epoch 503 with loss 0.5743\n",
      "Epoch 504, Loss: 0.5740, Val Loss: 0.5417\n",
      "Saving model at epoch 504 with loss 0.5740\n",
      "Epoch 505, Loss: 0.5736, Val Loss: 0.5415\n",
      "Saving model at epoch 505 with loss 0.5736\n",
      "Epoch 506, Loss: 0.5733, Val Loss: 0.5412\n",
      "Saving model at epoch 506 with loss 0.5733\n",
      "Epoch 507, Loss: 0.5730, Val Loss: 0.5409\n",
      "Saving model at epoch 507 with loss 0.5730\n",
      "Epoch 508, Loss: 0.5727, Val Loss: 0.5406\n",
      "Saving model at epoch 508 with loss 0.5727\n",
      "Epoch 509, Loss: 0.5724, Val Loss: 0.5403\n",
      "Saving model at epoch 509 with loss 0.5724\n",
      "Epoch 510, Loss: 0.5721, Val Loss: 0.5400\n",
      "Saving model at epoch 510 with loss 0.5721\n",
      "Epoch 511, Loss: 0.5718, Val Loss: 0.5398\n",
      "Saving model at epoch 511 with loss 0.5718\n",
      "Epoch 512, Loss: 0.5715, Val Loss: 0.5395\n",
      "Saving model at epoch 512 with loss 0.5715\n",
      "Epoch 513, Loss: 0.5712, Val Loss: 0.5392\n",
      "Saving model at epoch 513 with loss 0.5712\n",
      "Epoch 514, Loss: 0.5709, Val Loss: 0.5389\n",
      "Saving model at epoch 514 with loss 0.5709\n",
      "Epoch 515, Loss: 0.5706, Val Loss: 0.5386\n",
      "Saving model at epoch 515 with loss 0.5706\n",
      "Epoch 516, Loss: 0.5703, Val Loss: 0.5383\n",
      "Saving model at epoch 516 with loss 0.5703\n",
      "Epoch 517, Loss: 0.5700, Val Loss: 0.5381\n",
      "Saving model at epoch 517 with loss 0.5700\n",
      "Epoch 518, Loss: 0.5697, Val Loss: 0.5378\n",
      "Saving model at epoch 518 with loss 0.5697\n",
      "Epoch 519, Loss: 0.5694, Val Loss: 0.5375\n",
      "Saving model at epoch 519 with loss 0.5694\n",
      "Epoch 520, Loss: 0.5691, Val Loss: 0.5372\n",
      "Saving model at epoch 520 with loss 0.5691\n",
      "Epoch 521, Loss: 0.5687, Val Loss: 0.5369\n",
      "Saving model at epoch 521 with loss 0.5687\n",
      "Epoch 522, Loss: 0.5684, Val Loss: 0.5367\n",
      "Saving model at epoch 522 with loss 0.5684\n",
      "Epoch 523, Loss: 0.5681, Val Loss: 0.5364\n",
      "Saving model at epoch 523 with loss 0.5681\n",
      "Epoch 524, Loss: 0.5678, Val Loss: 0.5361\n",
      "Saving model at epoch 524 with loss 0.5678\n",
      "Epoch 525, Loss: 0.5675, Val Loss: 0.5358\n",
      "Saving model at epoch 525 with loss 0.5675\n",
      "Epoch 526, Loss: 0.5672, Val Loss: 0.5355\n",
      "Saving model at epoch 526 with loss 0.5672\n",
      "Epoch 527, Loss: 0.5669, Val Loss: 0.5353\n",
      "Saving model at epoch 527 with loss 0.5669\n",
      "Epoch 528, Loss: 0.5666, Val Loss: 0.5350\n",
      "Saving model at epoch 528 with loss 0.5666\n",
      "Epoch 529, Loss: 0.5663, Val Loss: 0.5347\n",
      "Saving model at epoch 529 with loss 0.5663\n",
      "Epoch 530, Loss: 0.5660, Val Loss: 0.5344\n",
      "Saving model at epoch 530 with loss 0.5660\n",
      "Epoch 531, Loss: 0.5657, Val Loss: 0.5341\n",
      "Saving model at epoch 531 with loss 0.5657\n",
      "Epoch 532, Loss: 0.5654, Val Loss: 0.5338\n",
      "Saving model at epoch 532 with loss 0.5654\n",
      "Epoch 533, Loss: 0.5651, Val Loss: 0.5336\n",
      "Saving model at epoch 533 with loss 0.5651\n",
      "Epoch 534, Loss: 0.5648, Val Loss: 0.5333\n",
      "Saving model at epoch 534 with loss 0.5648\n",
      "Epoch 535, Loss: 0.5645, Val Loss: 0.5330\n",
      "Saving model at epoch 535 with loss 0.5645\n",
      "Epoch 536, Loss: 0.5642, Val Loss: 0.5327\n",
      "Saving model at epoch 536 with loss 0.5642\n",
      "Epoch 537, Loss: 0.5639, Val Loss: 0.5324\n",
      "Saving model at epoch 537 with loss 0.5639\n",
      "Epoch 538, Loss: 0.5636, Val Loss: 0.5322\n",
      "Saving model at epoch 538 with loss 0.5636\n",
      "Epoch 539, Loss: 0.5633, Val Loss: 0.5319\n",
      "Saving model at epoch 539 with loss 0.5633\n",
      "Epoch 540, Loss: 0.5630, Val Loss: 0.5316\n",
      "Saving model at epoch 540 with loss 0.5630\n",
      "Epoch 541, Loss: 0.5627, Val Loss: 0.5313\n",
      "Saving model at epoch 541 with loss 0.5627\n",
      "Epoch 542, Loss: 0.5624, Val Loss: 0.5310\n",
      "Saving model at epoch 542 with loss 0.5624\n",
      "Epoch 543, Loss: 0.5621, Val Loss: 0.5308\n",
      "Saving model at epoch 543 with loss 0.5621\n",
      "Epoch 544, Loss: 0.5618, Val Loss: 0.5305\n",
      "Saving model at epoch 544 with loss 0.5618\n",
      "Epoch 545, Loss: 0.5615, Val Loss: 0.5302\n",
      "Saving model at epoch 545 with loss 0.5615\n",
      "Epoch 546, Loss: 0.5612, Val Loss: 0.5299\n",
      "Saving model at epoch 546 with loss 0.5612\n",
      "Epoch 547, Loss: 0.5609, Val Loss: 0.5296\n",
      "Saving model at epoch 547 with loss 0.5609\n",
      "Epoch 548, Loss: 0.5606, Val Loss: 0.5294\n",
      "Saving model at epoch 548 with loss 0.5606\n",
      "Epoch 549, Loss: 0.5603, Val Loss: 0.5291\n",
      "Saving model at epoch 549 with loss 0.5603\n",
      "Epoch 550, Loss: 0.5600, Val Loss: 0.5288\n",
      "Saving model at epoch 550 with loss 0.5600\n",
      "Epoch 551, Loss: 0.5597, Val Loss: 0.5285\n",
      "Saving model at epoch 551 with loss 0.5597\n",
      "Epoch 552, Loss: 0.5594, Val Loss: 0.5282\n",
      "Saving model at epoch 552 with loss 0.5594\n",
      "Epoch 553, Loss: 0.5591, Val Loss: 0.5280\n",
      "Saving model at epoch 553 with loss 0.5591\n",
      "Epoch 554, Loss: 0.5588, Val Loss: 0.5277\n",
      "Saving model at epoch 554 with loss 0.5588\n",
      "Epoch 555, Loss: 0.5585, Val Loss: 0.5274\n",
      "Saving model at epoch 555 with loss 0.5585\n",
      "Epoch 556, Loss: 0.5582, Val Loss: 0.5271\n",
      "Saving model at epoch 556 with loss 0.5582\n",
      "Epoch 557, Loss: 0.5579, Val Loss: 0.5268\n",
      "Saving model at epoch 557 with loss 0.5579\n",
      "Epoch 558, Loss: 0.5576, Val Loss: 0.5266\n",
      "Saving model at epoch 558 with loss 0.5576\n",
      "Epoch 559, Loss: 0.5573, Val Loss: 0.5263\n",
      "Saving model at epoch 559 with loss 0.5573\n",
      "Epoch 560, Loss: 0.5570, Val Loss: 0.5260\n",
      "Saving model at epoch 560 with loss 0.5570\n",
      "Epoch 561, Loss: 0.5567, Val Loss: 0.5257\n",
      "Saving model at epoch 561 with loss 0.5567\n",
      "Epoch 562, Loss: 0.5564, Val Loss: 0.5254\n",
      "Saving model at epoch 562 with loss 0.5564\n",
      "Epoch 563, Loss: 0.5561, Val Loss: 0.5252\n",
      "Saving model at epoch 563 with loss 0.5561\n",
      "Epoch 564, Loss: 0.5558, Val Loss: 0.5249\n",
      "Saving model at epoch 564 with loss 0.5558\n",
      "Epoch 565, Loss: 0.5555, Val Loss: 0.5246\n",
      "Saving model at epoch 565 with loss 0.5555\n",
      "Epoch 566, Loss: 0.5553, Val Loss: 0.5243\n",
      "Saving model at epoch 566 with loss 0.5553\n",
      "Epoch 567, Loss: 0.5550, Val Loss: 0.5240\n",
      "Saving model at epoch 567 with loss 0.5550\n",
      "Epoch 568, Loss: 0.5547, Val Loss: 0.5238\n",
      "Saving model at epoch 568 with loss 0.5547\n",
      "Epoch 569, Loss: 0.5544, Val Loss: 0.5235\n",
      "Saving model at epoch 569 with loss 0.5544\n",
      "Epoch 570, Loss: 0.5541, Val Loss: 0.5232\n",
      "Saving model at epoch 570 with loss 0.5541\n",
      "Epoch 571, Loss: 0.5538, Val Loss: 0.5229\n",
      "Saving model at epoch 571 with loss 0.5538\n",
      "Epoch 572, Loss: 0.5535, Val Loss: 0.5227\n",
      "Saving model at epoch 572 with loss 0.5535\n",
      "Epoch 573, Loss: 0.5532, Val Loss: 0.5224\n",
      "Saving model at epoch 573 with loss 0.5532\n",
      "Epoch 574, Loss: 0.5529, Val Loss: 0.5221\n",
      "Saving model at epoch 574 with loss 0.5529\n",
      "Epoch 575, Loss: 0.5526, Val Loss: 0.5218\n",
      "Saving model at epoch 575 with loss 0.5526\n",
      "Epoch 576, Loss: 0.5523, Val Loss: 0.5215\n",
      "Saving model at epoch 576 with loss 0.5523\n",
      "Epoch 577, Loss: 0.5520, Val Loss: 0.5213\n",
      "Saving model at epoch 577 with loss 0.5520\n",
      "Epoch 578, Loss: 0.5517, Val Loss: 0.5210\n",
      "Saving model at epoch 578 with loss 0.5517\n",
      "Epoch 579, Loss: 0.5514, Val Loss: 0.5207\n",
      "Saving model at epoch 579 with loss 0.5514\n",
      "Epoch 580, Loss: 0.5511, Val Loss: 0.5204\n",
      "Saving model at epoch 580 with loss 0.5511\n",
      "Epoch 581, Loss: 0.5508, Val Loss: 0.5202\n",
      "Saving model at epoch 581 with loss 0.5508\n",
      "Epoch 582, Loss: 0.5505, Val Loss: 0.5199\n",
      "Saving model at epoch 582 with loss 0.5505\n",
      "Epoch 583, Loss: 0.5502, Val Loss: 0.5196\n",
      "Saving model at epoch 583 with loss 0.5502\n",
      "Epoch 584, Loss: 0.5499, Val Loss: 0.5193\n",
      "Saving model at epoch 584 with loss 0.5499\n",
      "Epoch 585, Loss: 0.5496, Val Loss: 0.5190\n",
      "Saving model at epoch 585 with loss 0.5496\n",
      "Epoch 586, Loss: 0.5493, Val Loss: 0.5188\n",
      "Saving model at epoch 586 with loss 0.5493\n",
      "Epoch 587, Loss: 0.5490, Val Loss: 0.5185\n",
      "Saving model at epoch 587 with loss 0.5490\n",
      "Epoch 588, Loss: 0.5487, Val Loss: 0.5182\n",
      "Saving model at epoch 588 with loss 0.5487\n",
      "Epoch 589, Loss: 0.5484, Val Loss: 0.5179\n",
      "Saving model at epoch 589 with loss 0.5484\n",
      "Epoch 590, Loss: 0.5481, Val Loss: 0.5176\n",
      "Saving model at epoch 590 with loss 0.5481\n",
      "Epoch 591, Loss: 0.5478, Val Loss: 0.5174\n",
      "Saving model at epoch 591 with loss 0.5478\n",
      "Epoch 592, Loss: 0.5475, Val Loss: 0.5171\n",
      "Saving model at epoch 592 with loss 0.5475\n",
      "Epoch 593, Loss: 0.5472, Val Loss: 0.5168\n",
      "Saving model at epoch 593 with loss 0.5472\n",
      "Epoch 594, Loss: 0.5469, Val Loss: 0.5165\n",
      "Saving model at epoch 594 with loss 0.5469\n",
      "Epoch 595, Loss: 0.5467, Val Loss: 0.5162\n",
      "Saving model at epoch 595 with loss 0.5467\n",
      "Epoch 596, Loss: 0.5464, Val Loss: 0.5159\n",
      "Saving model at epoch 596 with loss 0.5464\n",
      "Epoch 597, Loss: 0.5461, Val Loss: 0.5157\n",
      "Saving model at epoch 597 with loss 0.5461\n",
      "Epoch 598, Loss: 0.5458, Val Loss: 0.5154\n",
      "Saving model at epoch 598 with loss 0.5458\n",
      "Epoch 599, Loss: 0.5455, Val Loss: 0.5151\n",
      "Saving model at epoch 599 with loss 0.5455\n",
      "Epoch 600, Loss: 0.5452, Val Loss: 0.5148\n",
      "Saving model at epoch 600 with loss 0.5452\n",
      "Epoch 601, Loss: 0.5449, Val Loss: 0.5145\n",
      "Saving model at epoch 601 with loss 0.5449\n",
      "Epoch 602, Loss: 0.5446, Val Loss: 0.5143\n",
      "Saving model at epoch 602 with loss 0.5446\n",
      "Epoch 603, Loss: 0.5443, Val Loss: 0.5140\n",
      "Saving model at epoch 603 with loss 0.5443\n",
      "Epoch 604, Loss: 0.5440, Val Loss: 0.5137\n",
      "Saving model at epoch 604 with loss 0.5440\n",
      "Epoch 605, Loss: 0.5437, Val Loss: 0.5134\n",
      "Saving model at epoch 605 with loss 0.5437\n",
      "Epoch 606, Loss: 0.5434, Val Loss: 0.5131\n",
      "Saving model at epoch 606 with loss 0.5434\n",
      "Epoch 607, Loss: 0.5431, Val Loss: 0.5128\n",
      "Saving model at epoch 607 with loss 0.5431\n",
      "Epoch 608, Loss: 0.5428, Val Loss: 0.5126\n",
      "Saving model at epoch 608 with loss 0.5428\n",
      "Epoch 609, Loss: 0.5425, Val Loss: 0.5123\n",
      "Saving model at epoch 609 with loss 0.5425\n",
      "Epoch 610, Loss: 0.5422, Val Loss: 0.5120\n",
      "Saving model at epoch 610 with loss 0.5422\n",
      "Epoch 611, Loss: 0.5419, Val Loss: 0.5117\n",
      "Saving model at epoch 611 with loss 0.5419\n",
      "Epoch 612, Loss: 0.5416, Val Loss: 0.5114\n",
      "Saving model at epoch 612 with loss 0.5416\n",
      "Epoch 613, Loss: 0.5413, Val Loss: 0.5111\n",
      "Saving model at epoch 613 with loss 0.5413\n",
      "Epoch 614, Loss: 0.5410, Val Loss: 0.5109\n",
      "Saving model at epoch 614 with loss 0.5410\n",
      "Epoch 615, Loss: 0.5407, Val Loss: 0.5106\n",
      "Saving model at epoch 615 with loss 0.5407\n",
      "Epoch 616, Loss: 0.5404, Val Loss: 0.5103\n",
      "Saving model at epoch 616 with loss 0.5404\n",
      "Epoch 617, Loss: 0.5401, Val Loss: 0.5100\n",
      "Saving model at epoch 617 with loss 0.5401\n",
      "Epoch 618, Loss: 0.5398, Val Loss: 0.5097\n",
      "Saving model at epoch 618 with loss 0.5398\n",
      "Epoch 619, Loss: 0.5395, Val Loss: 0.5094\n",
      "Saving model at epoch 619 with loss 0.5395\n",
      "Epoch 620, Loss: 0.5392, Val Loss: 0.5091\n",
      "Saving model at epoch 620 with loss 0.5392\n",
      "Epoch 621, Loss: 0.5389, Val Loss: 0.5088\n",
      "Saving model at epoch 621 with loss 0.5389\n",
      "Epoch 622, Loss: 0.5386, Val Loss: 0.5086\n",
      "Saving model at epoch 622 with loss 0.5386\n",
      "Epoch 623, Loss: 0.5383, Val Loss: 0.5083\n",
      "Saving model at epoch 623 with loss 0.5383\n",
      "Epoch 624, Loss: 0.5380, Val Loss: 0.5080\n",
      "Saving model at epoch 624 with loss 0.5380\n",
      "Epoch 625, Loss: 0.5377, Val Loss: 0.5077\n",
      "Saving model at epoch 625 with loss 0.5377\n",
      "Epoch 626, Loss: 0.5374, Val Loss: 0.5074\n",
      "Saving model at epoch 626 with loss 0.5374\n",
      "Epoch 627, Loss: 0.5371, Val Loss: 0.5071\n",
      "Saving model at epoch 627 with loss 0.5371\n",
      "Epoch 628, Loss: 0.5369, Val Loss: 0.5068\n",
      "Saving model at epoch 628 with loss 0.5369\n",
      "Epoch 629, Loss: 0.5366, Val Loss: 0.5065\n",
      "Saving model at epoch 629 with loss 0.5366\n",
      "Epoch 630, Loss: 0.5363, Val Loss: 0.5062\n",
      "Saving model at epoch 630 with loss 0.5363\n",
      "Epoch 631, Loss: 0.5360, Val Loss: 0.5060\n",
      "Saving model at epoch 631 with loss 0.5360\n",
      "Epoch 632, Loss: 0.5357, Val Loss: 0.5057\n",
      "Saving model at epoch 632 with loss 0.5357\n",
      "Epoch 633, Loss: 0.5354, Val Loss: 0.5054\n",
      "Saving model at epoch 633 with loss 0.5354\n",
      "Epoch 634, Loss: 0.5351, Val Loss: 0.5051\n",
      "Saving model at epoch 634 with loss 0.5351\n",
      "Epoch 635, Loss: 0.5348, Val Loss: 0.5048\n",
      "Saving model at epoch 635 with loss 0.5348\n",
      "Epoch 636, Loss: 0.5345, Val Loss: 0.5045\n",
      "Saving model at epoch 636 with loss 0.5345\n",
      "Epoch 637, Loss: 0.5342, Val Loss: 0.5042\n",
      "Saving model at epoch 637 with loss 0.5342\n",
      "Epoch 638, Loss: 0.5339, Val Loss: 0.5039\n",
      "Saving model at epoch 638 with loss 0.5339\n",
      "Epoch 639, Loss: 0.5336, Val Loss: 0.5036\n",
      "Saving model at epoch 639 with loss 0.5336\n",
      "Epoch 640, Loss: 0.5333, Val Loss: 0.5034\n",
      "Saving model at epoch 640 with loss 0.5333\n",
      "Epoch 641, Loss: 0.5330, Val Loss: 0.5031\n",
      "Saving model at epoch 641 with loss 0.5330\n",
      "Epoch 642, Loss: 0.5327, Val Loss: 0.5028\n",
      "Saving model at epoch 642 with loss 0.5327\n",
      "Epoch 643, Loss: 0.5324, Val Loss: 0.5025\n",
      "Saving model at epoch 643 with loss 0.5324\n",
      "Epoch 644, Loss: 0.5321, Val Loss: 0.5022\n",
      "Saving model at epoch 644 with loss 0.5321\n",
      "Epoch 645, Loss: 0.5318, Val Loss: 0.5019\n",
      "Saving model at epoch 645 with loss 0.5318\n",
      "Epoch 646, Loss: 0.5315, Val Loss: 0.5016\n",
      "Saving model at epoch 646 with loss 0.5315\n",
      "Epoch 647, Loss: 0.5313, Val Loss: 0.5013\n",
      "Saving model at epoch 647 with loss 0.5313\n",
      "Epoch 648, Loss: 0.5310, Val Loss: 0.5010\n",
      "Saving model at epoch 648 with loss 0.5310\n",
      "Epoch 649, Loss: 0.5307, Val Loss: 0.5007\n",
      "Saving model at epoch 649 with loss 0.5307\n",
      "Epoch 650, Loss: 0.5304, Val Loss: 0.5004\n",
      "Saving model at epoch 650 with loss 0.5304\n",
      "Epoch 651, Loss: 0.5301, Val Loss: 0.5002\n",
      "Saving model at epoch 651 with loss 0.5301\n",
      "Epoch 652, Loss: 0.5298, Val Loss: 0.4999\n",
      "Saving model at epoch 652 with loss 0.5298\n",
      "Epoch 653, Loss: 0.5295, Val Loss: 0.4996\n",
      "Saving model at epoch 653 with loss 0.5295\n",
      "Epoch 654, Loss: 0.5292, Val Loss: 0.4993\n",
      "Saving model at epoch 654 with loss 0.5292\n",
      "Epoch 655, Loss: 0.5289, Val Loss: 0.4990\n",
      "Saving model at epoch 655 with loss 0.5289\n",
      "Epoch 656, Loss: 0.5286, Val Loss: 0.4987\n",
      "Saving model at epoch 656 with loss 0.5286\n",
      "Epoch 657, Loss: 0.5283, Val Loss: 0.4984\n",
      "Saving model at epoch 657 with loss 0.5283\n",
      "Epoch 658, Loss: 0.5280, Val Loss: 0.4981\n",
      "Saving model at epoch 658 with loss 0.5280\n",
      "Epoch 659, Loss: 0.5277, Val Loss: 0.4978\n",
      "Saving model at epoch 659 with loss 0.5277\n",
      "Epoch 660, Loss: 0.5274, Val Loss: 0.4975\n",
      "Saving model at epoch 660 with loss 0.5274\n",
      "Epoch 661, Loss: 0.5271, Val Loss: 0.4972\n",
      "Saving model at epoch 661 with loss 0.5271\n",
      "Epoch 662, Loss: 0.5268, Val Loss: 0.4969\n",
      "Saving model at epoch 662 with loss 0.5268\n",
      "Epoch 663, Loss: 0.5266, Val Loss: 0.4966\n",
      "Saving model at epoch 663 with loss 0.5266\n",
      "Epoch 664, Loss: 0.5263, Val Loss: 0.4963\n",
      "Saving model at epoch 664 with loss 0.5263\n",
      "Epoch 665, Loss: 0.5260, Val Loss: 0.4960\n",
      "Saving model at epoch 665 with loss 0.5260\n",
      "Epoch 666, Loss: 0.5257, Val Loss: 0.4958\n",
      "Saving model at epoch 666 with loss 0.5257\n",
      "Epoch 667, Loss: 0.5254, Val Loss: 0.4955\n",
      "Saving model at epoch 667 with loss 0.5254\n",
      "Epoch 668, Loss: 0.5251, Val Loss: 0.4952\n",
      "Saving model at epoch 668 with loss 0.5251\n",
      "Epoch 669, Loss: 0.5248, Val Loss: 0.4949\n",
      "Saving model at epoch 669 with loss 0.5248\n",
      "Epoch 670, Loss: 0.5245, Val Loss: 0.4946\n",
      "Saving model at epoch 670 with loss 0.5245\n",
      "Epoch 671, Loss: 0.5242, Val Loss: 0.4943\n",
      "Saving model at epoch 671 with loss 0.5242\n",
      "Epoch 672, Loss: 0.5239, Val Loss: 0.4940\n",
      "Saving model at epoch 672 with loss 0.5239\n",
      "Epoch 673, Loss: 0.5236, Val Loss: 0.4937\n",
      "Saving model at epoch 673 with loss 0.5236\n",
      "Epoch 674, Loss: 0.5233, Val Loss: 0.4934\n",
      "Saving model at epoch 674 with loss 0.5233\n",
      "Epoch 675, Loss: 0.5230, Val Loss: 0.4931\n",
      "Saving model at epoch 675 with loss 0.5230\n",
      "Epoch 676, Loss: 0.5227, Val Loss: 0.4928\n",
      "Saving model at epoch 676 with loss 0.5227\n",
      "Epoch 677, Loss: 0.5224, Val Loss: 0.4925\n",
      "Saving model at epoch 677 with loss 0.5224\n",
      "Epoch 678, Loss: 0.5221, Val Loss: 0.4922\n",
      "Saving model at epoch 678 with loss 0.5221\n",
      "Epoch 679, Loss: 0.5219, Val Loss: 0.4919\n",
      "Saving model at epoch 679 with loss 0.5219\n",
      "Epoch 680, Loss: 0.5216, Val Loss: 0.4916\n",
      "Saving model at epoch 680 with loss 0.5216\n",
      "Epoch 681, Loss: 0.5213, Val Loss: 0.4913\n",
      "Saving model at epoch 681 with loss 0.5213\n",
      "Epoch 682, Loss: 0.5210, Val Loss: 0.4910\n",
      "Saving model at epoch 682 with loss 0.5210\n",
      "Epoch 683, Loss: 0.5207, Val Loss: 0.4907\n",
      "Saving model at epoch 683 with loss 0.5207\n",
      "Epoch 684, Loss: 0.5204, Val Loss: 0.4904\n",
      "Saving model at epoch 684 with loss 0.5204\n",
      "Epoch 685, Loss: 0.5201, Val Loss: 0.4902\n",
      "Saving model at epoch 685 with loss 0.5201\n",
      "Epoch 686, Loss: 0.5198, Val Loss: 0.4899\n",
      "Saving model at epoch 686 with loss 0.5198\n",
      "Epoch 687, Loss: 0.5195, Val Loss: 0.4896\n",
      "Saving model at epoch 687 with loss 0.5195\n",
      "Epoch 688, Loss: 0.5192, Val Loss: 0.4893\n",
      "Saving model at epoch 688 with loss 0.5192\n",
      "Epoch 689, Loss: 0.5189, Val Loss: 0.4890\n",
      "Saving model at epoch 689 with loss 0.5189\n",
      "Epoch 690, Loss: 0.5186, Val Loss: 0.4887\n",
      "Saving model at epoch 690 with loss 0.5186\n",
      "Epoch 691, Loss: 0.5183, Val Loss: 0.4884\n",
      "Saving model at epoch 691 with loss 0.5183\n",
      "Epoch 692, Loss: 0.5180, Val Loss: 0.4881\n",
      "Saving model at epoch 692 with loss 0.5180\n",
      "Epoch 693, Loss: 0.5178, Val Loss: 0.4878\n",
      "Saving model at epoch 693 with loss 0.5178\n",
      "Epoch 694, Loss: 0.5175, Val Loss: 0.4875\n",
      "Saving model at epoch 694 with loss 0.5175\n",
      "Epoch 695, Loss: 0.5172, Val Loss: 0.4872\n",
      "Saving model at epoch 695 with loss 0.5172\n",
      "Epoch 696, Loss: 0.5169, Val Loss: 0.4869\n",
      "Saving model at epoch 696 with loss 0.5169\n",
      "Epoch 697, Loss: 0.5166, Val Loss: 0.4866\n",
      "Saving model at epoch 697 with loss 0.5166\n",
      "Epoch 698, Loss: 0.5163, Val Loss: 0.4863\n",
      "Saving model at epoch 698 with loss 0.5163\n",
      "Epoch 699, Loss: 0.5160, Val Loss: 0.4860\n",
      "Saving model at epoch 699 with loss 0.5160\n",
      "Epoch 700, Loss: 0.5157, Val Loss: 0.4857\n",
      "Saving model at epoch 700 with loss 0.5157\n",
      "Epoch 701, Loss: 0.5154, Val Loss: 0.4855\n",
      "Saving model at epoch 701 with loss 0.5154\n",
      "Epoch 702, Loss: 0.5151, Val Loss: 0.4852\n",
      "Saving model at epoch 702 with loss 0.5151\n",
      "Epoch 703, Loss: 0.5149, Val Loss: 0.4849\n",
      "Saving model at epoch 703 with loss 0.5149\n",
      "Epoch 704, Loss: 0.5146, Val Loss: 0.4846\n",
      "Saving model at epoch 704 with loss 0.5146\n",
      "Epoch 705, Loss: 0.5143, Val Loss: 0.4843\n",
      "Saving model at epoch 705 with loss 0.5143\n",
      "Epoch 706, Loss: 0.5140, Val Loss: 0.4840\n",
      "Saving model at epoch 706 with loss 0.5140\n",
      "Epoch 707, Loss: 0.5137, Val Loss: 0.4837\n",
      "Saving model at epoch 707 with loss 0.5137\n",
      "Epoch 708, Loss: 0.5134, Val Loss: 0.4834\n",
      "Saving model at epoch 708 with loss 0.5134\n",
      "Epoch 709, Loss: 0.5131, Val Loss: 0.4831\n",
      "Saving model at epoch 709 with loss 0.5131\n",
      "Epoch 710, Loss: 0.5128, Val Loss: 0.4828\n",
      "Saving model at epoch 710 with loss 0.5128\n",
      "Epoch 711, Loss: 0.5126, Val Loss: 0.4825\n",
      "Saving model at epoch 711 with loss 0.5126\n",
      "Epoch 712, Loss: 0.5123, Val Loss: 0.4822\n",
      "Saving model at epoch 712 with loss 0.5123\n",
      "Epoch 713, Loss: 0.5120, Val Loss: 0.4820\n",
      "Saving model at epoch 713 with loss 0.5120\n",
      "Epoch 714, Loss: 0.5117, Val Loss: 0.4817\n",
      "Saving model at epoch 714 with loss 0.5117\n",
      "Epoch 715, Loss: 0.5114, Val Loss: 0.4814\n",
      "Saving model at epoch 715 with loss 0.5114\n",
      "Epoch 716, Loss: 0.5111, Val Loss: 0.4811\n",
      "Saving model at epoch 716 with loss 0.5111\n",
      "Epoch 717, Loss: 0.5108, Val Loss: 0.4808\n",
      "Saving model at epoch 717 with loss 0.5108\n",
      "Epoch 718, Loss: 0.5106, Val Loss: 0.4805\n",
      "Saving model at epoch 718 with loss 0.5106\n",
      "Epoch 719, Loss: 0.5103, Val Loss: 0.4802\n",
      "Saving model at epoch 719 with loss 0.5103\n",
      "Epoch 720, Loss: 0.5100, Val Loss: 0.4799\n",
      "Saving model at epoch 720 with loss 0.5100\n",
      "Epoch 721, Loss: 0.5097, Val Loss: 0.4796\n",
      "Saving model at epoch 721 with loss 0.5097\n",
      "Epoch 722, Loss: 0.5094, Val Loss: 0.4794\n",
      "Saving model at epoch 722 with loss 0.5094\n",
      "Epoch 723, Loss: 0.5091, Val Loss: 0.4791\n",
      "Saving model at epoch 723 with loss 0.5091\n",
      "Epoch 724, Loss: 0.5089, Val Loss: 0.4788\n",
      "Saving model at epoch 724 with loss 0.5089\n",
      "Epoch 725, Loss: 0.5086, Val Loss: 0.4785\n",
      "Saving model at epoch 725 with loss 0.5086\n",
      "Epoch 726, Loss: 0.5083, Val Loss: 0.4782\n",
      "Saving model at epoch 726 with loss 0.5083\n",
      "Epoch 727, Loss: 0.5080, Val Loss: 0.4779\n",
      "Saving model at epoch 727 with loss 0.5080\n",
      "Epoch 728, Loss: 0.5077, Val Loss: 0.4776\n",
      "Saving model at epoch 728 with loss 0.5077\n",
      "Epoch 729, Loss: 0.5075, Val Loss: 0.4774\n",
      "Saving model at epoch 729 with loss 0.5075\n",
      "Epoch 730, Loss: 0.5072, Val Loss: 0.4771\n",
      "Saving model at epoch 730 with loss 0.5072\n",
      "Epoch 731, Loss: 0.5069, Val Loss: 0.4768\n",
      "Saving model at epoch 731 with loss 0.5069\n",
      "Epoch 732, Loss: 0.5066, Val Loss: 0.4765\n",
      "Saving model at epoch 732 with loss 0.5066\n",
      "Epoch 733, Loss: 0.5064, Val Loss: 0.4762\n",
      "Saving model at epoch 733 with loss 0.5064\n",
      "Epoch 734, Loss: 0.5061, Val Loss: 0.4760\n",
      "Saving model at epoch 734 with loss 0.5061\n",
      "Epoch 735, Loss: 0.5058, Val Loss: 0.4757\n",
      "Saving model at epoch 735 with loss 0.5058\n",
      "Epoch 736, Loss: 0.5055, Val Loss: 0.4754\n",
      "Saving model at epoch 736 with loss 0.5055\n",
      "Epoch 737, Loss: 0.5053, Val Loss: 0.4751\n",
      "Saving model at epoch 737 with loss 0.5053\n",
      "Epoch 738, Loss: 0.5050, Val Loss: 0.4748\n",
      "Saving model at epoch 738 with loss 0.5050\n",
      "Epoch 739, Loss: 0.5047, Val Loss: 0.4746\n",
      "Saving model at epoch 739 with loss 0.5047\n",
      "Epoch 740, Loss: 0.5045, Val Loss: 0.4743\n",
      "Saving model at epoch 740 with loss 0.5045\n",
      "Epoch 741, Loss: 0.5042, Val Loss: 0.4740\n",
      "Saving model at epoch 741 with loss 0.5042\n",
      "Epoch 742, Loss: 0.5039, Val Loss: 0.4737\n",
      "Saving model at epoch 742 with loss 0.5039\n",
      "Epoch 743, Loss: 0.5036, Val Loss: 0.4735\n",
      "Saving model at epoch 743 with loss 0.5036\n",
      "Epoch 744, Loss: 0.5034, Val Loss: 0.4732\n",
      "Saving model at epoch 744 with loss 0.5034\n",
      "Epoch 745, Loss: 0.5031, Val Loss: 0.4729\n",
      "Saving model at epoch 745 with loss 0.5031\n",
      "Epoch 746, Loss: 0.5028, Val Loss: 0.4726\n",
      "Saving model at epoch 746 with loss 0.5028\n",
      "Epoch 747, Loss: 0.5026, Val Loss: 0.4724\n",
      "Saving model at epoch 747 with loss 0.5026\n",
      "Epoch 748, Loss: 0.5023, Val Loss: 0.4721\n",
      "Saving model at epoch 748 with loss 0.5023\n",
      "Epoch 749, Loss: 0.5020, Val Loss: 0.4718\n",
      "Saving model at epoch 749 with loss 0.5020\n",
      "Epoch 750, Loss: 0.5018, Val Loss: 0.4716\n",
      "Saving model at epoch 750 with loss 0.5018\n",
      "Epoch 751, Loss: 0.5015, Val Loss: 0.4713\n",
      "Saving model at epoch 751 with loss 0.5015\n",
      "Epoch 752, Loss: 0.5013, Val Loss: 0.4710\n",
      "Saving model at epoch 752 with loss 0.5013\n",
      "Epoch 753, Loss: 0.5010, Val Loss: 0.4708\n",
      "Saving model at epoch 753 with loss 0.5010\n",
      "Epoch 754, Loss: 0.5007, Val Loss: 0.4705\n",
      "Saving model at epoch 754 with loss 0.5007\n",
      "Epoch 755, Loss: 0.5005, Val Loss: 0.4702\n",
      "Saving model at epoch 755 with loss 0.5005\n",
      "Epoch 756, Loss: 0.5002, Val Loss: 0.4700\n",
      "Saving model at epoch 756 with loss 0.5002\n",
      "Epoch 757, Loss: 0.4999, Val Loss: 0.4697\n",
      "Saving model at epoch 757 with loss 0.4999\n",
      "Epoch 758, Loss: 0.4997, Val Loss: 0.4694\n",
      "Saving model at epoch 758 with loss 0.4997\n",
      "Epoch 759, Loss: 0.4994, Val Loss: 0.4692\n",
      "Saving model at epoch 759 with loss 0.4994\n",
      "Epoch 760, Loss: 0.4992, Val Loss: 0.4689\n",
      "Saving model at epoch 760 with loss 0.4992\n",
      "Epoch 761, Loss: 0.4989, Val Loss: 0.4686\n",
      "Saving model at epoch 761 with loss 0.4989\n",
      "Epoch 762, Loss: 0.4986, Val Loss: 0.4684\n",
      "Saving model at epoch 762 with loss 0.4986\n",
      "Epoch 763, Loss: 0.4984, Val Loss: 0.4681\n",
      "Saving model at epoch 763 with loss 0.4984\n",
      "Epoch 764, Loss: 0.4981, Val Loss: 0.4678\n",
      "Saving model at epoch 764 with loss 0.4981\n",
      "Epoch 765, Loss: 0.4979, Val Loss: 0.4676\n",
      "Saving model at epoch 765 with loss 0.4979\n",
      "Epoch 766, Loss: 0.4976, Val Loss: 0.4673\n",
      "Saving model at epoch 766 with loss 0.4976\n",
      "Epoch 767, Loss: 0.4974, Val Loss: 0.4670\n",
      "Saving model at epoch 767 with loss 0.4974\n",
      "Epoch 768, Loss: 0.4971, Val Loss: 0.4668\n",
      "Saving model at epoch 768 with loss 0.4971\n",
      "Epoch 769, Loss: 0.4969, Val Loss: 0.4665\n",
      "Saving model at epoch 769 with loss 0.4969\n",
      "Epoch 770, Loss: 0.4966, Val Loss: 0.4663\n",
      "Saving model at epoch 770 with loss 0.4966\n",
      "Epoch 771, Loss: 0.4964, Val Loss: 0.4660\n",
      "Saving model at epoch 771 with loss 0.4964\n",
      "Epoch 772, Loss: 0.4961, Val Loss: 0.4657\n",
      "Saving model at epoch 772 with loss 0.4961\n",
      "Epoch 773, Loss: 0.4959, Val Loss: 0.4655\n",
      "Saving model at epoch 773 with loss 0.4959\n",
      "Epoch 774, Loss: 0.4956, Val Loss: 0.4652\n",
      "Saving model at epoch 774 with loss 0.4956\n",
      "Epoch 775, Loss: 0.4954, Val Loss: 0.4650\n",
      "Saving model at epoch 775 with loss 0.4954\n",
      "Epoch 776, Loss: 0.4951, Val Loss: 0.4647\n",
      "Saving model at epoch 776 with loss 0.4951\n",
      "Epoch 777, Loss: 0.4949, Val Loss: 0.4645\n",
      "Saving model at epoch 777 with loss 0.4949\n",
      "Epoch 778, Loss: 0.4946, Val Loss: 0.4642\n",
      "Saving model at epoch 778 with loss 0.4946\n",
      "Epoch 779, Loss: 0.4944, Val Loss: 0.4640\n",
      "Saving model at epoch 779 with loss 0.4944\n",
      "Epoch 780, Loss: 0.4941, Val Loss: 0.4637\n",
      "Saving model at epoch 780 with loss 0.4941\n",
      "Epoch 781, Loss: 0.4939, Val Loss: 0.4635\n",
      "Saving model at epoch 781 with loss 0.4939\n",
      "Epoch 782, Loss: 0.4936, Val Loss: 0.4632\n",
      "Saving model at epoch 782 with loss 0.4936\n",
      "Epoch 783, Loss: 0.4934, Val Loss: 0.4630\n",
      "Saving model at epoch 783 with loss 0.4934\n",
      "Epoch 784, Loss: 0.4932, Val Loss: 0.4627\n",
      "Saving model at epoch 784 with loss 0.4932\n",
      "Epoch 785, Loss: 0.4929, Val Loss: 0.4625\n",
      "Saving model at epoch 785 with loss 0.4929\n",
      "Epoch 786, Loss: 0.4927, Val Loss: 0.4622\n",
      "Saving model at epoch 786 with loss 0.4927\n",
      "Epoch 787, Loss: 0.4925, Val Loss: 0.4620\n",
      "Saving model at epoch 787 with loss 0.4925\n",
      "Epoch 788, Loss: 0.4922, Val Loss: 0.4618\n",
      "Saving model at epoch 788 with loss 0.4922\n",
      "Epoch 789, Loss: 0.4920, Val Loss: 0.4615\n",
      "Saving model at epoch 789 with loss 0.4920\n",
      "Epoch 790, Loss: 0.4917, Val Loss: 0.4613\n",
      "Saving model at epoch 790 with loss 0.4917\n",
      "Epoch 791, Loss: 0.4915, Val Loss: 0.4610\n",
      "Saving model at epoch 791 with loss 0.4915\n",
      "Epoch 792, Loss: 0.4913, Val Loss: 0.4608\n",
      "Saving model at epoch 792 with loss 0.4913\n",
      "Epoch 793, Loss: 0.4910, Val Loss: 0.4606\n",
      "Saving model at epoch 793 with loss 0.4910\n",
      "Epoch 794, Loss: 0.4908, Val Loss: 0.4603\n",
      "Saving model at epoch 794 with loss 0.4908\n",
      "Epoch 795, Loss: 0.4906, Val Loss: 0.4601\n",
      "Saving model at epoch 795 with loss 0.4906\n",
      "Epoch 796, Loss: 0.4904, Val Loss: 0.4598\n",
      "Saving model at epoch 796 with loss 0.4904\n",
      "Epoch 797, Loss: 0.4901, Val Loss: 0.4596\n",
      "Saving model at epoch 797 with loss 0.4901\n",
      "Epoch 798, Loss: 0.4899, Val Loss: 0.4594\n",
      "Saving model at epoch 798 with loss 0.4899\n",
      "Epoch 799, Loss: 0.4897, Val Loss: 0.4592\n",
      "Saving model at epoch 799 with loss 0.4897\n",
      "Epoch 800, Loss: 0.4894, Val Loss: 0.4589\n",
      "Saving model at epoch 800 with loss 0.4894\n",
      "Epoch 801, Loss: 0.4892, Val Loss: 0.4587\n",
      "Saving model at epoch 801 with loss 0.4892\n",
      "Epoch 802, Loss: 0.4890, Val Loss: 0.4585\n",
      "Saving model at epoch 802 with loss 0.4890\n",
      "Epoch 803, Loss: 0.4888, Val Loss: 0.4582\n",
      "Saving model at epoch 803 with loss 0.4888\n",
      "Epoch 804, Loss: 0.4886, Val Loss: 0.4580\n",
      "Saving model at epoch 804 with loss 0.4886\n",
      "Epoch 805, Loss: 0.4883, Val Loss: 0.4578\n",
      "Saving model at epoch 805 with loss 0.4883\n",
      "Epoch 806, Loss: 0.4881, Val Loss: 0.4576\n",
      "Saving model at epoch 806 with loss 0.4881\n",
      "Epoch 807, Loss: 0.4879, Val Loss: 0.4573\n",
      "Saving model at epoch 807 with loss 0.4879\n",
      "Epoch 808, Loss: 0.4877, Val Loss: 0.4571\n",
      "Saving model at epoch 808 with loss 0.4877\n",
      "Epoch 809, Loss: 0.4875, Val Loss: 0.4569\n",
      "Saving model at epoch 809 with loss 0.4875\n",
      "Epoch 810, Loss: 0.4872, Val Loss: 0.4567\n",
      "Saving model at epoch 810 with loss 0.4872\n",
      "Epoch 811, Loss: 0.4870, Val Loss: 0.4565\n",
      "Saving model at epoch 811 with loss 0.4870\n",
      "Epoch 812, Loss: 0.4868, Val Loss: 0.4562\n",
      "Saving model at epoch 812 with loss 0.4868\n",
      "Epoch 813, Loss: 0.4866, Val Loss: 0.4560\n",
      "Saving model at epoch 813 with loss 0.4866\n",
      "Epoch 814, Loss: 0.4864, Val Loss: 0.4558\n",
      "Saving model at epoch 814 with loss 0.4864\n",
      "Epoch 815, Loss: 0.4862, Val Loss: 0.4556\n",
      "Saving model at epoch 815 with loss 0.4862\n",
      "Epoch 816, Loss: 0.4860, Val Loss: 0.4554\n",
      "Saving model at epoch 816 with loss 0.4860\n",
      "Epoch 817, Loss: 0.4858, Val Loss: 0.4552\n",
      "Saving model at epoch 817 with loss 0.4858\n",
      "Epoch 818, Loss: 0.4856, Val Loss: 0.4550\n",
      "Saving model at epoch 818 with loss 0.4856\n",
      "Epoch 819, Loss: 0.4853, Val Loss: 0.4548\n",
      "Saving model at epoch 819 with loss 0.4853\n",
      "Epoch 820, Loss: 0.4851, Val Loss: 0.4545\n",
      "Saving model at epoch 820 with loss 0.4851\n",
      "Epoch 821, Loss: 0.4849, Val Loss: 0.4543\n",
      "Saving model at epoch 821 with loss 0.4849\n",
      "Epoch 822, Loss: 0.4847, Val Loss: 0.4541\n",
      "Saving model at epoch 822 with loss 0.4847\n",
      "Epoch 823, Loss: 0.4845, Val Loss: 0.4539\n",
      "Saving model at epoch 823 with loss 0.4845\n",
      "Epoch 824, Loss: 0.4843, Val Loss: 0.4537\n",
      "Saving model at epoch 824 with loss 0.4843\n",
      "Epoch 825, Loss: 0.4841, Val Loss: 0.4535\n",
      "Saving model at epoch 825 with loss 0.4841\n",
      "Epoch 826, Loss: 0.4839, Val Loss: 0.4533\n",
      "Saving model at epoch 826 with loss 0.4839\n",
      "Epoch 827, Loss: 0.4837, Val Loss: 0.4531\n",
      "Saving model at epoch 827 with loss 0.4837\n",
      "Epoch 828, Loss: 0.4835, Val Loss: 0.4529\n",
      "Saving model at epoch 828 with loss 0.4835\n",
      "Epoch 829, Loss: 0.4833, Val Loss: 0.4527\n",
      "Saving model at epoch 829 with loss 0.4833\n",
      "Epoch 830, Loss: 0.4831, Val Loss: 0.4525\n",
      "Saving model at epoch 830 with loss 0.4831\n",
      "Epoch 831, Loss: 0.4829, Val Loss: 0.4523\n",
      "Saving model at epoch 831 with loss 0.4829\n",
      "Epoch 832, Loss: 0.4827, Val Loss: 0.4521\n",
      "Saving model at epoch 832 with loss 0.4827\n",
      "Epoch 833, Loss: 0.4825, Val Loss: 0.4519\n",
      "Saving model at epoch 833 with loss 0.4825\n",
      "Epoch 834, Loss: 0.4824, Val Loss: 0.4517\n",
      "Saving model at epoch 834 with loss 0.4824\n",
      "Epoch 835, Loss: 0.4822, Val Loss: 0.4515\n",
      "Saving model at epoch 835 with loss 0.4822\n",
      "Epoch 836, Loss: 0.4820, Val Loss: 0.4513\n",
      "Saving model at epoch 836 with loss 0.4820\n",
      "Epoch 837, Loss: 0.4818, Val Loss: 0.4511\n",
      "Saving model at epoch 837 with loss 0.4818\n",
      "Epoch 838, Loss: 0.4816, Val Loss: 0.4509\n",
      "Saving model at epoch 838 with loss 0.4816\n",
      "Epoch 839, Loss: 0.4814, Val Loss: 0.4508\n",
      "Saving model at epoch 839 with loss 0.4814\n",
      "Epoch 840, Loss: 0.4812, Val Loss: 0.4506\n",
      "Saving model at epoch 840 with loss 0.4812\n",
      "Epoch 841, Loss: 0.4810, Val Loss: 0.4504\n",
      "Saving model at epoch 841 with loss 0.4810\n",
      "Epoch 842, Loss: 0.4808, Val Loss: 0.4502\n",
      "Saving model at epoch 842 with loss 0.4808\n",
      "Epoch 843, Loss: 0.4807, Val Loss: 0.4500\n",
      "Saving model at epoch 843 with loss 0.4807\n",
      "Epoch 844, Loss: 0.4805, Val Loss: 0.4498\n",
      "Saving model at epoch 844 with loss 0.4805\n",
      "Epoch 845, Loss: 0.4803, Val Loss: 0.4496\n",
      "Saving model at epoch 845 with loss 0.4803\n",
      "Epoch 846, Loss: 0.4801, Val Loss: 0.4494\n",
      "Saving model at epoch 846 with loss 0.4801\n",
      "Epoch 847, Loss: 0.4799, Val Loss: 0.4493\n",
      "Saving model at epoch 847 with loss 0.4799\n",
      "Epoch 848, Loss: 0.4797, Val Loss: 0.4491\n",
      "Saving model at epoch 848 with loss 0.4797\n",
      "Epoch 849, Loss: 0.4796, Val Loss: 0.4489\n",
      "Saving model at epoch 849 with loss 0.4796\n",
      "Epoch 850, Loss: 0.4794, Val Loss: 0.4487\n",
      "Saving model at epoch 850 with loss 0.4794\n",
      "Epoch 851, Loss: 0.4792, Val Loss: 0.4485\n",
      "Saving model at epoch 851 with loss 0.4792\n",
      "Epoch 852, Loss: 0.4790, Val Loss: 0.4483\n",
      "Saving model at epoch 852 with loss 0.4790\n",
      "Epoch 853, Loss: 0.4789, Val Loss: 0.4482\n",
      "Saving model at epoch 853 with loss 0.4789\n",
      "Epoch 854, Loss: 0.4787, Val Loss: 0.4480\n",
      "Saving model at epoch 854 with loss 0.4787\n",
      "Epoch 855, Loss: 0.4785, Val Loss: 0.4478\n",
      "Saving model at epoch 855 with loss 0.4785\n",
      "Epoch 856, Loss: 0.4783, Val Loss: 0.4476\n",
      "Saving model at epoch 856 with loss 0.4783\n",
      "Epoch 857, Loss: 0.4782, Val Loss: 0.4475\n",
      "Saving model at epoch 857 with loss 0.4782\n",
      "Epoch 858, Loss: 0.4780, Val Loss: 0.4473\n",
      "Saving model at epoch 858 with loss 0.4780\n",
      "Epoch 859, Loss: 0.4778, Val Loss: 0.4471\n",
      "Saving model at epoch 859 with loss 0.4778\n",
      "Epoch 860, Loss: 0.4777, Val Loss: 0.4470\n",
      "Saving model at epoch 860 with loss 0.4777\n",
      "Epoch 861, Loss: 0.4775, Val Loss: 0.4468\n",
      "Saving model at epoch 861 with loss 0.4775\n",
      "Epoch 862, Loss: 0.4773, Val Loss: 0.4466\n",
      "Saving model at epoch 862 with loss 0.4773\n",
      "Epoch 863, Loss: 0.4772, Val Loss: 0.4464\n",
      "Saving model at epoch 863 with loss 0.4772\n",
      "Epoch 864, Loss: 0.4770, Val Loss: 0.4463\n",
      "Saving model at epoch 864 with loss 0.4770\n",
      "Epoch 865, Loss: 0.4768, Val Loss: 0.4461\n",
      "Saving model at epoch 865 with loss 0.4768\n",
      "Epoch 866, Loss: 0.4767, Val Loss: 0.4459\n",
      "Saving model at epoch 866 with loss 0.4767\n",
      "Epoch 867, Loss: 0.4765, Val Loss: 0.4458\n",
      "Saving model at epoch 867 with loss 0.4765\n",
      "Epoch 868, Loss: 0.4763, Val Loss: 0.4456\n",
      "Saving model at epoch 868 with loss 0.4763\n",
      "Epoch 869, Loss: 0.4762, Val Loss: 0.4454\n",
      "Saving model at epoch 869 with loss 0.4762\n",
      "Epoch 870, Loss: 0.4760, Val Loss: 0.4453\n",
      "Saving model at epoch 870 with loss 0.4760\n",
      "Epoch 871, Loss: 0.4759, Val Loss: 0.4451\n",
      "Saving model at epoch 871 with loss 0.4759\n",
      "Epoch 872, Loss: 0.4757, Val Loss: 0.4450\n",
      "Saving model at epoch 872 with loss 0.4757\n",
      "Epoch 873, Loss: 0.4755, Val Loss: 0.4448\n",
      "Saving model at epoch 873 with loss 0.4755\n",
      "Epoch 874, Loss: 0.4754, Val Loss: 0.4446\n",
      "Saving model at epoch 874 with loss 0.4754\n",
      "Epoch 875, Loss: 0.4752, Val Loss: 0.4445\n",
      "Saving model at epoch 875 with loss 0.4752\n",
      "Epoch 876, Loss: 0.4751, Val Loss: 0.4443\n",
      "Saving model at epoch 876 with loss 0.4751\n",
      "Epoch 877, Loss: 0.4749, Val Loss: 0.4442\n",
      "Saving model at epoch 877 with loss 0.4749\n",
      "Epoch 878, Loss: 0.4748, Val Loss: 0.4440\n",
      "Saving model at epoch 878 with loss 0.4748\n",
      "Epoch 879, Loss: 0.4746, Val Loss: 0.4439\n",
      "Saving model at epoch 879 with loss 0.4746\n",
      "Epoch 880, Loss: 0.4745, Val Loss: 0.4437\n",
      "Saving model at epoch 880 with loss 0.4745\n",
      "Epoch 881, Loss: 0.4743, Val Loss: 0.4435\n",
      "Saving model at epoch 881 with loss 0.4743\n",
      "Epoch 882, Loss: 0.4742, Val Loss: 0.4434\n",
      "Saving model at epoch 882 with loss 0.4742\n",
      "Epoch 883, Loss: 0.4740, Val Loss: 0.4432\n",
      "Saving model at epoch 883 with loss 0.4740\n",
      "Epoch 884, Loss: 0.4739, Val Loss: 0.4431\n",
      "Saving model at epoch 884 with loss 0.4739\n",
      "Epoch 885, Loss: 0.4737, Val Loss: 0.4429\n",
      "Saving model at epoch 885 with loss 0.4737\n",
      "Epoch 886, Loss: 0.4736, Val Loss: 0.4428\n",
      "Saving model at epoch 886 with loss 0.4736\n",
      "Epoch 887, Loss: 0.4734, Val Loss: 0.4426\n",
      "Saving model at epoch 887 with loss 0.4734\n",
      "Epoch 888, Loss: 0.4733, Val Loss: 0.4425\n",
      "Saving model at epoch 888 with loss 0.4733\n",
      "Epoch 889, Loss: 0.4731, Val Loss: 0.4423\n",
      "Saving model at epoch 889 with loss 0.4731\n",
      "Epoch 890, Loss: 0.4730, Val Loss: 0.4422\n",
      "Saving model at epoch 890 with loss 0.4730\n",
      "Epoch 891, Loss: 0.4728, Val Loss: 0.4421\n",
      "Saving model at epoch 891 with loss 0.4728\n",
      "Epoch 892, Loss: 0.4727, Val Loss: 0.4419\n",
      "Saving model at epoch 892 with loss 0.4727\n",
      "Epoch 893, Loss: 0.4725, Val Loss: 0.4418\n",
      "Saving model at epoch 893 with loss 0.4725\n",
      "Epoch 894, Loss: 0.4724, Val Loss: 0.4416\n",
      "Saving model at epoch 894 with loss 0.4724\n",
      "Epoch 895, Loss: 0.4723, Val Loss: 0.4415\n",
      "Saving model at epoch 895 with loss 0.4723\n",
      "Epoch 896, Loss: 0.4721, Val Loss: 0.4413\n",
      "Saving model at epoch 896 with loss 0.4721\n",
      "Epoch 897, Loss: 0.4720, Val Loss: 0.4412\n",
      "Saving model at epoch 897 with loss 0.4720\n",
      "Epoch 898, Loss: 0.4718, Val Loss: 0.4411\n",
      "Saving model at epoch 898 with loss 0.4718\n",
      "Epoch 899, Loss: 0.4717, Val Loss: 0.4409\n",
      "Saving model at epoch 899 with loss 0.4717\n",
      "Epoch 900, Loss: 0.4716, Val Loss: 0.4408\n",
      "Saving model at epoch 900 with loss 0.4716\n",
      "Epoch 901, Loss: 0.4714, Val Loss: 0.4406\n",
      "Saving model at epoch 901 with loss 0.4714\n",
      "Epoch 902, Loss: 0.4713, Val Loss: 0.4405\n",
      "Saving model at epoch 902 with loss 0.4713\n",
      "Epoch 903, Loss: 0.4712, Val Loss: 0.4404\n",
      "Saving model at epoch 903 with loss 0.4712\n",
      "Epoch 904, Loss: 0.4710, Val Loss: 0.4402\n",
      "Saving model at epoch 904 with loss 0.4710\n",
      "Epoch 905, Loss: 0.4709, Val Loss: 0.4401\n",
      "Saving model at epoch 905 with loss 0.4709\n",
      "Epoch 906, Loss: 0.4708, Val Loss: 0.4400\n",
      "Saving model at epoch 906 with loss 0.4708\n",
      "Epoch 907, Loss: 0.4706, Val Loss: 0.4398\n",
      "Saving model at epoch 907 with loss 0.4706\n",
      "Epoch 908, Loss: 0.4705, Val Loss: 0.4397\n",
      "Saving model at epoch 908 with loss 0.4705\n",
      "Epoch 909, Loss: 0.4704, Val Loss: 0.4396\n",
      "Saving model at epoch 909 with loss 0.4704\n",
      "Epoch 910, Loss: 0.4702, Val Loss: 0.4394\n",
      "Saving model at epoch 910 with loss 0.4702\n",
      "Epoch 911, Loss: 0.4701, Val Loss: 0.4393\n",
      "Saving model at epoch 911 with loss 0.4701\n",
      "Epoch 912, Loss: 0.4700, Val Loss: 0.4392\n",
      "Saving model at epoch 912 with loss 0.4700\n",
      "Epoch 913, Loss: 0.4698, Val Loss: 0.4390\n",
      "Saving model at epoch 913 with loss 0.4698\n",
      "Epoch 914, Loss: 0.4697, Val Loss: 0.4389\n",
      "Saving model at epoch 914 with loss 0.4697\n",
      "Epoch 915, Loss: 0.4696, Val Loss: 0.4388\n",
      "Saving model at epoch 915 with loss 0.4696\n",
      "Epoch 916, Loss: 0.4695, Val Loss: 0.4386\n",
      "Saving model at epoch 916 with loss 0.4695\n",
      "Epoch 917, Loss: 0.4693, Val Loss: 0.4385\n",
      "Saving model at epoch 917 with loss 0.4693\n",
      "Epoch 918, Loss: 0.4692, Val Loss: 0.4384\n",
      "Saving model at epoch 918 with loss 0.4692\n",
      "Epoch 919, Loss: 0.4691, Val Loss: 0.4383\n",
      "Saving model at epoch 919 with loss 0.4691\n",
      "Epoch 920, Loss: 0.4690, Val Loss: 0.4381\n",
      "Saving model at epoch 920 with loss 0.4690\n",
      "Epoch 921, Loss: 0.4688, Val Loss: 0.4380\n",
      "Saving model at epoch 921 with loss 0.4688\n",
      "Epoch 922, Loss: 0.4687, Val Loss: 0.4379\n",
      "Saving model at epoch 922 with loss 0.4687\n",
      "Epoch 923, Loss: 0.4686, Val Loss: 0.4378\n",
      "Saving model at epoch 923 with loss 0.4686\n",
      "Epoch 924, Loss: 0.4685, Val Loss: 0.4377\n",
      "Saving model at epoch 924 with loss 0.4685\n",
      "Epoch 925, Loss: 0.4684, Val Loss: 0.4375\n",
      "Saving model at epoch 925 with loss 0.4684\n",
      "Epoch 926, Loss: 0.4682, Val Loss: 0.4374\n",
      "Saving model at epoch 926 with loss 0.4682\n",
      "Epoch 927, Loss: 0.4681, Val Loss: 0.4373\n",
      "Saving model at epoch 927 with loss 0.4681\n",
      "Epoch 928, Loss: 0.4680, Val Loss: 0.4372\n",
      "Saving model at epoch 928 with loss 0.4680\n",
      "Epoch 929, Loss: 0.4679, Val Loss: 0.4371\n",
      "Saving model at epoch 929 with loss 0.4679\n",
      "Epoch 930, Loss: 0.4678, Val Loss: 0.4369\n",
      "Saving model at epoch 930 with loss 0.4678\n",
      "Epoch 931, Loss: 0.4676, Val Loss: 0.4368\n",
      "Saving model at epoch 931 with loss 0.4676\n",
      "Epoch 932, Loss: 0.4675, Val Loss: 0.4367\n",
      "Saving model at epoch 932 with loss 0.4675\n",
      "Epoch 933, Loss: 0.4674, Val Loss: 0.4366\n",
      "Saving model at epoch 933 with loss 0.4674\n",
      "Epoch 934, Loss: 0.4673, Val Loss: 0.4365\n",
      "Saving model at epoch 934 with loss 0.4673\n",
      "Epoch 935, Loss: 0.4672, Val Loss: 0.4364\n",
      "Saving model at epoch 935 with loss 0.4672\n",
      "Epoch 936, Loss: 0.4671, Val Loss: 0.4362\n",
      "Saving model at epoch 936 with loss 0.4671\n",
      "Epoch 937, Loss: 0.4670, Val Loss: 0.4361\n",
      "Saving model at epoch 937 with loss 0.4670\n",
      "Epoch 938, Loss: 0.4668, Val Loss: 0.4360\n",
      "Saving model at epoch 938 with loss 0.4668\n",
      "Epoch 939, Loss: 0.4667, Val Loss: 0.4359\n",
      "Saving model at epoch 939 with loss 0.4667\n",
      "Epoch 940, Loss: 0.4666, Val Loss: 0.4358\n",
      "Saving model at epoch 940 with loss 0.4666\n",
      "Epoch 941, Loss: 0.4665, Val Loss: 0.4357\n",
      "Saving model at epoch 941 with loss 0.4665\n",
      "Epoch 942, Loss: 0.4664, Val Loss: 0.4356\n",
      "Saving model at epoch 942 with loss 0.4664\n",
      "Epoch 943, Loss: 0.4663, Val Loss: 0.4355\n",
      "Saving model at epoch 943 with loss 0.4663\n",
      "Epoch 944, Loss: 0.4662, Val Loss: 0.4353\n",
      "Saving model at epoch 944 with loss 0.4662\n",
      "Epoch 945, Loss: 0.4661, Val Loss: 0.4352\n",
      "Saving model at epoch 945 with loss 0.4661\n",
      "Epoch 946, Loss: 0.4660, Val Loss: 0.4351\n",
      "Saving model at epoch 946 with loss 0.4660\n",
      "Epoch 947, Loss: 0.4658, Val Loss: 0.4350\n",
      "Saving model at epoch 947 with loss 0.4658\n",
      "Epoch 948, Loss: 0.4657, Val Loss: 0.4349\n",
      "Saving model at epoch 948 with loss 0.4657\n",
      "Epoch 949, Loss: 0.4656, Val Loss: 0.4348\n",
      "Saving model at epoch 949 with loss 0.4656\n",
      "Epoch 950, Loss: 0.4655, Val Loss: 0.4347\n",
      "Saving model at epoch 950 with loss 0.4655\n",
      "Epoch 951, Loss: 0.4654, Val Loss: 0.4346\n",
      "Saving model at epoch 951 with loss 0.4654\n",
      "Epoch 952, Loss: 0.4653, Val Loss: 0.4345\n",
      "Saving model at epoch 952 with loss 0.4653\n",
      "Epoch 953, Loss: 0.4652, Val Loss: 0.4344\n",
      "Saving model at epoch 953 with loss 0.4652\n",
      "Epoch 954, Loss: 0.4651, Val Loss: 0.4343\n",
      "Saving model at epoch 954 with loss 0.4651\n",
      "Epoch 955, Loss: 0.4650, Val Loss: 0.4342\n",
      "Saving model at epoch 955 with loss 0.4650\n",
      "Epoch 956, Loss: 0.4649, Val Loss: 0.4341\n",
      "Saving model at epoch 956 with loss 0.4649\n",
      "Epoch 957, Loss: 0.4648, Val Loss: 0.4340\n",
      "Saving model at epoch 957 with loss 0.4648\n",
      "Epoch 958, Loss: 0.4647, Val Loss: 0.4339\n",
      "Saving model at epoch 958 with loss 0.4647\n",
      "Epoch 959, Loss: 0.4646, Val Loss: 0.4338\n",
      "Saving model at epoch 959 with loss 0.4646\n",
      "Epoch 960, Loss: 0.4645, Val Loss: 0.4336\n",
      "Saving model at epoch 960 with loss 0.4645\n",
      "Epoch 961, Loss: 0.4644, Val Loss: 0.4335\n",
      "Saving model at epoch 961 with loss 0.4644\n",
      "Epoch 962, Loss: 0.4643, Val Loss: 0.4334\n",
      "Saving model at epoch 962 with loss 0.4643\n",
      "Epoch 963, Loss: 0.4642, Val Loss: 0.4333\n",
      "Saving model at epoch 963 with loss 0.4642\n",
      "Epoch 964, Loss: 0.4641, Val Loss: 0.4332\n",
      "Saving model at epoch 964 with loss 0.4641\n",
      "Epoch 965, Loss: 0.4640, Val Loss: 0.4331\n",
      "Saving model at epoch 965 with loss 0.4640\n",
      "Epoch 966, Loss: 0.4639, Val Loss: 0.4330\n",
      "Saving model at epoch 966 with loss 0.4639\n",
      "Epoch 967, Loss: 0.4638, Val Loss: 0.4329\n",
      "Saving model at epoch 967 with loss 0.4638\n",
      "Epoch 968, Loss: 0.4637, Val Loss: 0.4328\n",
      "Saving model at epoch 968 with loss 0.4637\n",
      "Epoch 969, Loss: 0.4636, Val Loss: 0.4327\n",
      "Saving model at epoch 969 with loss 0.4636\n",
      "Epoch 970, Loss: 0.4635, Val Loss: 0.4327\n",
      "Saving model at epoch 970 with loss 0.4635\n",
      "Epoch 971, Loss: 0.4634, Val Loss: 0.4326\n",
      "Saving model at epoch 971 with loss 0.4634\n",
      "Epoch 972, Loss: 0.4633, Val Loss: 0.4325\n",
      "Saving model at epoch 972 with loss 0.4633\n",
      "Epoch 973, Loss: 0.4632, Val Loss: 0.4324\n",
      "Saving model at epoch 973 with loss 0.4632\n",
      "Epoch 974, Loss: 0.4631, Val Loss: 0.4323\n",
      "Saving model at epoch 974 with loss 0.4631\n",
      "Epoch 975, Loss: 0.4630, Val Loss: 0.4322\n",
      "Saving model at epoch 975 with loss 0.4630\n",
      "Epoch 976, Loss: 0.4629, Val Loss: 0.4321\n",
      "Saving model at epoch 976 with loss 0.4629\n",
      "Epoch 977, Loss: 0.4628, Val Loss: 0.4320\n",
      "Saving model at epoch 977 with loss 0.4628\n",
      "Epoch 978, Loss: 0.4627, Val Loss: 0.4319\n",
      "Saving model at epoch 978 with loss 0.4627\n",
      "Epoch 979, Loss: 0.4626, Val Loss: 0.4318\n",
      "Saving model at epoch 979 with loss 0.4626\n",
      "Epoch 980, Loss: 0.4625, Val Loss: 0.4317\n",
      "Saving model at epoch 980 with loss 0.4625\n",
      "Epoch 981, Loss: 0.4624, Val Loss: 0.4316\n",
      "Saving model at epoch 981 with loss 0.4624\n",
      "Epoch 982, Loss: 0.4623, Val Loss: 0.4315\n",
      "Saving model at epoch 982 with loss 0.4623\n",
      "Epoch 983, Loss: 0.4622, Val Loss: 0.4314\n",
      "Saving model at epoch 983 with loss 0.4622\n",
      "Epoch 984, Loss: 0.4621, Val Loss: 0.4313\n",
      "Saving model at epoch 984 with loss 0.4621\n",
      "Epoch 985, Loss: 0.4620, Val Loss: 0.4312\n",
      "Saving model at epoch 985 with loss 0.4620\n",
      "Epoch 986, Loss: 0.4620, Val Loss: 0.4311\n",
      "Saving model at epoch 986 with loss 0.4620\n",
      "Epoch 987, Loss: 0.4619, Val Loss: 0.4310\n",
      "Saving model at epoch 987 with loss 0.4619\n",
      "Epoch 988, Loss: 0.4618, Val Loss: 0.4310\n",
      "Saving model at epoch 988 with loss 0.4618\n",
      "Epoch 989, Loss: 0.4617, Val Loss: 0.4309\n",
      "Saving model at epoch 989 with loss 0.4617\n",
      "Epoch 990, Loss: 0.4616, Val Loss: 0.4308\n",
      "Saving model at epoch 990 with loss 0.4616\n",
      "Epoch 991, Loss: 0.4615, Val Loss: 0.4307\n",
      "Saving model at epoch 991 with loss 0.4615\n",
      "Epoch 992, Loss: 0.4614, Val Loss: 0.4306\n",
      "Saving model at epoch 992 with loss 0.4614\n",
      "Epoch 993, Loss: 0.4613, Val Loss: 0.4305\n",
      "Saving model at epoch 993 with loss 0.4613\n",
      "Epoch 994, Loss: 0.4612, Val Loss: 0.4304\n",
      "Saving model at epoch 994 with loss 0.4612\n",
      "Epoch 995, Loss: 0.4611, Val Loss: 0.4303\n",
      "Saving model at epoch 995 with loss 0.4611\n",
      "Epoch 996, Loss: 0.4610, Val Loss: 0.4302\n",
      "Saving model at epoch 996 with loss 0.4610\n",
      "Epoch 997, Loss: 0.4610, Val Loss: 0.4301\n",
      "Saving model at epoch 997 with loss 0.4610\n",
      "Epoch 998, Loss: 0.4609, Val Loss: 0.4301\n",
      "Saving model at epoch 998 with loss 0.4609\n",
      "Epoch 999, Loss: 0.4608, Val Loss: 0.4300\n",
      "Saving model at epoch 999 with loss 0.4608\n",
      "Test Loss (MSE): 0.5177\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the features we want to keep\n",
    "# Keep only pickup and drop off positions, and add pickup and dropoff month, day, hour, and day of week\n",
    "features_model3 = [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\",\n",
    "                   \"pickup_month\", \"pickup_day\", \"pickup_hour\", \"dropoff_month\", \"dropoff_day\", \"dropoff_hour\", \n",
    "                   \"pickup_day_of_week\", \"dropoff_day_of_week\"]\n",
    "\n",
    "# Split 10% of the training data as validation set\n",
    "model3_x = select_features(X_train, features_model3)\n",
    "model3_y = y_train\n",
    "model3_x_train, model3_x_val, model3_y_train, model3_y_val = train_test_split(model3_x, model3_y, test_size=0.1, random_state=0)\n",
    "model3_x_test = select_features(X_test, features_model3)\n",
    "\n",
    "# Define Model\n",
    "model3 = Sequential()\n",
    "model3.add(LinearLayer(model3_x_train.shape[1], 12))\n",
    "model3.add(ReLU())\n",
    "model3.add(LinearLayer(12, 12))\n",
    "model3.add(ReLU())\n",
    "model3.add(LinearLayer(12, 12))\n",
    "model3.add(ReLU())\n",
    "model3.add(LinearLayer(12, 1))\n",
    "\n",
    "# Training Setup\n",
    "epochs_model3 = 1000\n",
    "learning_rate_model3 = 0.01\n",
    "loss_function_model3 = MeanSquaredErrorLoss()\n",
    "\n",
    "best_loss_model3 = float(\"inf\")\n",
    "patience_model3 = 3  # Early stopping patience\n",
    "stagnation_model3 = 0\n",
    "losses_model3 = []\n",
    "val_losses_model3 = []\n",
    "\n",
    "for epoch_model3 in range(epochs_model3):\n",
    "    predictions_model3 = model3.forward(model3_x_train)\n",
    "    loss_model3 = loss_function_model3.forward(predictions_model3, model3_y_train)\n",
    "    losses_model3.append(loss_model3)\n",
    "    \n",
    "    val_predictions_model3 = model3.forward(model3_x_val)\n",
    "    val_loss_model3 = loss_function_model3.forward(val_predictions_model3, model3_y_val)\n",
    "    val_losses_model3.append(val_loss_model3)\n",
    "    \n",
    "    print(f\"Epoch {epoch_model3}, Loss: {loss_model3:.4f}, Val Loss: {val_loss_model3:.4f}\")\n",
    "    \n",
    "    grad_output_model3 = loss_function_model3.backward()\n",
    "    model3.backward(grad_output_model3, learning_rate_model3)\n",
    "\n",
    "    # Early stopping\n",
    "    if loss_model3 < best_loss_model3:\n",
    "        best_loss_model3 = loss_model3\n",
    "        stagnation_model3 = 0\n",
    "        print(f\"Saving model at epoch {epoch_model3} with loss {loss_model3:.4f}\")\n",
    "        model3.save_weights(\"best_model3\")\n",
    "    else:\n",
    "        stagnation_model3 += 1\n",
    "        if stagnation_model3 >= patience_model3:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load best model and evaluate on test set\n",
    "test_predictions_model3 = model3.forward(model3_x_test)\n",
    "test_loss_model3 = loss_function_model3.forward(test_predictions_model3, y_test)\n",
    "print(f\"Test Loss (MSE): {test_loss_model3:.4f}\")\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses_model3, label='Training Loss')\n",
    "plt.plot(val_losses_model3, label='Validation Loss', linestyle='dashed')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
