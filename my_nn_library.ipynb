{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Neural Network Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"Base class for all layers in the neural network.\"\"\"\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Computes the forward pass.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"Computes the backward pass.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class LinearLayer(Layer):\n",
    "    \"\"\"Fully connected linear layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.weights = np.random.randn(output_dim, input_dim) * np.sqrt(2 / input_dim)\n",
    "        self.bias = np.zeros((output_dim, 1))\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Performs forward pass: output = input * weights^T + bias\"\"\"\n",
    "        self.input_data = input_data  # Store for use in backward pass\n",
    "        return np.dot(self.weights, input_data.T).T + self.bias.T\n",
    "\n",
    "    def backward(self, grad_output, learning_rate=0.01):\n",
    "        \"\"\"Computes gradients and updates parameters.\"\"\"\n",
    "        grad_input = np.dot(grad_output, self.weights)  # dL/dX\n",
    "        grad_weights = np.dot(grad_output.T, self.input_data)  # dL/dW\n",
    "        grad_bias = np.sum(grad_output, axis=0, keepdims=True).T  # dL/db\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weights -= learning_rate * grad_weights\n",
    "        self.bias -= learning_rate * grad_bias\n",
    "        \n",
    "        return grad_input\n",
    "    \n",
    "class Sigmoid(Layer):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    def forward(self, input_data):\n",
    "        self.output = 1 / (1 + np.exp(-input_data))\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * self.output * (1 - self.output)\n",
    "    \n",
    "class Tanh(Layer):\n",
    "    \"\"\"Tanh activation function.\"\"\"\n",
    "    def forward(self, input_data):\n",
    "        self.output = np.tanh(input_data)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * (1 - self.output ** 2)\n",
    "    \n",
    "class ReLU(Layer):\n",
    "    \"\"\"ReLU activation function.\"\"\"\n",
    "    def forward(self, input_data):\n",
    "        self.input_data = input_data\n",
    "        return np.maximum(0, input_data)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * (self.input_data > 0)\n",
    "\n",
    "class BinaryCrossEntropyLoss(Layer):\n",
    "    \"\"\"Binary Cross-Entropy Loss Function.\"\"\"\n",
    "    def forward(self, predictions, targets):\n",
    "        self.predictions = np.clip(predictions, 1e-12, 1 - 1e-12)  # Avoid log(0)\n",
    "        self.targets = targets\n",
    "        return -np.mean(targets * np.log(self.predictions) + (1 - targets) * np.log(1 - self.predictions))\n",
    "    \n",
    "    def backward(self):\n",
    "        return (self.predictions - self.targets) / (self.targets.shape[0])\n",
    "\n",
    "class MeanSquaredErrorLoss(Layer):\n",
    "    \"\"\"Mean Squared Error Loss Function.\"\"\"\n",
    "    def forward(self, predictions, targets):\n",
    "        self.predictions = predictions\n",
    "        self.targets = targets\n",
    "        # Ensure both predictions and targets have the same shape\n",
    "        assert predictions.shape == targets.shape, \"Shapes of predictions and targets do not match\"\n",
    "        return np.mean((predictions - targets) ** 2)  # Element-wise squared error\n",
    "\n",
    "    def backward(self):\n",
    "        # Compute the gradient of the loss with respect to predictions\n",
    "        return 2 * (self.predictions - self.targets) / self.predictions.size\n",
    "\n",
    "class Sequential(Layer):\n",
    "    \"\"\"Sequential model to stack multiple layers.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def add(self, layer):\n",
    "        \"\"\"Adds a new layer to the model.\"\"\"\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Performs a forward pass through all layers.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            input_data = layer.forward(input_data)\n",
    "        return input_data\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate=0.01):\n",
    "        \"\"\"Performs a backward pass through all layers.\"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_output = layer.backward(grad_output, learning_rate) if isinstance(layer, LinearLayer) else layer.backward(grad_output)\n",
    "    \n",
    "    def save_weights(self, filename):\n",
    "        \"\"\"Saves model weights to a file.\"\"\"\n",
    "        weights = [layer.weights for layer in self.layers if isinstance(layer, LinearLayer)]\n",
    "        biases = [layer.bias for layer in self.layers if isinstance(layer, LinearLayer)]\n",
    "        np.savez(filename, *weights, *biases)\n",
    "    \n",
    "    def load_weights(self, filename):\n",
    "        \"\"\"Loads model weights from a file while ensuring correct indexing.\"\"\"\n",
    "        data = np.load(filename)\n",
    "        keys = sorted(data.files)  # Ensure we get them in the right order\n",
    "        num_linear_layers = sum(1 for layer in self.layers if isinstance(layer, LinearLayer))\n",
    "\n",
    "        weight_keys = keys[:num_linear_layers]\n",
    "        bias_keys = keys[num_linear_layers:]\n",
    "\n",
    "        linear_layers = [layer for layer in self.layers if isinstance(layer, LinearLayer)]\n",
    "\n",
    "        for layer, w_key, b_key in zip(linear_layers, weight_keys, bias_keys):\n",
    "            layer.weights = data[w_key]\n",
    "            layer.bias = data[b_key]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Library Against XOR Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 7.2639\n",
      "Epoch 1000, Loss: 0.3836\n",
      "Epoch 2000, Loss: 0.0517\n",
      "Epoch 3000, Loss: 0.0263\n",
      "Epoch 4000, Loss: 0.0193\n",
      "Epoch 5000, Loss: 0.0158\n",
      "Epoch 6000, Loss: 0.0136\n",
      "Epoch 7000, Loss: 0.0121\n",
      "Epoch 8000, Loss: 0.0110\n",
      "Epoch 9000, Loss: 0.0101\n",
      "Final Predictions:\n",
      "[[4.67735818e-04]\n",
      " [9.82337554e-01]\n",
      " [9.81713307e-01]\n",
      " [9.25608756e-04]]\n"
     ]
    }
   ],
   "source": [
    "# XOR Problem Setup\n",
    "np.random.seed(0)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(LinearLayer(2, 2))  # Hidden layer with 2 neurons\n",
    "model.add(Tanh())\n",
    "model.add(LinearLayer(2, 1))  # Output layer\n",
    "model.add(Tanh())\n",
    "\n",
    "# Training\n",
    "epochs = 10000\n",
    "learning_rate = 0.1\n",
    "loss_function = BinaryCrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    predictions = model.forward(X)\n",
    "    loss = loss_function.forward(predictions, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    grad_output = loss_function.backward()\n",
    "    model.backward(grad_output, learning_rate)\n",
    "    \n",
    "    # Print loss every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Save trained model weights\n",
    "model.save_weights(\"XOR_solved\")\n",
    "\n",
    "# Test the model\n",
    "predictions = model.forward(X)\n",
    "print(\"Final Predictions:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR Problem Results\n",
    "\n",
    "Looks like using the tanh activations got the right predictions, couldn't get it to work well with sigmoid activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York Data Set\n",
    "https://drive.google.com/file/d/1xEtmFpP-WwZ-GC0B2njySPoLmdOCJOYM/view\n",
    "\n",
    "## Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before Preprocessing\n",
      "id                              id2425795\n",
      "vendor_id                               1\n",
      "pickup_datetime       2016-01-08 23:55:11\n",
      "dropoff_datetime      2016-01-09 00:04:32\n",
      "passenger_count                         1\n",
      "pickup_longitude               -73.955551\n",
      "pickup_latitude                 40.773346\n",
      "dropoff_longitude               -73.97364\n",
      "dropoff_latitude                  40.7635\n",
      "store_and_fwd_flag                      N\n",
      "Name: 879655, dtype: object\n",
      "\n",
      "After Preprocessing\n",
      "id                               id2425795\n",
      "vendor_id                        -1.072367\n",
      "pickup_datetime        2016-01-08 23:55:11\n",
      "dropoff_datetime       2016-01-09 00:04:32\n",
      "passenger_count                  -0.505442\n",
      "pickup_longitude                  0.244154\n",
      "pickup_latitude                   0.681191\n",
      "dropoff_longitude                 -0.00304\n",
      "dropoff_latitude                  0.326676\n",
      "store_and_fwd_flag                       N\n",
      "pickup_month                     -1.496875\n",
      "pickup_day                        -0.86245\n",
      "pickup_hour                       1.467487\n",
      "dropoff_month                    -1.496925\n",
      "dropoff_day                      -0.747615\n",
      "dropoff_hour                     -2.097212\n",
      "pickup_day_of_week                0.486031\n",
      "dropoff_day_of_week               0.994397\n",
      "Name: 879655, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "np.random.seed(0)\n",
    "dataset = np.load(\"nyc_taxi_data.npy\", allow_pickle=True).item()\n",
    "X_train, y_train = pd.DataFrame(dataset[\"X_train\"]), pd.Series(dataset[\"y_train\"])\n",
    "X_test, y_test = pd.DataFrame(dataset[\"X_test\"]), pd.Series(dataset[\"y_test\"])\n",
    "\n",
    "# print one row of the dataset\n",
    "print(\"\\nBefore Preprocessing\")\n",
    "print(X_train.iloc[0])\n",
    "\n",
    "# create function to split pickup and dropoff datetime into month, day, hour\n",
    "def split_datetime(df):\n",
    "    df[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"])\n",
    "    df[\"dropoff_datetime\"] = pd.to_datetime(df[\"dropoff_datetime\"])\n",
    "    df[\"pickup_month\"] = df[\"pickup_datetime\"].dt.month\n",
    "    df[\"pickup_day\"] = df[\"pickup_datetime\"].dt.day\n",
    "    df[\"pickup_hour\"] = df[\"pickup_datetime\"].dt.hour\n",
    "    df[\"dropoff_month\"] = df[\"dropoff_datetime\"].dt.month\n",
    "    df[\"dropoff_day\"] = df[\"dropoff_datetime\"].dt.day\n",
    "    df[\"dropoff_hour\"] = df[\"dropoff_datetime\"].dt.hour\n",
    "    return df\n",
    "\n",
    "# create function to add day of week, 0 = Monday, 6 = Sunday\n",
    "def add_day_of_week(df):\n",
    "    df[\"pickup_day_of_week\"] = df[\"pickup_datetime\"].dt.dayofweek\n",
    "    df[\"dropoff_day_of_week\"] = df[\"dropoff_datetime\"].dt.dayofweek\n",
    "    return df\n",
    "\n",
    "# create function to normalize all numerical features to same scale\n",
    "# should only be applied to numerical features\n",
    "#\n",
    "#   Feature Type\t                Recommended Scaling\n",
    "#   Longitude/Latitude\t            Min-Max (0 to 1)\n",
    "#   Time-based (hour, day, month)\tMin-Max (0 to 1) or Sin/Cos Encoding\n",
    "def normalize_numerical_features_0to1(df):\n",
    "    numerical_features = df.select_dtypes(include=[np.number]).columns\n",
    "    for feature in numerical_features:\n",
    "        if \"longitude\" in feature or \"latitude\" in feature:\n",
    "            df[feature] = (df[feature] - df[feature].min()) / (df[feature].max() - df[feature].min())\n",
    "        elif \"hour\" in feature or \"day\" in feature or \"month\" in feature:\n",
    "            df[feature] = (df[feature] - df[feature].min()) / (df[feature].max() - df[feature].min())\n",
    "        elif \"day_of_week\" in feature:\n",
    "            df[feature] = df[feature] / 6  # Scale day of week between 0 and 1\n",
    "    return df\n",
    "\n",
    "# create function to normalize all numerical features to same scale\n",
    "# should only be applied to numerical features\n",
    "# zscore normalization\n",
    "def normalize_numerical_features_zscore(df):\n",
    "    numerical_features = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numerical_features] = (df[numerical_features] - df[numerical_features].mean()) / df[numerical_features].std()\n",
    "    return df\n",
    "\n",
    "# function to perform split date time and add day of week given dataframe and normalize features\n",
    "def preprocess_data(df):\n",
    "    df = split_datetime(df)\n",
    "    df = add_day_of_week(df)\n",
    "    df = normalize_numerical_features_zscore(df) # Change to which ever normalization if needed\n",
    "    return df\n",
    "\n",
    "# preprocess the training and test data\n",
    "X_train = preprocess_data(X_train)\n",
    "X_test = preprocess_data(X_test)\n",
    "\n",
    "# Log-transform the target variable to reduce skewness\n",
    "y_train = np.log1p(y_train.to_numpy().reshape(-1, 1))\n",
    "y_test = np.log1p(y_test.to_numpy().reshape(-1, 1))\n",
    "\n",
    "# print one row of the dataset with after text title\n",
    "print(\"\\nAfter Preprocessing\")\n",
    "print(X_train.iloc[0])\n",
    "\n",
    "# create function to only keep the features we want given df and features\n",
    "def select_features(df, features):\n",
    "    return df[features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Pick Up and Drop Off Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 51.9858, Val Loss: 52.3596\n",
      "Saving model at epoch 0 with loss 51.9858\n",
      "Epoch 1, Loss: 30.9707, Val Loss: 29.4318\n",
      "Saving model at epoch 1 with loss 30.9707\n",
      "Epoch 2, Loss: 9.5798, Val Loss: 9.5978\n",
      "Saving model at epoch 2 with loss 9.5798\n",
      "Epoch 3, Loss: 1.2571, Val Loss: 1.1446\n",
      "Saving model at epoch 3 with loss 1.2571\n",
      "Epoch 4, Loss: 1.3884, Val Loss: 1.3676\n",
      "Epoch 5, Loss: 1.2576, Val Loss: 1.1783\n",
      "Epoch 6, Loss: 1.4444, Val Loss: 1.4360\n",
      "Early stopping triggered.\n",
      "Test Loss (MSE): 1.4205\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the features we want to keep\n",
    "# Keep only pickup and drop off positions\n",
    "features_model1 = [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]\n",
    "\n",
    "# Split 10% of the training data as validation set\n",
    "model1_x = select_features(X_train, features_model1)\n",
    "model1_y = y_train\n",
    "model1_x_train, model1_x_val, model1_y_train, model1_y_val = train_test_split(model1_x, model1_y, test_size=0.1, random_state=0)\n",
    "model1_x_test = select_features(X_test, features_model1)\n",
    "\n",
    "# Define Model\n",
    "model1 = Sequential()\n",
    "model1.add(LinearLayer(model1_x_train.shape[1], 4))\n",
    "model1.add(ReLU())\n",
    "model1.add(LinearLayer(4, 4))\n",
    "model1.add(ReLU())\n",
    "model1.add(LinearLayer(4, 4))\n",
    "model1.add(ReLU())\n",
    "model1.add(LinearLayer(4, 1))\n",
    "\n",
    "# Training Setup\n",
    "epochs_model1 = 1000\n",
    "learning_rate_model1 = 0.01\n",
    "loss_function_model1 = MeanSquaredErrorLoss()\n",
    "\n",
    "best_loss_model1 = float(\"inf\")\n",
    "patience_model1 = 3  # Early stopping patience\n",
    "stagnation_model1 = 0\n",
    "losses_model1 = []\n",
    "val_losses_model1 = []\n",
    "\n",
    "for epoch_model1 in range(epochs_model1):\n",
    "    predictions_model1 = model1.forward(model1_x_train)\n",
    "    loss_model1 = loss_function_model1.forward(predictions_model1, model1_y_train)\n",
    "    losses_model1.append(loss_model1)\n",
    "    \n",
    "    val_predictions_model1 = model1.forward(model1_x_val)\n",
    "    val_loss_model1 = loss_function_model1.forward(val_predictions_model1, model1_y_val)\n",
    "    val_losses_model1.append(val_loss_model1)\n",
    "    \n",
    "    print(f\"Epoch {epoch_model1}, Loss: {loss_model1:.4f}, Val Loss: {val_loss_model1:.4f}\")\n",
    "    \n",
    "    grad_output_model1 = loss_function_model1.backward()\n",
    "    model1.backward(grad_output_model1, learning_rate_model1)\n",
    "\n",
    "    # Early stopping\n",
    "    if loss_model1 < best_loss_model1:\n",
    "        best_loss_model1 = loss_model1\n",
    "        stagnation_model1 = 0\n",
    "        print(f\"Saving model at epoch {epoch_model1} with loss {loss_model1:.4f}\")\n",
    "    else:\n",
    "        stagnation_model1 += 1\n",
    "        if stagnation_model1 >= patience_model1:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load best model and evaluate on test set\n",
    "test_predictions_model1 = model1.forward(model1_x_test)\n",
    "test_loss_model1 = loss_function_model1.forward(test_predictions_model1, y_test)\n",
    "print(f\"Test Loss (MSE): {test_loss_model1:.4f}\")\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses_model1, label='Training Loss')\n",
    "plt.plot(val_losses_model1, label='Validation Loss', linestyle='dashed')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickup Time, Position and Dropoff Time, Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 43.4378, Val Loss: 43.6324\n",
      "Saving model at epoch 0 with loss 43.4378\n",
      "Epoch 1, Loss: 147.1695, Val Loss: 581.4821\n",
      "Epoch 2, Loss: 126.9268, Val Loss: 113.5305\n",
      "Epoch 3, Loss: 17.8386, Val Loss: 17.8643\n",
      "Saving model at epoch 3 with loss 17.8386\n",
      "Epoch 4, Loss: 11.7793, Val Loss: 11.9457\n",
      "Saving model at epoch 4 with loss 11.7793\n",
      "Epoch 5, Loss: 8.5453, Val Loss: 8.5436\n",
      "Saving model at epoch 5 with loss 8.5453\n",
      "Epoch 6, Loss: 4.9214, Val Loss: 4.9442\n",
      "Saving model at epoch 6 with loss 4.9214\n",
      "Epoch 7, Loss: 3.3781, Val Loss: 3.3971\n",
      "Saving model at epoch 7 with loss 3.3781\n",
      "Epoch 8, Loss: 2.5395, Val Loss: 2.5495\n",
      "Saving model at epoch 8 with loss 2.5395\n",
      "Epoch 9, Loss: 2.0826, Val Loss: 2.0892\n",
      "Saving model at epoch 9 with loss 2.0826\n",
      "Epoch 10, Loss: 1.8184, Val Loss: 1.8223\n",
      "Saving model at epoch 10 with loss 1.8184\n",
      "Epoch 11, Loss: 1.6445, Val Loss: 1.6467\n",
      "Saving model at epoch 11 with loss 1.6445\n",
      "Epoch 12, Loss: 1.5154, Val Loss: 1.5162\n",
      "Saving model at epoch 12 with loss 1.5154\n",
      "Epoch 13, Loss: 1.4113, Val Loss: 1.4111\n",
      "Saving model at epoch 13 with loss 1.4113\n",
      "Epoch 14, Loss: 1.3239, Val Loss: 1.3229\n",
      "Saving model at epoch 14 with loss 1.3239\n",
      "Epoch 15, Loss: 1.2489, Val Loss: 1.2471\n",
      "Saving model at epoch 15 with loss 1.2489\n",
      "Epoch 16, Loss: 1.1837, Val Loss: 1.1813\n",
      "Saving model at epoch 16 with loss 1.1837\n",
      "Epoch 17, Loss: 1.1268, Val Loss: 1.1239\n",
      "Saving model at epoch 17 with loss 1.1268\n",
      "Epoch 18, Loss: 1.0769, Val Loss: 1.0736\n",
      "Saving model at epoch 18 with loss 1.0769\n",
      "Epoch 19, Loss: 1.0330, Val Loss: 1.0293\n",
      "Saving model at epoch 19 with loss 1.0330\n",
      "Epoch 20, Loss: 0.9943, Val Loss: 0.9902\n",
      "Saving model at epoch 20 with loss 0.9943\n",
      "Epoch 21, Loss: 0.9600, Val Loss: 0.9557\n",
      "Saving model at epoch 21 with loss 0.9600\n",
      "Epoch 22, Loss: 0.9297, Val Loss: 0.9251\n",
      "Saving model at epoch 22 with loss 0.9297\n",
      "Epoch 23, Loss: 0.9027, Val Loss: 0.8979\n",
      "Saving model at epoch 23 with loss 0.9027\n",
      "Epoch 24, Loss: 0.8788, Val Loss: 0.8737\n",
      "Saving model at epoch 24 with loss 0.8788\n",
      "Epoch 25, Loss: 0.8574, Val Loss: 0.8522\n",
      "Saving model at epoch 25 with loss 0.8574\n",
      "Epoch 26, Loss: 0.8384, Val Loss: 0.8330\n",
      "Saving model at epoch 26 with loss 0.8384\n",
      "Epoch 27, Loss: 0.8213, Val Loss: 0.8157\n",
      "Saving model at epoch 27 with loss 0.8213\n",
      "Epoch 28, Loss: 0.8060, Val Loss: 0.8003\n",
      "Saving model at epoch 28 with loss 0.8060\n",
      "Epoch 29, Loss: 0.7923, Val Loss: 0.7865\n",
      "Saving model at epoch 29 with loss 0.7923\n",
      "Epoch 30, Loss: 0.7799, Val Loss: 0.7740\n",
      "Saving model at epoch 30 with loss 0.7799\n",
      "Epoch 31, Loss: 0.7688, Val Loss: 0.7628\n",
      "Saving model at epoch 31 with loss 0.7688\n",
      "Epoch 32, Loss: 0.7588, Val Loss: 0.7526\n",
      "Saving model at epoch 32 with loss 0.7588\n",
      "Epoch 33, Loss: 0.7497, Val Loss: 0.7435\n",
      "Saving model at epoch 33 with loss 0.7497\n",
      "Epoch 34, Loss: 0.7414, Val Loss: 0.7352\n",
      "Saving model at epoch 34 with loss 0.7414\n",
      "Epoch 35, Loss: 0.7340, Val Loss: 0.7276\n",
      "Saving model at epoch 35 with loss 0.7340\n",
      "Epoch 36, Loss: 0.7272, Val Loss: 0.7208\n",
      "Saving model at epoch 36 with loss 0.7272\n",
      "Epoch 37, Loss: 0.7210, Val Loss: 0.7145\n",
      "Saving model at epoch 37 with loss 0.7210\n",
      "Epoch 38, Loss: 0.7153, Val Loss: 0.7088\n",
      "Saving model at epoch 38 with loss 0.7153\n",
      "Epoch 39, Loss: 0.7101, Val Loss: 0.7036\n",
      "Saving model at epoch 39 with loss 0.7101\n",
      "Epoch 40, Loss: 0.7054, Val Loss: 0.6988\n",
      "Saving model at epoch 40 with loss 0.7054\n",
      "Epoch 41, Loss: 0.7010, Val Loss: 0.6944\n",
      "Saving model at epoch 41 with loss 0.7010\n",
      "Epoch 42, Loss: 0.6970, Val Loss: 0.6904\n",
      "Saving model at epoch 42 with loss 0.6970\n",
      "Epoch 43, Loss: 0.6933, Val Loss: 0.6866\n",
      "Saving model at epoch 43 with loss 0.6933\n",
      "Epoch 44, Loss: 0.6899, Val Loss: 0.6832\n",
      "Saving model at epoch 44 with loss 0.6899\n",
      "Epoch 45, Loss: 0.6868, Val Loss: 0.6801\n",
      "Saving model at epoch 45 with loss 0.6868\n",
      "Epoch 46, Loss: 0.6839, Val Loss: 0.6771\n",
      "Saving model at epoch 46 with loss 0.6839\n",
      "Epoch 47, Loss: 0.6812, Val Loss: 0.6744\n",
      "Saving model at epoch 47 with loss 0.6812\n",
      "Epoch 48, Loss: 0.6787, Val Loss: 0.6719\n",
      "Saving model at epoch 48 with loss 0.6787\n",
      "Epoch 49, Loss: 0.6764, Val Loss: 0.6696\n",
      "Saving model at epoch 49 with loss 0.6764\n",
      "Epoch 50, Loss: 0.6742, Val Loss: 0.6674\n",
      "Saving model at epoch 50 with loss 0.6742\n",
      "Epoch 51, Loss: 0.6722, Val Loss: 0.6654\n",
      "Saving model at epoch 51 with loss 0.6722\n",
      "Epoch 52, Loss: 0.6703, Val Loss: 0.6635\n",
      "Saving model at epoch 52 with loss 0.6703\n",
      "Epoch 53, Loss: 0.6686, Val Loss: 0.6617\n",
      "Saving model at epoch 53 with loss 0.6686\n",
      "Epoch 54, Loss: 0.6670, Val Loss: 0.6600\n",
      "Saving model at epoch 54 with loss 0.6670\n",
      "Epoch 55, Loss: 0.6654, Val Loss: 0.6585\n",
      "Saving model at epoch 55 with loss 0.6654\n",
      "Epoch 56, Loss: 0.6640, Val Loss: 0.6570\n",
      "Saving model at epoch 56 with loss 0.6640\n",
      "Epoch 57, Loss: 0.6626, Val Loss: 0.6557\n",
      "Saving model at epoch 57 with loss 0.6626\n",
      "Epoch 58, Loss: 0.6614, Val Loss: 0.6544\n",
      "Saving model at epoch 58 with loss 0.6614\n",
      "Epoch 59, Loss: 0.6602, Val Loss: 0.6532\n",
      "Saving model at epoch 59 with loss 0.6602\n",
      "Epoch 60, Loss: 0.6591, Val Loss: 0.6521\n",
      "Saving model at epoch 60 with loss 0.6591\n",
      "Epoch 61, Loss: 0.6580, Val Loss: 0.6510\n",
      "Saving model at epoch 61 with loss 0.6580\n",
      "Epoch 62, Loss: 0.6570, Val Loss: 0.6500\n",
      "Saving model at epoch 62 with loss 0.6570\n",
      "Epoch 63, Loss: 0.6560, Val Loss: 0.6490\n",
      "Saving model at epoch 63 with loss 0.6560\n",
      "Epoch 64, Loss: 0.6551, Val Loss: 0.6481\n",
      "Saving model at epoch 64 with loss 0.6551\n",
      "Epoch 65, Loss: 0.6543, Val Loss: 0.6473\n",
      "Saving model at epoch 65 with loss 0.6543\n",
      "Epoch 66, Loss: 0.6535, Val Loss: 0.6465\n",
      "Saving model at epoch 66 with loss 0.6535\n",
      "Epoch 67, Loss: 0.6527, Val Loss: 0.6457\n",
      "Saving model at epoch 67 with loss 0.6527\n",
      "Epoch 68, Loss: 0.6520, Val Loss: 0.6450\n",
      "Saving model at epoch 68 with loss 0.6520\n",
      "Epoch 69, Loss: 0.6513, Val Loss: 0.6443\n",
      "Saving model at epoch 69 with loss 0.6513\n",
      "Epoch 70, Loss: 0.6507, Val Loss: 0.6436\n",
      "Saving model at epoch 70 with loss 0.6507\n",
      "Epoch 71, Loss: 0.6500, Val Loss: 0.6430\n",
      "Saving model at epoch 71 with loss 0.6500\n",
      "Epoch 72, Loss: 0.6495, Val Loss: 0.6424\n",
      "Saving model at epoch 72 with loss 0.6495\n",
      "Epoch 73, Loss: 0.6489, Val Loss: 0.6418\n",
      "Saving model at epoch 73 with loss 0.6489\n",
      "Epoch 74, Loss: 0.6483, Val Loss: 0.6413\n",
      "Saving model at epoch 74 with loss 0.6483\n",
      "Epoch 75, Loss: 0.6478, Val Loss: 0.6408\n",
      "Saving model at epoch 75 with loss 0.6478\n",
      "Epoch 76, Loss: 0.6473, Val Loss: 0.6403\n",
      "Saving model at epoch 76 with loss 0.6473\n",
      "Epoch 77, Loss: 0.6469, Val Loss: 0.6398\n",
      "Saving model at epoch 77 with loss 0.6469\n",
      "Epoch 78, Loss: 0.6464, Val Loss: 0.6393\n",
      "Saving model at epoch 78 with loss 0.6464\n",
      "Epoch 79, Loss: 0.6460, Val Loss: 0.6389\n",
      "Saving model at epoch 79 with loss 0.6460\n",
      "Epoch 80, Loss: 0.6456, Val Loss: 0.6385\n",
      "Saving model at epoch 80 with loss 0.6456\n",
      "Epoch 81, Loss: 0.6452, Val Loss: 0.6381\n",
      "Saving model at epoch 81 with loss 0.6452\n",
      "Epoch 82, Loss: 0.6448, Val Loss: 0.6377\n",
      "Saving model at epoch 82 with loss 0.6448\n",
      "Epoch 83, Loss: 0.6444, Val Loss: 0.6373\n",
      "Saving model at epoch 83 with loss 0.6444\n",
      "Epoch 84, Loss: 0.6441, Val Loss: 0.6370\n",
      "Saving model at epoch 84 with loss 0.6441\n",
      "Epoch 85, Loss: 0.6437, Val Loss: 0.6366\n",
      "Saving model at epoch 85 with loss 0.6437\n",
      "Epoch 86, Loss: 0.6434, Val Loss: 0.6363\n",
      "Saving model at epoch 86 with loss 0.6434\n",
      "Epoch 87, Loss: 0.6431, Val Loss: 0.6360\n",
      "Saving model at epoch 87 with loss 0.6431\n",
      "Epoch 88, Loss: 0.6428, Val Loss: 0.6357\n",
      "Saving model at epoch 88 with loss 0.6428\n",
      "Epoch 89, Loss: 0.6425, Val Loss: 0.6354\n",
      "Saving model at epoch 89 with loss 0.6425\n",
      "Epoch 90, Loss: 0.6422, Val Loss: 0.6351\n",
      "Saving model at epoch 90 with loss 0.6422\n",
      "Epoch 91, Loss: 0.6420, Val Loss: 0.6348\n",
      "Saving model at epoch 91 with loss 0.6420\n",
      "Epoch 92, Loss: 0.6417, Val Loss: 0.6346\n",
      "Saving model at epoch 92 with loss 0.6417\n",
      "Epoch 93, Loss: 0.6414, Val Loss: 0.6343\n",
      "Saving model at epoch 93 with loss 0.6414\n",
      "Epoch 94, Loss: 0.6412, Val Loss: 0.6341\n",
      "Saving model at epoch 94 with loss 0.6412\n",
      "Epoch 95, Loss: 0.6410, Val Loss: 0.6338\n",
      "Saving model at epoch 95 with loss 0.6410\n",
      "Epoch 96, Loss: 0.6407, Val Loss: 0.6336\n",
      "Saving model at epoch 96 with loss 0.6407\n",
      "Epoch 97, Loss: 0.6405, Val Loss: 0.6334\n",
      "Saving model at epoch 97 with loss 0.6405\n",
      "Epoch 98, Loss: 0.6403, Val Loss: 0.6332\n",
      "Saving model at epoch 98 with loss 0.6403\n",
      "Epoch 99, Loss: 0.6401, Val Loss: 0.6330\n",
      "Saving model at epoch 99 with loss 0.6401\n",
      "Epoch 100, Loss: 0.6399, Val Loss: 0.6328\n",
      "Saving model at epoch 100 with loss 0.6399\n",
      "Epoch 101, Loss: 0.6397, Val Loss: 0.6326\n",
      "Saving model at epoch 101 with loss 0.6397\n",
      "Epoch 102, Loss: 0.6395, Val Loss: 0.6324\n",
      "Saving model at epoch 102 with loss 0.6395\n",
      "Epoch 103, Loss: 0.6393, Val Loss: 0.6322\n",
      "Saving model at epoch 103 with loss 0.6393\n",
      "Epoch 104, Loss: 0.6392, Val Loss: 0.6320\n",
      "Saving model at epoch 104 with loss 0.6392\n",
      "Epoch 105, Loss: 0.6390, Val Loss: 0.6318\n",
      "Saving model at epoch 105 with loss 0.6390\n",
      "Epoch 106, Loss: 0.6388, Val Loss: 0.6317\n",
      "Saving model at epoch 106 with loss 0.6388\n",
      "Epoch 107, Loss: 0.6387, Val Loss: 0.6315\n",
      "Saving model at epoch 107 with loss 0.6387\n",
      "Epoch 108, Loss: 0.6385, Val Loss: 0.6313\n",
      "Saving model at epoch 108 with loss 0.6385\n",
      "Epoch 109, Loss: 0.6383, Val Loss: 0.6312\n",
      "Saving model at epoch 109 with loss 0.6383\n",
      "Epoch 110, Loss: 0.6382, Val Loss: 0.6310\n",
      "Saving model at epoch 110 with loss 0.6382\n",
      "Epoch 111, Loss: 0.6380, Val Loss: 0.6309\n",
      "Saving model at epoch 111 with loss 0.6380\n",
      "Epoch 112, Loss: 0.6379, Val Loss: 0.6307\n",
      "Saving model at epoch 112 with loss 0.6379\n",
      "Epoch 113, Loss: 0.6378, Val Loss: 0.6306\n",
      "Saving model at epoch 113 with loss 0.6378\n",
      "Epoch 114, Loss: 0.6376, Val Loss: 0.6305\n",
      "Saving model at epoch 114 with loss 0.6376\n",
      "Epoch 115, Loss: 0.6375, Val Loss: 0.6303\n",
      "Saving model at epoch 115 with loss 0.6375\n",
      "Epoch 116, Loss: 0.6374, Val Loss: 0.6302\n",
      "Saving model at epoch 116 with loss 0.6374\n",
      "Epoch 117, Loss: 0.6372, Val Loss: 0.6301\n",
      "Saving model at epoch 117 with loss 0.6372\n",
      "Epoch 118, Loss: 0.6371, Val Loss: 0.6300\n",
      "Saving model at epoch 118 with loss 0.6371\n",
      "Epoch 119, Loss: 0.6370, Val Loss: 0.6298\n",
      "Saving model at epoch 119 with loss 0.6370\n",
      "Epoch 120, Loss: 0.6369, Val Loss: 0.6297\n",
      "Saving model at epoch 120 with loss 0.6369\n",
      "Epoch 121, Loss: 0.6368, Val Loss: 0.6296\n",
      "Saving model at epoch 121 with loss 0.6368\n",
      "Epoch 122, Loss: 0.6367, Val Loss: 0.6295\n",
      "Saving model at epoch 122 with loss 0.6367\n",
      "Epoch 123, Loss: 0.6366, Val Loss: 0.6294\n",
      "Saving model at epoch 123 with loss 0.6366\n",
      "Epoch 124, Loss: 0.6365, Val Loss: 0.6293\n",
      "Saving model at epoch 124 with loss 0.6365\n",
      "Epoch 125, Loss: 0.6363, Val Loss: 0.6292\n",
      "Saving model at epoch 125 with loss 0.6363\n",
      "Epoch 126, Loss: 0.6362, Val Loss: 0.6291\n",
      "Saving model at epoch 126 with loss 0.6362\n",
      "Epoch 127, Loss: 0.6361, Val Loss: 0.6290\n",
      "Saving model at epoch 127 with loss 0.6361\n",
      "Epoch 128, Loss: 0.6361, Val Loss: 0.6289\n",
      "Saving model at epoch 128 with loss 0.6361\n",
      "Epoch 129, Loss: 0.6360, Val Loss: 0.6288\n",
      "Saving model at epoch 129 with loss 0.6360\n",
      "Epoch 130, Loss: 0.6359, Val Loss: 0.6287\n",
      "Saving model at epoch 130 with loss 0.6359\n",
      "Epoch 131, Loss: 0.6358, Val Loss: 0.6286\n",
      "Saving model at epoch 131 with loss 0.6358\n",
      "Epoch 132, Loss: 0.6357, Val Loss: 0.6285\n",
      "Saving model at epoch 132 with loss 0.6357\n",
      "Epoch 133, Loss: 0.6356, Val Loss: 0.6284\n",
      "Saving model at epoch 133 with loss 0.6356\n",
      "Epoch 134, Loss: 0.6355, Val Loss: 0.6283\n",
      "Saving model at epoch 134 with loss 0.6355\n",
      "Epoch 135, Loss: 0.6354, Val Loss: 0.6282\n",
      "Saving model at epoch 135 with loss 0.6354\n",
      "Epoch 136, Loss: 0.6353, Val Loss: 0.6282\n",
      "Saving model at epoch 136 with loss 0.6353\n",
      "Epoch 137, Loss: 0.6353, Val Loss: 0.6281\n",
      "Saving model at epoch 137 with loss 0.6353\n",
      "Epoch 138, Loss: 0.6352, Val Loss: 0.6280\n",
      "Saving model at epoch 138 with loss 0.6352\n",
      "Epoch 139, Loss: 0.6351, Val Loss: 0.6279\n",
      "Saving model at epoch 139 with loss 0.6351\n",
      "Epoch 140, Loss: 0.6350, Val Loss: 0.6278\n",
      "Saving model at epoch 140 with loss 0.6350\n",
      "Epoch 141, Loss: 0.6349, Val Loss: 0.6278\n",
      "Saving model at epoch 141 with loss 0.6349\n",
      "Epoch 142, Loss: 0.6349, Val Loss: 0.6277\n",
      "Saving model at epoch 142 with loss 0.6349\n",
      "Epoch 143, Loss: 0.6348, Val Loss: 0.6276\n",
      "Saving model at epoch 143 with loss 0.6348\n",
      "Epoch 144, Loss: 0.6347, Val Loss: 0.6275\n",
      "Saving model at epoch 144 with loss 0.6347\n",
      "Epoch 145, Loss: 0.6347, Val Loss: 0.6275\n",
      "Saving model at epoch 145 with loss 0.6347\n",
      "Epoch 146, Loss: 0.6346, Val Loss: 0.6274\n",
      "Saving model at epoch 146 with loss 0.6346\n",
      "Epoch 147, Loss: 0.6345, Val Loss: 0.6273\n",
      "Saving model at epoch 147 with loss 0.6345\n",
      "Epoch 148, Loss: 0.6345, Val Loss: 0.6273\n",
      "Saving model at epoch 148 with loss 0.6345\n",
      "Epoch 149, Loss: 0.6344, Val Loss: 0.6272\n",
      "Saving model at epoch 149 with loss 0.6344\n",
      "Epoch 150, Loss: 0.6343, Val Loss: 0.6271\n",
      "Saving model at epoch 150 with loss 0.6343\n",
      "Epoch 151, Loss: 0.6343, Val Loss: 0.6271\n",
      "Saving model at epoch 151 with loss 0.6343\n",
      "Epoch 152, Loss: 0.6342, Val Loss: 0.6270\n",
      "Saving model at epoch 152 with loss 0.6342\n",
      "Epoch 153, Loss: 0.6341, Val Loss: 0.6269\n",
      "Saving model at epoch 153 with loss 0.6341\n",
      "Epoch 154, Loss: 0.6341, Val Loss: 0.6269\n",
      "Saving model at epoch 154 with loss 0.6341\n",
      "Epoch 155, Loss: 0.6340, Val Loss: 0.6268\n",
      "Saving model at epoch 155 with loss 0.6340\n",
      "Epoch 156, Loss: 0.6340, Val Loss: 0.6268\n",
      "Saving model at epoch 156 with loss 0.6340\n",
      "Epoch 157, Loss: 0.6339, Val Loss: 0.6267\n",
      "Saving model at epoch 157 with loss 0.6339\n",
      "Epoch 158, Loss: 0.6338, Val Loss: 0.6266\n",
      "Saving model at epoch 158 with loss 0.6338\n",
      "Epoch 159, Loss: 0.6338, Val Loss: 0.6266\n",
      "Saving model at epoch 159 with loss 0.6338\n",
      "Epoch 160, Loss: 0.6337, Val Loss: 0.6265\n",
      "Saving model at epoch 160 with loss 0.6337\n",
      "Epoch 161, Loss: 0.6337, Val Loss: 0.6265\n",
      "Saving model at epoch 161 with loss 0.6337\n",
      "Epoch 162, Loss: 0.6336, Val Loss: 0.6264\n",
      "Saving model at epoch 162 with loss 0.6336\n",
      "Epoch 163, Loss: 0.6336, Val Loss: 0.6264\n",
      "Saving model at epoch 163 with loss 0.6336\n",
      "Epoch 164, Loss: 0.6335, Val Loss: 0.6263\n",
      "Saving model at epoch 164 with loss 0.6335\n",
      "Epoch 165, Loss: 0.6335, Val Loss: 0.6263\n",
      "Saving model at epoch 165 with loss 0.6335\n",
      "Epoch 166, Loss: 0.6334, Val Loss: 0.6262\n",
      "Saving model at epoch 166 with loss 0.6334\n",
      "Epoch 167, Loss: 0.6333, Val Loss: 0.6261\n",
      "Saving model at epoch 167 with loss 0.6333\n",
      "Epoch 168, Loss: 0.6333, Val Loss: 0.6261\n",
      "Saving model at epoch 168 with loss 0.6333\n",
      "Epoch 169, Loss: 0.6332, Val Loss: 0.6260\n",
      "Saving model at epoch 169 with loss 0.6332\n",
      "Epoch 170, Loss: 0.6332, Val Loss: 0.6260\n",
      "Saving model at epoch 170 with loss 0.6332\n",
      "Epoch 171, Loss: 0.6331, Val Loss: 0.6259\n",
      "Saving model at epoch 171 with loss 0.6331\n",
      "Epoch 172, Loss: 0.6331, Val Loss: 0.6259\n",
      "Saving model at epoch 172 with loss 0.6331\n",
      "Epoch 173, Loss: 0.6331, Val Loss: 0.6258\n",
      "Saving model at epoch 173 with loss 0.6331\n",
      "Epoch 174, Loss: 0.6330, Val Loss: 0.6258\n",
      "Saving model at epoch 174 with loss 0.6330\n",
      "Epoch 175, Loss: 0.6330, Val Loss: 0.6258\n",
      "Saving model at epoch 175 with loss 0.6330\n",
      "Epoch 176, Loss: 0.6329, Val Loss: 0.6257\n",
      "Saving model at epoch 176 with loss 0.6329\n",
      "Epoch 177, Loss: 0.6329, Val Loss: 0.6257\n",
      "Saving model at epoch 177 with loss 0.6329\n",
      "Epoch 178, Loss: 0.6328, Val Loss: 0.6256\n",
      "Saving model at epoch 178 with loss 0.6328\n",
      "Epoch 179, Loss: 0.6328, Val Loss: 0.6256\n",
      "Saving model at epoch 179 with loss 0.6328\n",
      "Epoch 180, Loss: 0.6327, Val Loss: 0.6255\n",
      "Saving model at epoch 180 with loss 0.6327\n",
      "Epoch 181, Loss: 0.6327, Val Loss: 0.6255\n",
      "Saving model at epoch 181 with loss 0.6327\n",
      "Epoch 182, Loss: 0.6326, Val Loss: 0.6254\n",
      "Saving model at epoch 182 with loss 0.6326\n",
      "Epoch 183, Loss: 0.6326, Val Loss: 0.6254\n",
      "Saving model at epoch 183 with loss 0.6326\n",
      "Epoch 184, Loss: 0.6326, Val Loss: 0.6253\n",
      "Saving model at epoch 184 with loss 0.6326\n",
      "Epoch 185, Loss: 0.6325, Val Loss: 0.6253\n",
      "Saving model at epoch 185 with loss 0.6325\n",
      "Epoch 186, Loss: 0.6325, Val Loss: 0.6253\n",
      "Saving model at epoch 186 with loss 0.6325\n",
      "Epoch 187, Loss: 0.6324, Val Loss: 0.6252\n",
      "Saving model at epoch 187 with loss 0.6324\n",
      "Epoch 188, Loss: 0.6324, Val Loss: 0.6252\n",
      "Saving model at epoch 188 with loss 0.6324\n",
      "Epoch 189, Loss: 0.6324, Val Loss: 0.6251\n",
      "Saving model at epoch 189 with loss 0.6324\n",
      "Epoch 190, Loss: 0.6323, Val Loss: 0.6251\n",
      "Saving model at epoch 190 with loss 0.6323\n",
      "Epoch 191, Loss: 0.6323, Val Loss: 0.6251\n",
      "Saving model at epoch 191 with loss 0.6323\n",
      "Epoch 192, Loss: 0.6322, Val Loss: 0.6250\n",
      "Saving model at epoch 192 with loss 0.6322\n",
      "Epoch 193, Loss: 0.6322, Val Loss: 0.6250\n",
      "Saving model at epoch 193 with loss 0.6322\n",
      "Epoch 194, Loss: 0.6322, Val Loss: 0.6249\n",
      "Saving model at epoch 194 with loss 0.6322\n",
      "Epoch 195, Loss: 0.6321, Val Loss: 0.6249\n",
      "Saving model at epoch 195 with loss 0.6321\n",
      "Epoch 196, Loss: 0.6321, Val Loss: 0.6249\n",
      "Saving model at epoch 196 with loss 0.6321\n",
      "Epoch 197, Loss: 0.6321, Val Loss: 0.6248\n",
      "Saving model at epoch 197 with loss 0.6321\n",
      "Epoch 198, Loss: 0.6320, Val Loss: 0.6248\n",
      "Saving model at epoch 198 with loss 0.6320\n",
      "Epoch 199, Loss: 0.6320, Val Loss: 0.6247\n",
      "Saving model at epoch 199 with loss 0.6320\n",
      "Epoch 200, Loss: 0.6319, Val Loss: 0.6247\n",
      "Saving model at epoch 200 with loss 0.6319\n",
      "Epoch 201, Loss: 0.6319, Val Loss: 0.6247\n",
      "Saving model at epoch 201 with loss 0.6319\n",
      "Epoch 202, Loss: 0.6319, Val Loss: 0.6246\n",
      "Saving model at epoch 202 with loss 0.6319\n",
      "Epoch 203, Loss: 0.6318, Val Loss: 0.6246\n",
      "Saving model at epoch 203 with loss 0.6318\n",
      "Epoch 204, Loss: 0.6318, Val Loss: 0.6246\n",
      "Saving model at epoch 204 with loss 0.6318\n",
      "Epoch 205, Loss: 0.6318, Val Loss: 0.6245\n",
      "Saving model at epoch 205 with loss 0.6318\n",
      "Epoch 206, Loss: 0.6317, Val Loss: 0.6245\n",
      "Saving model at epoch 206 with loss 0.6317\n",
      "Epoch 207, Loss: 0.6317, Val Loss: 0.6245\n",
      "Saving model at epoch 207 with loss 0.6317\n",
      "Epoch 208, Loss: 0.6317, Val Loss: 0.6244\n",
      "Saving model at epoch 208 with loss 0.6317\n",
      "Epoch 209, Loss: 0.6316, Val Loss: 0.6244\n",
      "Saving model at epoch 209 with loss 0.6316\n",
      "Epoch 210, Loss: 0.6316, Val Loss: 0.6244\n",
      "Saving model at epoch 210 with loss 0.6316\n",
      "Epoch 211, Loss: 0.6316, Val Loss: 0.6243\n",
      "Saving model at epoch 211 with loss 0.6316\n",
      "Epoch 212, Loss: 0.6315, Val Loss: 0.6243\n",
      "Saving model at epoch 212 with loss 0.6315\n",
      "Epoch 213, Loss: 0.6315, Val Loss: 0.6243\n",
      "Saving model at epoch 213 with loss 0.6315\n",
      "Epoch 214, Loss: 0.6315, Val Loss: 0.6242\n",
      "Saving model at epoch 214 with loss 0.6315\n",
      "Epoch 215, Loss: 0.6314, Val Loss: 0.6242\n",
      "Saving model at epoch 215 with loss 0.6314\n",
      "Epoch 216, Loss: 0.6314, Val Loss: 0.6242\n",
      "Saving model at epoch 216 with loss 0.6314\n",
      "Epoch 217, Loss: 0.6314, Val Loss: 0.6241\n",
      "Saving model at epoch 217 with loss 0.6314\n",
      "Epoch 218, Loss: 0.6314, Val Loss: 0.6241\n",
      "Saving model at epoch 218 with loss 0.6314\n",
      "Epoch 219, Loss: 0.6313, Val Loss: 0.6241\n",
      "Saving model at epoch 219 with loss 0.6313\n",
      "Epoch 220, Loss: 0.6313, Val Loss: 0.6240\n",
      "Saving model at epoch 220 with loss 0.6313\n",
      "Epoch 221, Loss: 0.6313, Val Loss: 0.6240\n",
      "Saving model at epoch 221 with loss 0.6313\n",
      "Epoch 222, Loss: 0.6312, Val Loss: 0.6240\n",
      "Saving model at epoch 222 with loss 0.6312\n",
      "Epoch 223, Loss: 0.6312, Val Loss: 0.6240\n",
      "Saving model at epoch 223 with loss 0.6312\n",
      "Epoch 224, Loss: 0.6312, Val Loss: 0.6239\n",
      "Saving model at epoch 224 with loss 0.6312\n",
      "Epoch 225, Loss: 0.6312, Val Loss: 0.6239\n",
      "Saving model at epoch 225 with loss 0.6312\n",
      "Epoch 226, Loss: 0.6311, Val Loss: 0.6239\n",
      "Saving model at epoch 226 with loss 0.6311\n",
      "Epoch 227, Loss: 0.6311, Val Loss: 0.6238\n",
      "Saving model at epoch 227 with loss 0.6311\n",
      "Epoch 228, Loss: 0.6311, Val Loss: 0.6238\n",
      "Saving model at epoch 228 with loss 0.6311\n",
      "Epoch 229, Loss: 0.6310, Val Loss: 0.6238\n",
      "Saving model at epoch 229 with loss 0.6310\n",
      "Epoch 230, Loss: 0.6310, Val Loss: 0.6238\n",
      "Saving model at epoch 230 with loss 0.6310\n",
      "Epoch 231, Loss: 0.6310, Val Loss: 0.6237\n",
      "Saving model at epoch 231 with loss 0.6310\n",
      "Epoch 232, Loss: 0.6310, Val Loss: 0.6237\n",
      "Saving model at epoch 232 with loss 0.6310\n",
      "Epoch 233, Loss: 0.6309, Val Loss: 0.6237\n",
      "Saving model at epoch 233 with loss 0.6309\n",
      "Epoch 234, Loss: 0.6309, Val Loss: 0.6236\n",
      "Saving model at epoch 234 with loss 0.6309\n",
      "Epoch 235, Loss: 0.6309, Val Loss: 0.6236\n",
      "Saving model at epoch 235 with loss 0.6309\n",
      "Epoch 236, Loss: 0.6309, Val Loss: 0.6236\n",
      "Saving model at epoch 236 with loss 0.6309\n",
      "Epoch 237, Loss: 0.6308, Val Loss: 0.6236\n",
      "Saving model at epoch 237 with loss 0.6308\n",
      "Epoch 238, Loss: 0.6308, Val Loss: 0.6235\n",
      "Saving model at epoch 238 with loss 0.6308\n",
      "Epoch 239, Loss: 0.6308, Val Loss: 0.6235\n",
      "Saving model at epoch 239 with loss 0.6308\n",
      "Epoch 240, Loss: 0.6308, Val Loss: 0.6235\n",
      "Saving model at epoch 240 with loss 0.6308\n",
      "Epoch 241, Loss: 0.6307, Val Loss: 0.6235\n",
      "Saving model at epoch 241 with loss 0.6307\n",
      "Epoch 242, Loss: 0.6307, Val Loss: 0.6234\n",
      "Saving model at epoch 242 with loss 0.6307\n",
      "Epoch 243, Loss: 0.6307, Val Loss: 0.6234\n",
      "Saving model at epoch 243 with loss 0.6307\n",
      "Epoch 244, Loss: 0.6307, Val Loss: 0.6234\n",
      "Saving model at epoch 244 with loss 0.6307\n",
      "Epoch 245, Loss: 0.6307, Val Loss: 0.6234\n",
      "Saving model at epoch 245 with loss 0.6307\n",
      "Epoch 246, Loss: 0.6306, Val Loss: 0.6233\n",
      "Saving model at epoch 246 with loss 0.6306\n",
      "Epoch 247, Loss: 0.6306, Val Loss: 0.6233\n",
      "Saving model at epoch 247 with loss 0.6306\n",
      "Epoch 248, Loss: 0.6306, Val Loss: 0.6233\n",
      "Saving model at epoch 248 with loss 0.6306\n",
      "Epoch 249, Loss: 0.6306, Val Loss: 0.6233\n",
      "Saving model at epoch 249 with loss 0.6306\n",
      "Epoch 250, Loss: 0.6305, Val Loss: 0.6233\n",
      "Saving model at epoch 250 with loss 0.6305\n",
      "Epoch 251, Loss: 0.6305, Val Loss: 0.6232\n",
      "Saving model at epoch 251 with loss 0.6305\n",
      "Epoch 252, Loss: 0.6305, Val Loss: 0.6232\n",
      "Saving model at epoch 252 with loss 0.6305\n",
      "Epoch 253, Loss: 0.6305, Val Loss: 0.6232\n",
      "Saving model at epoch 253 with loss 0.6305\n",
      "Epoch 254, Loss: 0.6305, Val Loss: 0.6232\n",
      "Saving model at epoch 254 with loss 0.6305\n",
      "Epoch 255, Loss: 0.6304, Val Loss: 0.6231\n",
      "Saving model at epoch 255 with loss 0.6304\n",
      "Epoch 256, Loss: 0.6304, Val Loss: 0.6231\n",
      "Saving model at epoch 256 with loss 0.6304\n",
      "Epoch 257, Loss: 0.6304, Val Loss: 0.6231\n",
      "Saving model at epoch 257 with loss 0.6304\n",
      "Epoch 258, Loss: 0.6304, Val Loss: 0.6231\n",
      "Saving model at epoch 258 with loss 0.6304\n",
      "Epoch 259, Loss: 0.6304, Val Loss: 0.6231\n",
      "Saving model at epoch 259 with loss 0.6304\n",
      "Epoch 260, Loss: 0.6303, Val Loss: 0.6230\n",
      "Saving model at epoch 260 with loss 0.6303\n",
      "Epoch 261, Loss: 0.6303, Val Loss: 0.6230\n",
      "Saving model at epoch 261 with loss 0.6303\n",
      "Epoch 262, Loss: 0.6303, Val Loss: 0.6230\n",
      "Saving model at epoch 262 with loss 0.6303\n",
      "Epoch 263, Loss: 0.6303, Val Loss: 0.6230\n",
      "Saving model at epoch 263 with loss 0.6303\n",
      "Epoch 264, Loss: 0.6303, Val Loss: 0.6230\n",
      "Saving model at epoch 264 with loss 0.6303\n",
      "Epoch 265, Loss: 0.6303, Val Loss: 0.6229\n",
      "Saving model at epoch 265 with loss 0.6303\n",
      "Epoch 266, Loss: 0.6302, Val Loss: 0.6229\n",
      "Saving model at epoch 266 with loss 0.6302\n",
      "Epoch 267, Loss: 0.6302, Val Loss: 0.6229\n",
      "Saving model at epoch 267 with loss 0.6302\n",
      "Epoch 268, Loss: 0.6302, Val Loss: 0.6229\n",
      "Saving model at epoch 268 with loss 0.6302\n",
      "Epoch 269, Loss: 0.6302, Val Loss: 0.6229\n",
      "Saving model at epoch 269 with loss 0.6302\n",
      "Epoch 270, Loss: 0.6302, Val Loss: 0.6229\n",
      "Saving model at epoch 270 with loss 0.6302\n",
      "Epoch 271, Loss: 0.6301, Val Loss: 0.6228\n",
      "Saving model at epoch 271 with loss 0.6301\n",
      "Epoch 272, Loss: 0.6301, Val Loss: 0.6228\n",
      "Saving model at epoch 272 with loss 0.6301\n",
      "Epoch 273, Loss: 0.6301, Val Loss: 0.6228\n",
      "Saving model at epoch 273 with loss 0.6301\n",
      "Epoch 274, Loss: 0.6301, Val Loss: 0.6228\n",
      "Saving model at epoch 274 with loss 0.6301\n",
      "Epoch 275, Loss: 0.6301, Val Loss: 0.6228\n",
      "Saving model at epoch 275 with loss 0.6301\n",
      "Epoch 276, Loss: 0.6301, Val Loss: 0.6228\n",
      "Saving model at epoch 276 with loss 0.6301\n",
      "Epoch 277, Loss: 0.6301, Val Loss: 0.6227\n",
      "Saving model at epoch 277 with loss 0.6301\n",
      "Epoch 278, Loss: 0.6300, Val Loss: 0.6227\n",
      "Saving model at epoch 278 with loss 0.6300\n",
      "Epoch 279, Loss: 0.6300, Val Loss: 0.6227\n",
      "Saving model at epoch 279 with loss 0.6300\n",
      "Epoch 280, Loss: 0.6300, Val Loss: 0.6227\n",
      "Saving model at epoch 280 with loss 0.6300\n",
      "Epoch 281, Loss: 0.6300, Val Loss: 0.6227\n",
      "Saving model at epoch 281 with loss 0.6300\n",
      "Epoch 282, Loss: 0.6300, Val Loss: 0.6227\n",
      "Saving model at epoch 282 with loss 0.6300\n",
      "Epoch 283, Loss: 0.6300, Val Loss: 0.6226\n",
      "Saving model at epoch 283 with loss 0.6300\n",
      "Epoch 284, Loss: 0.6300, Val Loss: 0.6226\n",
      "Saving model at epoch 284 with loss 0.6300\n",
      "Epoch 285, Loss: 0.6299, Val Loss: 0.6226\n",
      "Saving model at epoch 285 with loss 0.6299\n",
      "Epoch 286, Loss: 0.6299, Val Loss: 0.6226\n",
      "Saving model at epoch 286 with loss 0.6299\n",
      "Epoch 287, Loss: 0.6299, Val Loss: 0.6226\n",
      "Saving model at epoch 287 with loss 0.6299\n",
      "Epoch 288, Loss: 0.6299, Val Loss: 0.6226\n",
      "Saving model at epoch 288 with loss 0.6299\n",
      "Epoch 289, Loss: 0.6299, Val Loss: 0.6226\n",
      "Saving model at epoch 289 with loss 0.6299\n",
      "Epoch 290, Loss: 0.6299, Val Loss: 0.6225\n",
      "Saving model at epoch 290 with loss 0.6299\n",
      "Epoch 291, Loss: 0.6299, Val Loss: 0.6225\n",
      "Saving model at epoch 291 with loss 0.6299\n",
      "Epoch 292, Loss: 0.6299, Val Loss: 0.6225\n",
      "Saving model at epoch 292 with loss 0.6299\n",
      "Epoch 293, Loss: 0.6298, Val Loss: 0.6225\n",
      "Saving model at epoch 293 with loss 0.6298\n",
      "Epoch 294, Loss: 0.6298, Val Loss: 0.6225\n",
      "Saving model at epoch 294 with loss 0.6298\n",
      "Epoch 295, Loss: 0.6298, Val Loss: 0.6225\n",
      "Saving model at epoch 295 with loss 0.6298\n",
      "Epoch 296, Loss: 0.6298, Val Loss: 0.6225\n",
      "Saving model at epoch 296 with loss 0.6298\n",
      "Epoch 297, Loss: 0.6298, Val Loss: 0.6225\n",
      "Saving model at epoch 297 with loss 0.6298\n",
      "Epoch 298, Loss: 0.6298, Val Loss: 0.6224\n",
      "Saving model at epoch 298 with loss 0.6298\n",
      "Epoch 299, Loss: 0.6298, Val Loss: 0.6224\n",
      "Saving model at epoch 299 with loss 0.6298\n",
      "Epoch 300, Loss: 0.6298, Val Loss: 0.6224\n",
      "Saving model at epoch 300 with loss 0.6298\n",
      "Epoch 301, Loss: 0.6297, Val Loss: 0.6224\n",
      "Saving model at epoch 301 with loss 0.6297\n",
      "Epoch 302, Loss: 0.6297, Val Loss: 0.6224\n",
      "Saving model at epoch 302 with loss 0.6297\n",
      "Epoch 303, Loss: 0.6297, Val Loss: 0.6224\n",
      "Saving model at epoch 303 with loss 0.6297\n",
      "Epoch 304, Loss: 0.6297, Val Loss: 0.6224\n",
      "Saving model at epoch 304 with loss 0.6297\n",
      "Epoch 305, Loss: 0.6297, Val Loss: 0.6224\n",
      "Saving model at epoch 305 with loss 0.6297\n",
      "Epoch 306, Loss: 0.6297, Val Loss: 0.6224\n",
      "Saving model at epoch 306 with loss 0.6297\n",
      "Epoch 307, Loss: 0.6297, Val Loss: 0.6223\n",
      "Saving model at epoch 307 with loss 0.6297\n",
      "Epoch 308, Loss: 0.6297, Val Loss: 0.6223\n",
      "Saving model at epoch 308 with loss 0.6297\n",
      "Epoch 309, Loss: 0.6297, Val Loss: 0.6223\n",
      "Saving model at epoch 309 with loss 0.6297\n",
      "Epoch 310, Loss: 0.6297, Val Loss: 0.6223\n",
      "Saving model at epoch 310 with loss 0.6297\n",
      "Epoch 311, Loss: 0.6297, Val Loss: 0.6223\n",
      "Saving model at epoch 311 with loss 0.6297\n",
      "Epoch 312, Loss: 0.6296, Val Loss: 0.6223\n",
      "Saving model at epoch 312 with loss 0.6296\n",
      "Epoch 313, Loss: 0.6296, Val Loss: 0.6223\n",
      "Saving model at epoch 313 with loss 0.6296\n",
      "Epoch 314, Loss: 0.6296, Val Loss: 0.6223\n",
      "Saving model at epoch 314 with loss 0.6296\n",
      "Epoch 315, Loss: 0.6296, Val Loss: 0.6223\n",
      "Saving model at epoch 315 with loss 0.6296\n",
      "Epoch 316, Loss: 0.6296, Val Loss: 0.6223\n",
      "Saving model at epoch 316 with loss 0.6296\n",
      "Epoch 317, Loss: 0.6296, Val Loss: 0.6222\n",
      "Saving model at epoch 317 with loss 0.6296\n",
      "Epoch 318, Loss: 0.6296, Val Loss: 0.6222\n",
      "Saving model at epoch 318 with loss 0.6296\n",
      "Epoch 319, Loss: 0.6296, Val Loss: 0.6222\n",
      "Saving model at epoch 319 with loss 0.6296\n",
      "Epoch 320, Loss: 0.6296, Val Loss: 0.6222\n",
      "Saving model at epoch 320 with loss 0.6296\n",
      "Epoch 321, Loss: 0.6296, Val Loss: 0.6222\n",
      "Saving model at epoch 321 with loss 0.6296\n",
      "Epoch 322, Loss: 0.6296, Val Loss: 0.6222\n",
      "Saving model at epoch 322 with loss 0.6296\n",
      "Epoch 323, Loss: 0.6296, Val Loss: 0.6222\n",
      "Saving model at epoch 323 with loss 0.6296\n",
      "Epoch 324, Loss: 0.6295, Val Loss: 0.6222\n",
      "Saving model at epoch 324 with loss 0.6295\n",
      "Epoch 325, Loss: 0.6295, Val Loss: 0.6222\n",
      "Saving model at epoch 325 with loss 0.6295\n",
      "Epoch 326, Loss: 0.6295, Val Loss: 0.6222\n",
      "Saving model at epoch 326 with loss 0.6295\n",
      "Epoch 327, Loss: 0.6295, Val Loss: 0.6222\n",
      "Saving model at epoch 327 with loss 0.6295\n",
      "Epoch 328, Loss: 0.6295, Val Loss: 0.6222\n",
      "Saving model at epoch 328 with loss 0.6295\n",
      "Epoch 329, Loss: 0.6295, Val Loss: 0.6221\n",
      "Saving model at epoch 329 with loss 0.6295\n",
      "Epoch 330, Loss: 0.6295, Val Loss: 0.6221\n",
      "Saving model at epoch 330 with loss 0.6295\n",
      "Epoch 331, Loss: 0.6295, Val Loss: 0.6221\n",
      "Saving model at epoch 331 with loss 0.6295\n",
      "Epoch 332, Loss: 0.6295, Val Loss: 0.6221\n",
      "Saving model at epoch 332 with loss 0.6295\n",
      "Epoch 333, Loss: 0.6295, Val Loss: 0.6221\n",
      "Saving model at epoch 333 with loss 0.6295\n",
      "Epoch 334, Loss: 0.6295, Val Loss: 0.6221\n",
      "Saving model at epoch 334 with loss 0.6295\n",
      "Epoch 335, Loss: 0.6295, Val Loss: 0.6221\n",
      "Saving model at epoch 335 with loss 0.6295\n",
      "Epoch 336, Loss: 0.6295, Val Loss: 0.6221\n",
      "Saving model at epoch 336 with loss 0.6295\n",
      "Epoch 337, Loss: 0.6295, Val Loss: 0.6221\n",
      "Saving model at epoch 337 with loss 0.6295\n",
      "Epoch 338, Loss: 0.6295, Val Loss: 0.6221\n",
      "Saving model at epoch 338 with loss 0.6295\n",
      "Epoch 339, Loss: 0.6295, Val Loss: 0.6221\n",
      "Saving model at epoch 339 with loss 0.6295\n",
      "Epoch 340, Loss: 0.6294, Val Loss: 0.6221\n",
      "Saving model at epoch 340 with loss 0.6294\n",
      "Epoch 341, Loss: 0.6294, Val Loss: 0.6221\n",
      "Saving model at epoch 341 with loss 0.6294\n",
      "Epoch 342, Loss: 0.6294, Val Loss: 0.6221\n",
      "Saving model at epoch 342 with loss 0.6294\n",
      "Epoch 343, Loss: 0.6294, Val Loss: 0.6221\n",
      "Saving model at epoch 343 with loss 0.6294\n",
      "Epoch 344, Loss: 0.6294, Val Loss: 0.6220\n",
      "Saving model at epoch 344 with loss 0.6294\n",
      "Epoch 345, Loss: 0.6294, Val Loss: 0.6220\n",
      "Saving model at epoch 345 with loss 0.6294\n",
      "Epoch 346, Loss: 0.6294, Val Loss: 0.6220\n",
      "Saving model at epoch 346 with loss 0.6294\n",
      "Epoch 347, Loss: 0.6294, Val Loss: 0.6220\n",
      "Saving model at epoch 347 with loss 0.6294\n",
      "Epoch 348, Loss: 0.6294, Val Loss: 0.6220\n",
      "Saving model at epoch 348 with loss 0.6294\n",
      "Epoch 349, Loss: 0.6294, Val Loss: 0.6220\n",
      "Saving model at epoch 349 with loss 0.6294\n",
      "Epoch 350, Loss: 0.6294, Val Loss: 0.6220\n",
      "Saving model at epoch 350 with loss 0.6294\n",
      "Epoch 351, Loss: 0.6294, Val Loss: 0.6220\n",
      "Saving model at epoch 351 with loss 0.6294\n",
      "Epoch 352, Loss: 0.6294, Val Loss: 0.6220\n",
      "Saving model at epoch 352 with loss 0.6294\n",
      "Epoch 353, Loss: 0.6294, Val Loss: 0.6220\n",
      "Saving model at epoch 353 with loss 0.6294\n",
      "Epoch 354, Loss: 0.6294, Val Loss: 0.6220\n",
      "Saving model at epoch 354 with loss 0.6294\n",
      "Epoch 355, Loss: 0.6294, Val Loss: 0.6220\n",
      "Saving model at epoch 355 with loss 0.6294\n",
      "Epoch 356, Loss: 0.6294, Val Loss: 0.6220\n",
      "Saving model at epoch 356 with loss 0.6294\n",
      "Epoch 357, Loss: 0.6294, Val Loss: 0.6220\n",
      "Saving model at epoch 357 with loss 0.6294\n",
      "Epoch 358, Loss: 0.6294, Val Loss: 0.6220\n",
      "Saving model at epoch 358 with loss 0.6294\n",
      "Epoch 359, Loss: 0.6294, Val Loss: 0.6220\n",
      "Saving model at epoch 359 with loss 0.6294\n",
      "Epoch 360, Loss: 0.6294, Val Loss: 0.6220\n",
      "Saving model at epoch 360 with loss 0.6294\n",
      "Epoch 361, Loss: 0.6294, Val Loss: 0.6220\n",
      "Saving model at epoch 361 with loss 0.6294\n",
      "Epoch 362, Loss: 0.6294, Val Loss: 0.6220\n",
      "Saving model at epoch 362 with loss 0.6294\n",
      "Epoch 363, Loss: 0.6293, Val Loss: 0.6220\n",
      "Saving model at epoch 363 with loss 0.6293\n",
      "Epoch 364, Loss: 0.6293, Val Loss: 0.6220\n",
      "Saving model at epoch 364 with loss 0.6293\n",
      "Epoch 365, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 365 with loss 0.6293\n",
      "Epoch 366, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 366 with loss 0.6293\n",
      "Epoch 367, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 367 with loss 0.6293\n",
      "Epoch 368, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 368 with loss 0.6293\n",
      "Epoch 369, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 369 with loss 0.6293\n",
      "Epoch 370, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 370 with loss 0.6293\n",
      "Epoch 371, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 371 with loss 0.6293\n",
      "Epoch 372, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 372 with loss 0.6293\n",
      "Epoch 373, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 373 with loss 0.6293\n",
      "Epoch 374, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 374 with loss 0.6293\n",
      "Epoch 375, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 375 with loss 0.6293\n",
      "Epoch 376, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 376 with loss 0.6293\n",
      "Epoch 377, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 377 with loss 0.6293\n",
      "Epoch 378, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 378 with loss 0.6293\n",
      "Epoch 379, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 379 with loss 0.6293\n",
      "Epoch 380, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 380 with loss 0.6293\n",
      "Epoch 381, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 381 with loss 0.6293\n",
      "Epoch 382, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 382 with loss 0.6293\n",
      "Epoch 383, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 383 with loss 0.6293\n",
      "Epoch 384, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 384 with loss 0.6293\n",
      "Epoch 385, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 385 with loss 0.6293\n",
      "Epoch 386, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 386 with loss 0.6293\n",
      "Epoch 387, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 387 with loss 0.6293\n",
      "Epoch 388, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 388 with loss 0.6293\n",
      "Epoch 389, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 389 with loss 0.6293\n",
      "Epoch 390, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 390 with loss 0.6293\n",
      "Epoch 391, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 391 with loss 0.6293\n",
      "Epoch 392, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 392 with loss 0.6293\n",
      "Epoch 393, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 393 with loss 0.6293\n",
      "Epoch 394, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 394 with loss 0.6293\n",
      "Epoch 395, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 395 with loss 0.6293\n",
      "Epoch 396, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 396 with loss 0.6293\n",
      "Epoch 397, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 397 with loss 0.6293\n",
      "Epoch 398, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 398 with loss 0.6293\n",
      "Epoch 399, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 399 with loss 0.6293\n",
      "Epoch 400, Loss: 0.6293, Val Loss: 0.6219\n",
      "Saving model at epoch 400 with loss 0.6293\n",
      "Epoch 401, Loss: 0.6293, Val Loss: 0.6218\n",
      "Saving model at epoch 401 with loss 0.6293\n",
      "Epoch 402, Loss: 0.6293, Val Loss: 0.6218\n",
      "Saving model at epoch 402 with loss 0.6293\n",
      "Epoch 403, Loss: 0.6293, Val Loss: 0.6218\n",
      "Saving model at epoch 403 with loss 0.6293\n",
      "Epoch 404, Loss: 0.6293, Val Loss: 0.6218\n",
      "Saving model at epoch 404 with loss 0.6293\n",
      "Epoch 405, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 405 with loss 0.6292\n",
      "Epoch 406, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 406 with loss 0.6292\n",
      "Epoch 407, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 407 with loss 0.6292\n",
      "Epoch 408, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 408 with loss 0.6292\n",
      "Epoch 409, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 409 with loss 0.6292\n",
      "Epoch 410, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 410 with loss 0.6292\n",
      "Epoch 411, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 411 with loss 0.6292\n",
      "Epoch 412, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 412 with loss 0.6292\n",
      "Epoch 413, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 413 with loss 0.6292\n",
      "Epoch 414, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 414 with loss 0.6292\n",
      "Epoch 415, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 415 with loss 0.6292\n",
      "Epoch 416, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 416 with loss 0.6292\n",
      "Epoch 417, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 417 with loss 0.6292\n",
      "Epoch 418, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 418 with loss 0.6292\n",
      "Epoch 419, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 419 with loss 0.6292\n",
      "Epoch 420, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 420 with loss 0.6292\n",
      "Epoch 421, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 421 with loss 0.6292\n",
      "Epoch 422, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 422 with loss 0.6292\n",
      "Epoch 423, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 423 with loss 0.6292\n",
      "Epoch 424, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 424 with loss 0.6292\n",
      "Epoch 425, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 425 with loss 0.6292\n",
      "Epoch 426, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 426 with loss 0.6292\n",
      "Epoch 427, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 427 with loss 0.6292\n",
      "Epoch 428, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 428 with loss 0.6292\n",
      "Epoch 429, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 429 with loss 0.6292\n",
      "Epoch 430, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 430 with loss 0.6292\n",
      "Epoch 431, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 431 with loss 0.6292\n",
      "Epoch 432, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 432 with loss 0.6292\n",
      "Epoch 433, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 433 with loss 0.6292\n",
      "Epoch 434, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 434 with loss 0.6292\n",
      "Epoch 435, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 435 with loss 0.6292\n",
      "Epoch 436, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 436 with loss 0.6292\n",
      "Epoch 437, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 437 with loss 0.6292\n",
      "Epoch 438, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 438 with loss 0.6292\n",
      "Epoch 439, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 439 with loss 0.6292\n",
      "Epoch 440, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 440 with loss 0.6292\n",
      "Epoch 441, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 441 with loss 0.6292\n",
      "Epoch 442, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 442 with loss 0.6292\n",
      "Epoch 443, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 443 with loss 0.6292\n",
      "Epoch 444, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 444 with loss 0.6292\n",
      "Epoch 445, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 445 with loss 0.6292\n",
      "Epoch 446, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 446 with loss 0.6292\n",
      "Epoch 447, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 447 with loss 0.6292\n",
      "Epoch 448, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 448 with loss 0.6292\n",
      "Epoch 449, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 449 with loss 0.6292\n",
      "Epoch 450, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 450 with loss 0.6292\n",
      "Epoch 451, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 451 with loss 0.6292\n",
      "Epoch 452, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 452 with loss 0.6292\n",
      "Epoch 453, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 453 with loss 0.6292\n",
      "Epoch 454, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 454 with loss 0.6292\n",
      "Epoch 455, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 455 with loss 0.6292\n",
      "Epoch 456, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 456 with loss 0.6292\n",
      "Epoch 457, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 457 with loss 0.6292\n",
      "Epoch 458, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 458 with loss 0.6292\n",
      "Epoch 459, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 459 with loss 0.6292\n",
      "Epoch 460, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 460 with loss 0.6292\n",
      "Epoch 461, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 461 with loss 0.6292\n",
      "Epoch 462, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 462 with loss 0.6292\n",
      "Epoch 463, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 463 with loss 0.6292\n",
      "Epoch 464, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 464 with loss 0.6292\n",
      "Epoch 465, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 465 with loss 0.6292\n",
      "Epoch 466, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 466 with loss 0.6292\n",
      "Epoch 467, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 467 with loss 0.6292\n",
      "Epoch 468, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 468 with loss 0.6292\n",
      "Epoch 469, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 469 with loss 0.6292\n",
      "Epoch 470, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 470 with loss 0.6292\n",
      "Epoch 471, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 471 with loss 0.6292\n",
      "Epoch 472, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 472 with loss 0.6292\n",
      "Epoch 473, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 473 with loss 0.6292\n",
      "Epoch 474, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 474 with loss 0.6292\n",
      "Epoch 475, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 475 with loss 0.6292\n",
      "Epoch 476, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 476 with loss 0.6292\n",
      "Epoch 477, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 477 with loss 0.6292\n",
      "Epoch 478, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 478 with loss 0.6292\n",
      "Epoch 479, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 479 with loss 0.6292\n",
      "Epoch 480, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 480 with loss 0.6292\n",
      "Epoch 481, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 481 with loss 0.6292\n",
      "Epoch 482, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 482 with loss 0.6292\n",
      "Epoch 483, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 483 with loss 0.6292\n",
      "Epoch 484, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 484 with loss 0.6292\n",
      "Epoch 485, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 485 with loss 0.6292\n",
      "Epoch 486, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 486 with loss 0.6292\n",
      "Epoch 487, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 487 with loss 0.6292\n",
      "Epoch 488, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 488 with loss 0.6292\n",
      "Epoch 489, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 489 with loss 0.6292\n",
      "Epoch 490, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 490 with loss 0.6292\n",
      "Epoch 491, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 491 with loss 0.6292\n",
      "Epoch 492, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 492 with loss 0.6292\n",
      "Epoch 493, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 493 with loss 0.6292\n",
      "Epoch 494, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 494 with loss 0.6292\n",
      "Epoch 495, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 495 with loss 0.6292\n",
      "Epoch 496, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 496 with loss 0.6292\n",
      "Epoch 497, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 497 with loss 0.6292\n",
      "Epoch 498, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 498 with loss 0.6292\n",
      "Epoch 499, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 499 with loss 0.6292\n",
      "Epoch 500, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 500 with loss 0.6292\n",
      "Epoch 501, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 501 with loss 0.6292\n",
      "Epoch 502, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 502 with loss 0.6292\n",
      "Epoch 503, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 503 with loss 0.6292\n",
      "Epoch 504, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 504 with loss 0.6292\n",
      "Epoch 505, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 505 with loss 0.6292\n",
      "Epoch 506, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 506 with loss 0.6292\n",
      "Epoch 507, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 507 with loss 0.6292\n",
      "Epoch 508, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 508 with loss 0.6292\n",
      "Epoch 509, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 509 with loss 0.6292\n",
      "Epoch 510, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 510 with loss 0.6292\n",
      "Epoch 511, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 511 with loss 0.6292\n",
      "Epoch 512, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 512 with loss 0.6292\n",
      "Epoch 513, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 513 with loss 0.6292\n",
      "Epoch 514, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 514 with loss 0.6292\n",
      "Epoch 515, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 515 with loss 0.6292\n",
      "Epoch 516, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 516 with loss 0.6292\n",
      "Epoch 517, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 517 with loss 0.6292\n",
      "Epoch 518, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 518 with loss 0.6292\n",
      "Epoch 519, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 519 with loss 0.6292\n",
      "Epoch 520, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 520 with loss 0.6292\n",
      "Epoch 521, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 521 with loss 0.6292\n",
      "Epoch 522, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 522 with loss 0.6292\n",
      "Epoch 523, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 523 with loss 0.6292\n",
      "Epoch 524, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 524 with loss 0.6292\n",
      "Epoch 525, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 525 with loss 0.6292\n",
      "Epoch 526, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 526 with loss 0.6292\n",
      "Epoch 527, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 527 with loss 0.6292\n",
      "Epoch 528, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 528 with loss 0.6292\n",
      "Epoch 529, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 529 with loss 0.6292\n",
      "Epoch 530, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 530 with loss 0.6292\n",
      "Epoch 531, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 531 with loss 0.6292\n",
      "Epoch 532, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 532 with loss 0.6292\n",
      "Epoch 533, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 533 with loss 0.6292\n",
      "Epoch 534, Loss: 0.6292, Val Loss: 0.6218\n",
      "Saving model at epoch 534 with loss 0.6292\n",
      "Epoch 535, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 535 with loss 0.6292\n",
      "Epoch 536, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 536 with loss 0.6292\n",
      "Epoch 537, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 537 with loss 0.6292\n",
      "Epoch 538, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 538 with loss 0.6292\n",
      "Epoch 539, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 539 with loss 0.6292\n",
      "Epoch 540, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 540 with loss 0.6292\n",
      "Epoch 541, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 541 with loss 0.6292\n",
      "Epoch 542, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 542 with loss 0.6292\n",
      "Epoch 543, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 543 with loss 0.6292\n",
      "Epoch 544, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 544 with loss 0.6292\n",
      "Epoch 545, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 545 with loss 0.6292\n",
      "Epoch 546, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 546 with loss 0.6292\n",
      "Epoch 547, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 547 with loss 0.6292\n",
      "Epoch 548, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 548 with loss 0.6292\n",
      "Epoch 549, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 549 with loss 0.6292\n",
      "Epoch 550, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 550 with loss 0.6292\n",
      "Epoch 551, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 551 with loss 0.6292\n",
      "Epoch 552, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 552 with loss 0.6292\n",
      "Epoch 553, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 553 with loss 0.6292\n",
      "Epoch 554, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 554 with loss 0.6292\n",
      "Epoch 555, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 555 with loss 0.6292\n",
      "Epoch 556, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 556 with loss 0.6292\n",
      "Epoch 557, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 557 with loss 0.6292\n",
      "Epoch 558, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 558 with loss 0.6292\n",
      "Epoch 559, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 559 with loss 0.6292\n",
      "Epoch 560, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 560 with loss 0.6292\n",
      "Epoch 561, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 561 with loss 0.6292\n",
      "Epoch 562, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 562 with loss 0.6292\n",
      "Epoch 563, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 563 with loss 0.6292\n",
      "Epoch 564, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 564 with loss 0.6292\n",
      "Epoch 565, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 565 with loss 0.6292\n",
      "Epoch 566, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 566 with loss 0.6292\n",
      "Epoch 567, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 567 with loss 0.6292\n",
      "Epoch 568, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 568 with loss 0.6292\n",
      "Epoch 569, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 569 with loss 0.6292\n",
      "Epoch 570, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 570 with loss 0.6292\n",
      "Epoch 571, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 571 with loss 0.6292\n",
      "Epoch 572, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 572 with loss 0.6292\n",
      "Epoch 573, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 573 with loss 0.6292\n",
      "Epoch 574, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 574 with loss 0.6292\n",
      "Epoch 575, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 575 with loss 0.6292\n",
      "Epoch 576, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 576 with loss 0.6292\n",
      "Epoch 577, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 577 with loss 0.6292\n",
      "Epoch 578, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 578 with loss 0.6292\n",
      "Epoch 579, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 579 with loss 0.6292\n",
      "Epoch 580, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 580 with loss 0.6292\n",
      "Epoch 581, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 581 with loss 0.6292\n",
      "Epoch 582, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 582 with loss 0.6292\n",
      "Epoch 583, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 583 with loss 0.6292\n",
      "Epoch 584, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 584 with loss 0.6292\n",
      "Epoch 585, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 585 with loss 0.6292\n",
      "Epoch 586, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 586 with loss 0.6292\n",
      "Epoch 587, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 587 with loss 0.6292\n",
      "Epoch 588, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 588 with loss 0.6292\n",
      "Epoch 589, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 589 with loss 0.6292\n",
      "Epoch 590, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 590 with loss 0.6292\n",
      "Epoch 591, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 591 with loss 0.6292\n",
      "Epoch 592, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 592 with loss 0.6292\n",
      "Epoch 593, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 593 with loss 0.6292\n",
      "Epoch 594, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 594 with loss 0.6292\n",
      "Epoch 595, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 595 with loss 0.6292\n",
      "Epoch 596, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 596 with loss 0.6292\n",
      "Epoch 597, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 597 with loss 0.6292\n",
      "Epoch 598, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 598 with loss 0.6292\n",
      "Epoch 599, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 599 with loss 0.6292\n",
      "Epoch 600, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 600 with loss 0.6292\n",
      "Epoch 601, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 601 with loss 0.6292\n",
      "Epoch 602, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 602 with loss 0.6292\n",
      "Epoch 603, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 603 with loss 0.6292\n",
      "Epoch 604, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 604 with loss 0.6292\n",
      "Epoch 605, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 605 with loss 0.6292\n",
      "Epoch 606, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 606 with loss 0.6292\n",
      "Epoch 607, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 607 with loss 0.6292\n",
      "Epoch 608, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 608 with loss 0.6292\n",
      "Epoch 609, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 609 with loss 0.6292\n",
      "Epoch 610, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 610 with loss 0.6292\n",
      "Epoch 611, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 611 with loss 0.6292\n",
      "Epoch 612, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 612 with loss 0.6292\n",
      "Epoch 613, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 613 with loss 0.6292\n",
      "Epoch 614, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 614 with loss 0.6292\n",
      "Epoch 615, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 615 with loss 0.6292\n",
      "Epoch 616, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 616 with loss 0.6292\n",
      "Epoch 617, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 617 with loss 0.6292\n",
      "Epoch 618, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 618 with loss 0.6292\n",
      "Epoch 619, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 619 with loss 0.6292\n",
      "Epoch 620, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 620 with loss 0.6292\n",
      "Epoch 621, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 621 with loss 0.6292\n",
      "Epoch 622, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 622 with loss 0.6292\n",
      "Epoch 623, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 623 with loss 0.6292\n",
      "Epoch 624, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 624 with loss 0.6292\n",
      "Epoch 625, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 625 with loss 0.6292\n",
      "Epoch 626, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 626 with loss 0.6292\n",
      "Epoch 627, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 627 with loss 0.6292\n",
      "Epoch 628, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 628 with loss 0.6292\n",
      "Epoch 629, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 629 with loss 0.6292\n",
      "Epoch 630, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 630 with loss 0.6292\n",
      "Epoch 631, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 631 with loss 0.6292\n",
      "Epoch 632, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 632 with loss 0.6292\n",
      "Epoch 633, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 633 with loss 0.6292\n",
      "Epoch 634, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 634 with loss 0.6292\n",
      "Epoch 635, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 635 with loss 0.6292\n",
      "Epoch 636, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 636 with loss 0.6292\n",
      "Epoch 637, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 637 with loss 0.6292\n",
      "Epoch 638, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 638 with loss 0.6292\n",
      "Epoch 639, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 639 with loss 0.6292\n",
      "Epoch 640, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 640 with loss 0.6292\n",
      "Epoch 641, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 641 with loss 0.6292\n",
      "Epoch 642, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 642 with loss 0.6292\n",
      "Epoch 643, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 643 with loss 0.6292\n",
      "Epoch 644, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 644 with loss 0.6292\n",
      "Epoch 645, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 645 with loss 0.6292\n",
      "Epoch 646, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 646 with loss 0.6292\n",
      "Epoch 647, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 647 with loss 0.6292\n",
      "Epoch 648, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 648 with loss 0.6292\n",
      "Epoch 649, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 649 with loss 0.6292\n",
      "Epoch 650, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 650 with loss 0.6292\n",
      "Epoch 651, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 651 with loss 0.6292\n",
      "Epoch 652, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 652 with loss 0.6292\n",
      "Epoch 653, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 653 with loss 0.6292\n",
      "Epoch 654, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 654 with loss 0.6292\n",
      "Epoch 655, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 655 with loss 0.6292\n",
      "Epoch 656, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 656 with loss 0.6292\n",
      "Epoch 657, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 657 with loss 0.6292\n",
      "Epoch 658, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 658 with loss 0.6292\n",
      "Epoch 659, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 659 with loss 0.6292\n",
      "Epoch 660, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 660 with loss 0.6292\n",
      "Epoch 661, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 661 with loss 0.6292\n",
      "Epoch 662, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 662 with loss 0.6292\n",
      "Epoch 663, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 663 with loss 0.6292\n",
      "Epoch 664, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 664 with loss 0.6292\n",
      "Epoch 665, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 665 with loss 0.6292\n",
      "Epoch 666, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 666 with loss 0.6292\n",
      "Epoch 667, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 667 with loss 0.6292\n",
      "Epoch 668, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 668 with loss 0.6292\n",
      "Epoch 669, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 669 with loss 0.6292\n",
      "Epoch 670, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 670 with loss 0.6292\n",
      "Epoch 671, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 671 with loss 0.6292\n",
      "Epoch 672, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 672 with loss 0.6292\n",
      "Epoch 673, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 673 with loss 0.6292\n",
      "Epoch 674, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 674 with loss 0.6292\n",
      "Epoch 675, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 675 with loss 0.6292\n",
      "Epoch 676, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 676 with loss 0.6292\n",
      "Epoch 677, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 677 with loss 0.6292\n",
      "Epoch 678, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 678 with loss 0.6292\n",
      "Epoch 679, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 679 with loss 0.6292\n",
      "Epoch 680, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 680 with loss 0.6292\n",
      "Epoch 681, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 681 with loss 0.6292\n",
      "Epoch 682, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 682 with loss 0.6292\n",
      "Epoch 683, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 683 with loss 0.6292\n",
      "Epoch 684, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 684 with loss 0.6292\n",
      "Epoch 685, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 685 with loss 0.6292\n",
      "Epoch 686, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 686 with loss 0.6292\n",
      "Epoch 687, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 687 with loss 0.6292\n",
      "Epoch 688, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 688 with loss 0.6292\n",
      "Epoch 689, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 689 with loss 0.6292\n",
      "Epoch 690, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 690 with loss 0.6292\n",
      "Epoch 691, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 691 with loss 0.6292\n",
      "Epoch 692, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 692 with loss 0.6292\n",
      "Epoch 693, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 693 with loss 0.6292\n",
      "Epoch 694, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 694 with loss 0.6292\n",
      "Epoch 695, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 695 with loss 0.6292\n",
      "Epoch 696, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 696 with loss 0.6292\n",
      "Epoch 697, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 697 with loss 0.6292\n",
      "Epoch 698, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 698 with loss 0.6292\n",
      "Epoch 699, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 699 with loss 0.6292\n",
      "Epoch 700, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 700 with loss 0.6292\n",
      "Epoch 701, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 701 with loss 0.6292\n",
      "Epoch 702, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 702 with loss 0.6292\n",
      "Epoch 703, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 703 with loss 0.6292\n",
      "Epoch 704, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 704 with loss 0.6292\n",
      "Epoch 705, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 705 with loss 0.6292\n",
      "Epoch 706, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 706 with loss 0.6292\n",
      "Epoch 707, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 707 with loss 0.6292\n",
      "Epoch 708, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 708 with loss 0.6292\n",
      "Epoch 709, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 709 with loss 0.6292\n",
      "Epoch 710, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 710 with loss 0.6292\n",
      "Epoch 711, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 711 with loss 0.6292\n",
      "Epoch 712, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 712 with loss 0.6292\n",
      "Epoch 713, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 713 with loss 0.6292\n",
      "Epoch 714, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 714 with loss 0.6292\n",
      "Epoch 715, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 715 with loss 0.6292\n",
      "Epoch 716, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 716 with loss 0.6292\n",
      "Epoch 717, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 717 with loss 0.6292\n",
      "Epoch 718, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 718 with loss 0.6292\n",
      "Epoch 719, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 719 with loss 0.6292\n",
      "Epoch 720, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 720 with loss 0.6292\n",
      "Epoch 721, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 721 with loss 0.6292\n",
      "Epoch 722, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 722 with loss 0.6292\n",
      "Epoch 723, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 723 with loss 0.6292\n",
      "Epoch 724, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 724 with loss 0.6292\n",
      "Epoch 725, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 725 with loss 0.6292\n",
      "Epoch 726, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 726 with loss 0.6292\n",
      "Epoch 727, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 727 with loss 0.6292\n",
      "Epoch 728, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 728 with loss 0.6292\n",
      "Epoch 729, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 729 with loss 0.6292\n",
      "Epoch 730, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 730 with loss 0.6292\n",
      "Epoch 731, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 731 with loss 0.6292\n",
      "Epoch 732, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 732 with loss 0.6292\n",
      "Epoch 733, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 733 with loss 0.6292\n",
      "Epoch 734, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 734 with loss 0.6292\n",
      "Epoch 735, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 735 with loss 0.6292\n",
      "Epoch 736, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 736 with loss 0.6292\n",
      "Epoch 737, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 737 with loss 0.6292\n",
      "Epoch 738, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 738 with loss 0.6292\n",
      "Epoch 739, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 739 with loss 0.6292\n",
      "Epoch 740, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 740 with loss 0.6292\n",
      "Epoch 741, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 741 with loss 0.6292\n",
      "Epoch 742, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 742 with loss 0.6292\n",
      "Epoch 743, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 743 with loss 0.6292\n",
      "Epoch 744, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 744 with loss 0.6292\n",
      "Epoch 745, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 745 with loss 0.6292\n",
      "Epoch 746, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 746 with loss 0.6292\n",
      "Epoch 747, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 747 with loss 0.6292\n",
      "Epoch 748, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 748 with loss 0.6292\n",
      "Epoch 749, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 749 with loss 0.6292\n",
      "Epoch 750, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 750 with loss 0.6292\n",
      "Epoch 751, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 751 with loss 0.6292\n",
      "Epoch 752, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 752 with loss 0.6292\n",
      "Epoch 753, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 753 with loss 0.6292\n",
      "Epoch 754, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 754 with loss 0.6292\n",
      "Epoch 755, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 755 with loss 0.6292\n",
      "Epoch 756, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 756 with loss 0.6292\n",
      "Epoch 757, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 757 with loss 0.6292\n",
      "Epoch 758, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 758 with loss 0.6292\n",
      "Epoch 759, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 759 with loss 0.6292\n",
      "Epoch 760, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 760 with loss 0.6292\n",
      "Epoch 761, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 761 with loss 0.6292\n",
      "Epoch 762, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 762 with loss 0.6292\n",
      "Epoch 763, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 763 with loss 0.6292\n",
      "Epoch 764, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 764 with loss 0.6292\n",
      "Epoch 765, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 765 with loss 0.6292\n",
      "Epoch 766, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 766 with loss 0.6292\n",
      "Epoch 767, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 767 with loss 0.6292\n",
      "Epoch 768, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 768 with loss 0.6292\n",
      "Epoch 769, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 769 with loss 0.6292\n",
      "Epoch 770, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 770 with loss 0.6292\n",
      "Epoch 771, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 771 with loss 0.6292\n",
      "Epoch 772, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 772 with loss 0.6292\n",
      "Epoch 773, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 773 with loss 0.6292\n",
      "Epoch 774, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 774 with loss 0.6292\n",
      "Epoch 775, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 775 with loss 0.6292\n",
      "Epoch 776, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 776 with loss 0.6292\n",
      "Epoch 777, Loss: 0.6292, Val Loss: 0.6217\n",
      "Saving model at epoch 777 with loss 0.6292\n",
      "Epoch 778, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 778 with loss 0.6291\n",
      "Epoch 779, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 779 with loss 0.6291\n",
      "Epoch 780, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 780 with loss 0.6291\n",
      "Epoch 781, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 781 with loss 0.6291\n",
      "Epoch 782, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 782 with loss 0.6291\n",
      "Epoch 783, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 783 with loss 0.6291\n",
      "Epoch 784, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 784 with loss 0.6291\n",
      "Epoch 785, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 785 with loss 0.6291\n",
      "Epoch 786, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 786 with loss 0.6291\n",
      "Epoch 787, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 787 with loss 0.6291\n",
      "Epoch 788, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 788 with loss 0.6291\n",
      "Epoch 789, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 789 with loss 0.6291\n",
      "Epoch 790, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 790 with loss 0.6291\n",
      "Epoch 791, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 791 with loss 0.6291\n",
      "Epoch 792, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 792 with loss 0.6291\n",
      "Epoch 793, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 793 with loss 0.6291\n",
      "Epoch 794, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 794 with loss 0.6291\n",
      "Epoch 795, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 795 with loss 0.6291\n",
      "Epoch 796, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 796 with loss 0.6291\n",
      "Epoch 797, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 797 with loss 0.6291\n",
      "Epoch 798, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 798 with loss 0.6291\n",
      "Epoch 799, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 799 with loss 0.6291\n",
      "Epoch 800, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 800 with loss 0.6291\n",
      "Epoch 801, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 801 with loss 0.6291\n",
      "Epoch 802, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 802 with loss 0.6291\n",
      "Epoch 803, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 803 with loss 0.6291\n",
      "Epoch 804, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 804 with loss 0.6291\n",
      "Epoch 805, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 805 with loss 0.6291\n",
      "Epoch 806, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 806 with loss 0.6291\n",
      "Epoch 807, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 807 with loss 0.6291\n",
      "Epoch 808, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 808 with loss 0.6291\n",
      "Epoch 809, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 809 with loss 0.6291\n",
      "Epoch 810, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 810 with loss 0.6291\n",
      "Epoch 811, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 811 with loss 0.6291\n",
      "Epoch 812, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 812 with loss 0.6291\n",
      "Epoch 813, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 813 with loss 0.6291\n",
      "Epoch 814, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 814 with loss 0.6291\n",
      "Epoch 815, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 815 with loss 0.6291\n",
      "Epoch 816, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 816 with loss 0.6291\n",
      "Epoch 817, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 817 with loss 0.6291\n",
      "Epoch 818, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 818 with loss 0.6291\n",
      "Epoch 819, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 819 with loss 0.6291\n",
      "Epoch 820, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 820 with loss 0.6291\n",
      "Epoch 821, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 821 with loss 0.6291\n",
      "Epoch 822, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 822 with loss 0.6291\n",
      "Epoch 823, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 823 with loss 0.6291\n",
      "Epoch 824, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 824 with loss 0.6291\n",
      "Epoch 825, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 825 with loss 0.6291\n",
      "Epoch 826, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 826 with loss 0.6291\n",
      "Epoch 827, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 827 with loss 0.6291\n",
      "Epoch 828, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 828 with loss 0.6291\n",
      "Epoch 829, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 829 with loss 0.6291\n",
      "Epoch 830, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 830 with loss 0.6291\n",
      "Epoch 831, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 831 with loss 0.6291\n",
      "Epoch 832, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 832 with loss 0.6291\n",
      "Epoch 833, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 833 with loss 0.6291\n",
      "Epoch 834, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 834 with loss 0.6291\n",
      "Epoch 835, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 835 with loss 0.6291\n",
      "Epoch 836, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 836 with loss 0.6291\n",
      "Epoch 837, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 837 with loss 0.6291\n",
      "Epoch 838, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 838 with loss 0.6291\n",
      "Epoch 839, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 839 with loss 0.6291\n",
      "Epoch 840, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 840 with loss 0.6291\n",
      "Epoch 841, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 841 with loss 0.6291\n",
      "Epoch 842, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 842 with loss 0.6291\n",
      "Epoch 843, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 843 with loss 0.6291\n",
      "Epoch 844, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 844 with loss 0.6291\n",
      "Epoch 845, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 845 with loss 0.6291\n",
      "Epoch 846, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 846 with loss 0.6291\n",
      "Epoch 847, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 847 with loss 0.6291\n",
      "Epoch 848, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 848 with loss 0.6291\n",
      "Epoch 849, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 849 with loss 0.6291\n",
      "Epoch 850, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 850 with loss 0.6291\n",
      "Epoch 851, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 851 with loss 0.6291\n",
      "Epoch 852, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 852 with loss 0.6291\n",
      "Epoch 853, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 853 with loss 0.6291\n",
      "Epoch 854, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 854 with loss 0.6291\n",
      "Epoch 855, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 855 with loss 0.6291\n",
      "Epoch 856, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 856 with loss 0.6291\n",
      "Epoch 857, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 857 with loss 0.6291\n",
      "Epoch 858, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 858 with loss 0.6291\n",
      "Epoch 859, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 859 with loss 0.6291\n",
      "Epoch 860, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 860 with loss 0.6291\n",
      "Epoch 861, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 861 with loss 0.6291\n",
      "Epoch 862, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 862 with loss 0.6291\n",
      "Epoch 863, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 863 with loss 0.6291\n",
      "Epoch 864, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 864 with loss 0.6291\n",
      "Epoch 865, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 865 with loss 0.6291\n",
      "Epoch 866, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 866 with loss 0.6291\n",
      "Epoch 867, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 867 with loss 0.6291\n",
      "Epoch 868, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 868 with loss 0.6291\n",
      "Epoch 869, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 869 with loss 0.6291\n",
      "Epoch 870, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 870 with loss 0.6291\n",
      "Epoch 871, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 871 with loss 0.6291\n",
      "Epoch 872, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 872 with loss 0.6291\n",
      "Epoch 873, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 873 with loss 0.6291\n",
      "Epoch 874, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 874 with loss 0.6291\n",
      "Epoch 875, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 875 with loss 0.6291\n",
      "Epoch 876, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 876 with loss 0.6291\n",
      "Epoch 877, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 877 with loss 0.6291\n",
      "Epoch 878, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 878 with loss 0.6291\n",
      "Epoch 879, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 879 with loss 0.6291\n",
      "Epoch 880, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 880 with loss 0.6291\n",
      "Epoch 881, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 881 with loss 0.6291\n",
      "Epoch 882, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 882 with loss 0.6291\n",
      "Epoch 883, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 883 with loss 0.6291\n",
      "Epoch 884, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 884 with loss 0.6291\n",
      "Epoch 885, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 885 with loss 0.6291\n",
      "Epoch 886, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 886 with loss 0.6291\n",
      "Epoch 887, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 887 with loss 0.6291\n",
      "Epoch 888, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 888 with loss 0.6291\n",
      "Epoch 889, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 889 with loss 0.6291\n",
      "Epoch 890, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 890 with loss 0.6291\n",
      "Epoch 891, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 891 with loss 0.6291\n",
      "Epoch 892, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 892 with loss 0.6291\n",
      "Epoch 893, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 893 with loss 0.6291\n",
      "Epoch 894, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 894 with loss 0.6291\n",
      "Epoch 895, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 895 with loss 0.6291\n",
      "Epoch 896, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 896 with loss 0.6291\n",
      "Epoch 897, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 897 with loss 0.6291\n",
      "Epoch 898, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 898 with loss 0.6291\n",
      "Epoch 899, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 899 with loss 0.6291\n",
      "Epoch 900, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 900 with loss 0.6291\n",
      "Epoch 901, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 901 with loss 0.6291\n",
      "Epoch 902, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 902 with loss 0.6291\n",
      "Epoch 903, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 903 with loss 0.6291\n",
      "Epoch 904, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 904 with loss 0.6291\n",
      "Epoch 905, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 905 with loss 0.6291\n",
      "Epoch 906, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 906 with loss 0.6291\n",
      "Epoch 907, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 907 with loss 0.6291\n",
      "Epoch 908, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 908 with loss 0.6291\n",
      "Epoch 909, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 909 with loss 0.6291\n",
      "Epoch 910, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 910 with loss 0.6291\n",
      "Epoch 911, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 911 with loss 0.6291\n",
      "Epoch 912, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 912 with loss 0.6291\n",
      "Epoch 913, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 913 with loss 0.6291\n",
      "Epoch 914, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 914 with loss 0.6291\n",
      "Epoch 915, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 915 with loss 0.6291\n",
      "Epoch 916, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 916 with loss 0.6291\n",
      "Epoch 917, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 917 with loss 0.6291\n",
      "Epoch 918, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 918 with loss 0.6291\n",
      "Epoch 919, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 919 with loss 0.6291\n",
      "Epoch 920, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 920 with loss 0.6291\n",
      "Epoch 921, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 921 with loss 0.6291\n",
      "Epoch 922, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 922 with loss 0.6291\n",
      "Epoch 923, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 923 with loss 0.6291\n",
      "Epoch 924, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 924 with loss 0.6291\n",
      "Epoch 925, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 925 with loss 0.6291\n",
      "Epoch 926, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 926 with loss 0.6291\n",
      "Epoch 927, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 927 with loss 0.6291\n",
      "Epoch 928, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 928 with loss 0.6291\n",
      "Epoch 929, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 929 with loss 0.6291\n",
      "Epoch 930, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 930 with loss 0.6291\n",
      "Epoch 931, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 931 with loss 0.6291\n",
      "Epoch 932, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 932 with loss 0.6291\n",
      "Epoch 933, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 933 with loss 0.6291\n",
      "Epoch 934, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 934 with loss 0.6291\n",
      "Epoch 935, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 935 with loss 0.6291\n",
      "Epoch 936, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 936 with loss 0.6291\n",
      "Epoch 937, Loss: 0.6291, Val Loss: 0.6217\n",
      "Saving model at epoch 937 with loss 0.6291\n",
      "Epoch 938, Loss: 0.6291, Val Loss: 0.6217\n"
     ]
    }
   ],
   "source": [
    "# Define the features we want to keep\n",
    "# Keep only pickup and drop off positions, and add pickup and dropoff month, day, hour\n",
    "features_model2 = [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\",\n",
    "                   \"pickup_month\", \"pickup_day\", \"pickup_hour\", \"dropoff_month\", \"dropoff_day\", \"dropoff_hour\"]\n",
    "\n",
    "# Split 10% of the training data as validation set\n",
    "model2_x = select_features(X_train, features_model2)\n",
    "model2_y = y_train\n",
    "model2_x_train, model2_x_val, model2_y_train, model2_y_val = train_test_split(model2_x, model2_y, test_size=0.1, random_state=0)\n",
    "model2_x_test = select_features(X_test, features_model2)\n",
    "\n",
    "# Define Model\n",
    "model2 = Sequential()\n",
    "model2.add(LinearLayer(model2_x_train.shape[1], 10))\n",
    "model2.add(ReLU())\n",
    "model2.add(LinearLayer(10, 10))\n",
    "model2.add(ReLU())\n",
    "model2.add(LinearLayer(10, 10))\n",
    "model2.add(ReLU())\n",
    "model2.add(LinearLayer(10, 1))\n",
    "\n",
    "# Training Setup\n",
    "epochs_model2 = 1000\n",
    "learning_rate_model2 = 0.01\n",
    "loss_function_model2 = MeanSquaredErrorLoss()\n",
    "\n",
    "best_loss_model2 = float(\"inf\")\n",
    "patience_model2 = 3  # Early stopping patience\n",
    "stagnation_model2 = 0\n",
    "losses_model2 = []\n",
    "val_losses_model2 = []\n",
    "\n",
    "for epoch_model2 in range(epochs_model2):\n",
    "    predictions_model2 = model2.forward(model2_x_train)\n",
    "    loss_model2 = loss_function_model2.forward(predictions_model2, model2_y_train)\n",
    "    losses_model2.append(loss_model2)\n",
    "    \n",
    "    val_predictions_model2 = model2.forward(model2_x_val)\n",
    "    val_loss_model2 = loss_function_model2.forward(val_predictions_model2, model2_y_val)\n",
    "    val_losses_model2.append(val_loss_model2)\n",
    "    \n",
    "    print(f\"Epoch {epoch_model2}, Loss: {loss_model2:.4f}, Val Loss: {val_loss_model2:.4f}\")\n",
    "    \n",
    "    grad_output_model2 = loss_function_model2.backward()\n",
    "    model2.backward(grad_output_model2, learning_rate_model2)\n",
    "\n",
    "    # Early stopping\n",
    "    if loss_model2 < best_loss_model2:\n",
    "        best_loss_model2 = loss_model2\n",
    "        stagnation_model2 = 0\n",
    "        print(f\"Saving model at epoch {epoch_model2} with loss {loss_model2:.4f}\")\n",
    "        model2.save_weights(\"best_model2\")\n",
    "    else:\n",
    "        stagnation_model2 += 1\n",
    "        if stagnation_model2 >= patience_model2:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load best model and evaluate on test set\n",
    "test_predictions_model2 = model2.forward(model2_x_test)\n",
    "test_loss_model2 = loss_function_model2.forward(test_predictions_model2, y_test)\n",
    "print(f\"Test Loss (MSE): {test_loss_model2:.4f}\")\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses_model2, label='Training Loss')\n",
    "plt.plot(val_losses_model2, label='Validation Loss', linestyle='dashed')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickup Time, Position and Dropoff Time, Position and day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 31.8480, Val Loss: 31.7364\n",
      "Saving model at epoch 0 with loss 31.8480\n",
      "Epoch 1, Loss: 26.5976, Val Loss: 26.4379\n",
      "Saving model at epoch 1 with loss 26.5976\n",
      "Epoch 2, Loss: 21.4951, Val Loss: 21.2860\n",
      "Saving model at epoch 2 with loss 21.4951\n",
      "Epoch 3, Loss: 16.8343, Val Loss: 16.5748\n",
      "Saving model at epoch 3 with loss 16.8343\n",
      "Epoch 4, Loss: 13.0499, Val Loss: 12.7379\n",
      "Saving model at epoch 4 with loss 13.0499\n",
      "Epoch 5, Loss: 10.3685, Val Loss: 10.0069\n",
      "Saving model at epoch 5 with loss 10.3685\n",
      "Epoch 6, Loss: 8.6661, Val Loss: 8.2669\n",
      "Saving model at epoch 6 with loss 8.6661\n",
      "Epoch 7, Loss: 7.6072, Val Loss: 7.1994\n",
      "Saving model at epoch 7 with loss 7.6072\n",
      "Epoch 8, Loss: 6.9171, Val Loss: 6.4844\n",
      "Saving model at epoch 8 with loss 6.9171\n",
      "Epoch 9, Loss: 6.3795, Val Loss: 5.9475\n",
      "Saving model at epoch 9 with loss 6.3795\n",
      "Epoch 10, Loss: 5.9520, Val Loss: 5.5378\n",
      "Saving model at epoch 10 with loss 5.9520\n",
      "Epoch 11, Loss: 5.6032, Val Loss: 5.1921\n",
      "Saving model at epoch 11 with loss 5.6032\n",
      "Epoch 12, Loss: 5.3052, Val Loss: 4.9168\n",
      "Saving model at epoch 12 with loss 5.3052\n",
      "Epoch 13, Loss: 5.0406, Val Loss: 4.6691\n",
      "Saving model at epoch 13 with loss 5.0406\n",
      "Epoch 14, Loss: 4.8039, Val Loss: 4.4571\n",
      "Saving model at epoch 14 with loss 4.8039\n",
      "Epoch 15, Loss: 4.5984, Val Loss: 4.2656\n",
      "Saving model at epoch 15 with loss 4.5984\n",
      "Epoch 16, Loss: 4.4123, Val Loss: 4.0940\n",
      "Saving model at epoch 16 with loss 4.4123\n",
      "Epoch 17, Loss: 4.2413, Val Loss: 3.9417\n",
      "Saving model at epoch 17 with loss 4.2413\n",
      "Epoch 18, Loss: 4.0882, Val Loss: 3.8010\n",
      "Saving model at epoch 18 with loss 4.0882\n",
      "Epoch 19, Loss: 3.9469, Val Loss: 3.6727\n",
      "Saving model at epoch 19 with loss 3.9469\n",
      "Epoch 20, Loss: 3.8155, Val Loss: 3.5555\n",
      "Saving model at epoch 20 with loss 3.8155\n",
      "Epoch 21, Loss: 3.6949, Val Loss: 3.4461\n",
      "Saving model at epoch 21 with loss 3.6949\n",
      "Epoch 22, Loss: 3.5829, Val Loss: 3.3440\n",
      "Saving model at epoch 22 with loss 3.5829\n",
      "Epoch 23, Loss: 3.4778, Val Loss: 3.2501\n",
      "Saving model at epoch 23 with loss 3.4778\n",
      "Epoch 24, Loss: 3.3787, Val Loss: 3.1602\n",
      "Saving model at epoch 24 with loss 3.3787\n",
      "Epoch 25, Loss: 3.2866, Val Loss: 3.0779\n",
      "Saving model at epoch 25 with loss 3.2866\n",
      "Epoch 26, Loss: 3.1995, Val Loss: 2.9985\n",
      "Saving model at epoch 26 with loss 3.1995\n",
      "Epoch 27, Loss: 3.1172, Val Loss: 2.9240\n",
      "Saving model at epoch 27 with loss 3.1172\n",
      "Epoch 28, Loss: 3.0400, Val Loss: 2.8540\n",
      "Saving model at epoch 28 with loss 3.0400\n",
      "Epoch 29, Loss: 2.9663, Val Loss: 2.7870\n",
      "Saving model at epoch 29 with loss 2.9663\n",
      "Epoch 30, Loss: 2.8963, Val Loss: 2.7234\n",
      "Saving model at epoch 30 with loss 2.8963\n",
      "Epoch 31, Loss: 2.8295, Val Loss: 2.6631\n",
      "Saving model at epoch 31 with loss 2.8295\n",
      "Epoch 32, Loss: 2.7668, Val Loss: 2.6056\n",
      "Saving model at epoch 32 with loss 2.7668\n",
      "Epoch 33, Loss: 2.7066, Val Loss: 2.5508\n",
      "Saving model at epoch 33 with loss 2.7066\n",
      "Epoch 34, Loss: 2.6492, Val Loss: 2.4982\n",
      "Saving model at epoch 34 with loss 2.6492\n",
      "Epoch 35, Loss: 2.5945, Val Loss: 2.4482\n",
      "Saving model at epoch 35 with loss 2.5945\n",
      "Epoch 36, Loss: 2.5419, Val Loss: 2.4002\n",
      "Saving model at epoch 36 with loss 2.5419\n",
      "Epoch 37, Loss: 2.4915, Val Loss: 2.3541\n",
      "Saving model at epoch 37 with loss 2.4915\n",
      "Epoch 38, Loss: 2.4435, Val Loss: 2.3099\n",
      "Saving model at epoch 38 with loss 2.4435\n",
      "Epoch 39, Loss: 2.3972, Val Loss: 2.2675\n",
      "Saving model at epoch 39 with loss 2.3972\n",
      "Epoch 40, Loss: 2.3529, Val Loss: 2.2268\n",
      "Saving model at epoch 40 with loss 2.3529\n",
      "Epoch 41, Loss: 2.3101, Val Loss: 2.1876\n",
      "Saving model at epoch 41 with loss 2.3101\n",
      "Epoch 42, Loss: 2.2692, Val Loss: 2.1498\n",
      "Saving model at epoch 42 with loss 2.2692\n",
      "Epoch 43, Loss: 2.2297, Val Loss: 2.1134\n",
      "Saving model at epoch 43 with loss 2.2297\n",
      "Epoch 44, Loss: 2.1917, Val Loss: 2.0783\n",
      "Saving model at epoch 44 with loss 2.1917\n",
      "Epoch 45, Loss: 2.1550, Val Loss: 2.0445\n",
      "Saving model at epoch 45 with loss 2.1550\n",
      "Epoch 46, Loss: 2.1198, Val Loss: 2.0119\n",
      "Saving model at epoch 46 with loss 2.1198\n",
      "Epoch 47, Loss: 2.0856, Val Loss: 1.9803\n",
      "Saving model at epoch 47 with loss 2.0856\n",
      "Epoch 48, Loss: 2.0528, Val Loss: 1.9498\n",
      "Saving model at epoch 48 with loss 2.0528\n",
      "Epoch 49, Loss: 2.0209, Val Loss: 1.9203\n",
      "Saving model at epoch 49 with loss 2.0209\n",
      "Epoch 50, Loss: 1.9902, Val Loss: 1.8918\n",
      "Saving model at epoch 50 with loss 1.9902\n",
      "Epoch 51, Loss: 1.9609, Val Loss: 1.8641\n",
      "Saving model at epoch 51 with loss 1.9609\n",
      "Epoch 52, Loss: 1.9327, Val Loss: 1.8374\n",
      "Saving model at epoch 52 with loss 1.9327\n",
      "Epoch 53, Loss: 1.9053, Val Loss: 1.8114\n",
      "Saving model at epoch 53 with loss 1.9053\n",
      "Epoch 54, Loss: 1.8788, Val Loss: 1.7863\n",
      "Saving model at epoch 54 with loss 1.8788\n",
      "Epoch 55, Loss: 1.8530, Val Loss: 1.7619\n",
      "Saving model at epoch 55 with loss 1.8530\n",
      "Epoch 56, Loss: 1.8281, Val Loss: 1.7383\n",
      "Saving model at epoch 56 with loss 1.8281\n",
      "Epoch 57, Loss: 1.8039, Val Loss: 1.7153\n",
      "Saving model at epoch 57 with loss 1.8039\n",
      "Epoch 58, Loss: 1.7804, Val Loss: 1.6930\n",
      "Saving model at epoch 58 with loss 1.7804\n",
      "Epoch 59, Loss: 1.7576, Val Loss: 1.6713\n",
      "Saving model at epoch 59 with loss 1.7576\n",
      "Epoch 60, Loss: 1.7354, Val Loss: 1.6503\n",
      "Saving model at epoch 60 with loss 1.7354\n",
      "Epoch 61, Loss: 1.7139, Val Loss: 1.6298\n",
      "Saving model at epoch 61 with loss 1.7139\n",
      "Epoch 62, Loss: 1.6930, Val Loss: 1.6098\n",
      "Saving model at epoch 62 with loss 1.6930\n",
      "Epoch 63, Loss: 1.6726, Val Loss: 1.5904\n",
      "Saving model at epoch 63 with loss 1.6726\n",
      "Epoch 64, Loss: 1.6528, Val Loss: 1.5716\n",
      "Saving model at epoch 64 with loss 1.6528\n",
      "Epoch 65, Loss: 1.6335, Val Loss: 1.5532\n",
      "Saving model at epoch 65 with loss 1.6335\n",
      "Epoch 66, Loss: 1.6148, Val Loss: 1.5353\n",
      "Saving model at epoch 66 with loss 1.6148\n",
      "Epoch 67, Loss: 1.5965, Val Loss: 1.5178\n",
      "Saving model at epoch 67 with loss 1.5965\n",
      "Epoch 68, Loss: 1.5786, Val Loss: 1.5008\n",
      "Saving model at epoch 68 with loss 1.5786\n",
      "Epoch 69, Loss: 1.5612, Val Loss: 1.4842\n",
      "Saving model at epoch 69 with loss 1.5612\n",
      "Epoch 70, Loss: 1.5443, Val Loss: 1.4681\n",
      "Saving model at epoch 70 with loss 1.5443\n",
      "Epoch 71, Loss: 1.5279, Val Loss: 1.4525\n",
      "Saving model at epoch 71 with loss 1.5279\n",
      "Epoch 72, Loss: 1.5119, Val Loss: 1.4372\n",
      "Saving model at epoch 72 with loss 1.5119\n",
      "Epoch 73, Loss: 1.4962, Val Loss: 1.4224\n",
      "Saving model at epoch 73 with loss 1.4962\n",
      "Epoch 74, Loss: 1.4810, Val Loss: 1.4078\n",
      "Saving model at epoch 74 with loss 1.4810\n",
      "Epoch 75, Loss: 1.4662, Val Loss: 1.3936\n",
      "Saving model at epoch 75 with loss 1.4662\n",
      "Epoch 76, Loss: 1.4517, Val Loss: 1.3798\n",
      "Saving model at epoch 76 with loss 1.4517\n",
      "Epoch 77, Loss: 1.4376, Val Loss: 1.3663\n",
      "Saving model at epoch 77 with loss 1.4376\n",
      "Epoch 78, Loss: 1.4239, Val Loss: 1.3531\n",
      "Saving model at epoch 78 with loss 1.4239\n",
      "Epoch 79, Loss: 1.4105, Val Loss: 1.3402\n",
      "Saving model at epoch 79 with loss 1.4105\n",
      "Epoch 80, Loss: 1.3974, Val Loss: 1.3277\n",
      "Saving model at epoch 80 with loss 1.3974\n",
      "Epoch 81, Loss: 1.3846, Val Loss: 1.3154\n",
      "Saving model at epoch 81 with loss 1.3846\n",
      "Epoch 82, Loss: 1.3721, Val Loss: 1.3033\n",
      "Saving model at epoch 82 with loss 1.3721\n",
      "Epoch 83, Loss: 1.3598, Val Loss: 1.2916\n",
      "Saving model at epoch 83 with loss 1.3598\n",
      "Epoch 84, Loss: 1.3479, Val Loss: 1.2801\n",
      "Saving model at epoch 84 with loss 1.3479\n",
      "Epoch 85, Loss: 1.3362, Val Loss: 1.2688\n",
      "Saving model at epoch 85 with loss 1.3362\n",
      "Epoch 86, Loss: 1.3247, Val Loss: 1.2578\n",
      "Saving model at epoch 86 with loss 1.3247\n",
      "Epoch 87, Loss: 1.3135, Val Loss: 1.2470\n",
      "Saving model at epoch 87 with loss 1.3135\n",
      "Epoch 88, Loss: 1.3026, Val Loss: 1.2365\n",
      "Saving model at epoch 88 with loss 1.3026\n",
      "Epoch 89, Loss: 1.2919, Val Loss: 1.2262\n",
      "Saving model at epoch 89 with loss 1.2919\n",
      "Epoch 90, Loss: 1.2814, Val Loss: 1.2161\n",
      "Saving model at epoch 90 with loss 1.2814\n",
      "Epoch 91, Loss: 1.2713, Val Loss: 1.2061\n",
      "Saving model at epoch 91 with loss 1.2713\n",
      "Epoch 92, Loss: 1.2613, Val Loss: 1.1964\n",
      "Saving model at epoch 92 with loss 1.2613\n",
      "Epoch 93, Loss: 1.2515, Val Loss: 1.1869\n",
      "Saving model at epoch 93 with loss 1.2515\n",
      "Epoch 94, Loss: 1.2420, Val Loss: 1.1776\n",
      "Saving model at epoch 94 with loss 1.2420\n",
      "Epoch 95, Loss: 1.2326, Val Loss: 1.1685\n",
      "Saving model at epoch 95 with loss 1.2326\n",
      "Epoch 96, Loss: 1.2234, Val Loss: 1.1596\n",
      "Saving model at epoch 96 with loss 1.2234\n",
      "Epoch 97, Loss: 1.2145, Val Loss: 1.1508\n",
      "Saving model at epoch 97 with loss 1.2145\n",
      "Epoch 98, Loss: 1.2057, Val Loss: 1.1422\n",
      "Saving model at epoch 98 with loss 1.2057\n",
      "Epoch 99, Loss: 1.1971, Val Loss: 1.1338\n",
      "Saving model at epoch 99 with loss 1.1971\n",
      "Epoch 100, Loss: 1.1886, Val Loss: 1.1256\n",
      "Saving model at epoch 100 with loss 1.1886\n",
      "Epoch 101, Loss: 1.1803, Val Loss: 1.1175\n",
      "Saving model at epoch 101 with loss 1.1803\n",
      "Epoch 102, Loss: 1.1722, Val Loss: 1.1096\n",
      "Saving model at epoch 102 with loss 1.1722\n",
      "Epoch 103, Loss: 1.1642, Val Loss: 1.1018\n",
      "Saving model at epoch 103 with loss 1.1642\n",
      "Epoch 104, Loss: 1.1564, Val Loss: 1.0941\n",
      "Saving model at epoch 104 with loss 1.1564\n",
      "Epoch 105, Loss: 1.1488, Val Loss: 1.0866\n",
      "Saving model at epoch 105 with loss 1.1488\n",
      "Epoch 106, Loss: 1.1413, Val Loss: 1.0793\n",
      "Saving model at epoch 106 with loss 1.1413\n",
      "Epoch 107, Loss: 1.1339, Val Loss: 1.0720\n",
      "Saving model at epoch 107 with loss 1.1339\n",
      "Epoch 108, Loss: 1.1266, Val Loss: 1.0650\n",
      "Saving model at epoch 108 with loss 1.1266\n",
      "Epoch 109, Loss: 1.1196, Val Loss: 1.0580\n",
      "Saving model at epoch 109 with loss 1.1196\n",
      "Epoch 110, Loss: 1.1126, Val Loss: 1.0512\n",
      "Saving model at epoch 110 with loss 1.1126\n",
      "Epoch 111, Loss: 1.1058, Val Loss: 1.0445\n",
      "Saving model at epoch 111 with loss 1.1058\n",
      "Epoch 112, Loss: 1.0990, Val Loss: 1.0378\n",
      "Saving model at epoch 112 with loss 1.0990\n",
      "Epoch 113, Loss: 1.0924, Val Loss: 1.0314\n",
      "Saving model at epoch 113 with loss 1.0924\n",
      "Epoch 114, Loss: 1.0859, Val Loss: 1.0250\n",
      "Saving model at epoch 114 with loss 1.0859\n",
      "Epoch 115, Loss: 1.0796, Val Loss: 1.0187\n",
      "Saving model at epoch 115 with loss 1.0796\n",
      "Epoch 116, Loss: 1.0733, Val Loss: 1.0126\n",
      "Saving model at epoch 116 with loss 1.0733\n",
      "Epoch 117, Loss: 1.0672, Val Loss: 1.0065\n",
      "Saving model at epoch 117 with loss 1.0672\n",
      "Epoch 118, Loss: 1.0612, Val Loss: 1.0006\n",
      "Saving model at epoch 118 with loss 1.0612\n",
      "Epoch 119, Loss: 1.0552, Val Loss: 0.9947\n",
      "Saving model at epoch 119 with loss 1.0552\n",
      "Epoch 120, Loss: 1.0494, Val Loss: 0.9889\n",
      "Saving model at epoch 120 with loss 1.0494\n",
      "Epoch 121, Loss: 1.0437, Val Loss: 0.9833\n",
      "Saving model at epoch 121 with loss 1.0437\n",
      "Epoch 122, Loss: 1.0381, Val Loss: 0.9777\n",
      "Saving model at epoch 122 with loss 1.0381\n",
      "Epoch 123, Loss: 1.0326, Val Loss: 0.9722\n",
      "Saving model at epoch 123 with loss 1.0326\n",
      "Epoch 124, Loss: 1.0271, Val Loss: 0.9669\n",
      "Saving model at epoch 124 with loss 1.0271\n",
      "Epoch 125, Loss: 1.0218, Val Loss: 0.9616\n",
      "Saving model at epoch 125 with loss 1.0218\n",
      "Epoch 126, Loss: 1.0165, Val Loss: 0.9564\n",
      "Saving model at epoch 126 with loss 1.0165\n",
      "Epoch 127, Loss: 1.0114, Val Loss: 0.9512\n",
      "Saving model at epoch 127 with loss 1.0114\n",
      "Epoch 128, Loss: 1.0063, Val Loss: 0.9462\n",
      "Saving model at epoch 128 with loss 1.0063\n",
      "Epoch 129, Loss: 1.0014, Val Loss: 0.9412\n",
      "Saving model at epoch 129 with loss 1.0014\n",
      "Epoch 130, Loss: 0.9965, Val Loss: 0.9364\n",
      "Saving model at epoch 130 with loss 0.9965\n",
      "Epoch 131, Loss: 0.9916, Val Loss: 0.9316\n",
      "Saving model at epoch 131 with loss 0.9916\n",
      "Epoch 132, Loss: 0.9869, Val Loss: 0.9268\n",
      "Saving model at epoch 132 with loss 0.9869\n",
      "Epoch 133, Loss: 0.9822, Val Loss: 0.9222\n",
      "Saving model at epoch 133 with loss 0.9822\n",
      "Epoch 134, Loss: 0.9777, Val Loss: 0.9176\n",
      "Saving model at epoch 134 with loss 0.9777\n",
      "Epoch 135, Loss: 0.9733, Val Loss: 0.9131\n",
      "Saving model at epoch 135 with loss 0.9733\n",
      "Epoch 136, Loss: 0.9688, Val Loss: 0.9087\n",
      "Saving model at epoch 136 with loss 0.9688\n",
      "Epoch 137, Loss: 0.9646, Val Loss: 0.9044\n",
      "Saving model at epoch 137 with loss 0.9646\n",
      "Epoch 138, Loss: 0.9603, Val Loss: 0.9001\n",
      "Saving model at epoch 138 with loss 0.9603\n",
      "Epoch 139, Loss: 0.9563, Val Loss: 0.8959\n",
      "Saving model at epoch 139 with loss 0.9563\n",
      "Epoch 140, Loss: 0.9521, Val Loss: 0.8918\n",
      "Saving model at epoch 140 with loss 0.9521\n",
      "Epoch 141, Loss: 0.9481, Val Loss: 0.8877\n",
      "Saving model at epoch 141 with loss 0.9481\n",
      "Epoch 142, Loss: 0.9442, Val Loss: 0.8837\n",
      "Saving model at epoch 142 with loss 0.9442\n",
      "Epoch 143, Loss: 0.9402, Val Loss: 0.8798\n",
      "Saving model at epoch 143 with loss 0.9402\n",
      "Epoch 144, Loss: 0.9365, Val Loss: 0.8759\n",
      "Saving model at epoch 144 with loss 0.9365\n",
      "Epoch 145, Loss: 0.9326, Val Loss: 0.8720\n",
      "Saving model at epoch 145 with loss 0.9326\n",
      "Epoch 146, Loss: 0.9289, Val Loss: 0.8682\n",
      "Saving model at epoch 146 with loss 0.9289\n",
      "Epoch 147, Loss: 0.9251, Val Loss: 0.8645\n",
      "Saving model at epoch 147 with loss 0.9251\n",
      "Epoch 148, Loss: 0.9216, Val Loss: 0.8608\n",
      "Saving model at epoch 148 with loss 0.9216\n",
      "Epoch 149, Loss: 0.9180, Val Loss: 0.8572\n",
      "Saving model at epoch 149 with loss 0.9180\n",
      "Epoch 150, Loss: 0.9144, Val Loss: 0.8536\n",
      "Saving model at epoch 150 with loss 0.9144\n",
      "Epoch 151, Loss: 0.9110, Val Loss: 0.8501\n",
      "Saving model at epoch 151 with loss 0.9110\n",
      "Epoch 152, Loss: 0.9075, Val Loss: 0.8466\n",
      "Saving model at epoch 152 with loss 0.9075\n",
      "Epoch 153, Loss: 0.9042, Val Loss: 0.8432\n",
      "Saving model at epoch 153 with loss 0.9042\n",
      "Epoch 154, Loss: 0.9007, Val Loss: 0.8398\n",
      "Saving model at epoch 154 with loss 0.9007\n",
      "Epoch 155, Loss: 0.8975, Val Loss: 0.8364\n",
      "Saving model at epoch 155 with loss 0.8975\n",
      "Epoch 156, Loss: 0.8942, Val Loss: 0.8332\n",
      "Saving model at epoch 156 with loss 0.8942\n",
      "Epoch 157, Loss: 0.8911, Val Loss: 0.8299\n",
      "Saving model at epoch 157 with loss 0.8911\n",
      "Epoch 158, Loss: 0.8878, Val Loss: 0.8267\n",
      "Saving model at epoch 158 with loss 0.8878\n",
      "Epoch 159, Loss: 0.8848, Val Loss: 0.8235\n",
      "Saving model at epoch 159 with loss 0.8848\n",
      "Epoch 160, Loss: 0.8818, Val Loss: 0.8204\n",
      "Saving model at epoch 160 with loss 0.8818\n",
      "Epoch 161, Loss: 0.8786, Val Loss: 0.8174\n",
      "Saving model at epoch 161 with loss 0.8786\n",
      "Epoch 162, Loss: 0.8757, Val Loss: 0.8143\n",
      "Saving model at epoch 162 with loss 0.8757\n",
      "Epoch 163, Loss: 0.8727, Val Loss: 0.8113\n",
      "Saving model at epoch 163 with loss 0.8727\n",
      "Epoch 164, Loss: 0.8698, Val Loss: 0.8083\n",
      "Saving model at epoch 164 with loss 0.8698\n",
      "Epoch 165, Loss: 0.8669, Val Loss: 0.8054\n",
      "Saving model at epoch 165 with loss 0.8669\n",
      "Epoch 166, Loss: 0.8641, Val Loss: 0.8025\n",
      "Saving model at epoch 166 with loss 0.8641\n",
      "Epoch 167, Loss: 0.8612, Val Loss: 0.7997\n",
      "Saving model at epoch 167 with loss 0.8612\n",
      "Epoch 168, Loss: 0.8585, Val Loss: 0.7969\n",
      "Saving model at epoch 168 with loss 0.8585\n",
      "Epoch 169, Loss: 0.8558, Val Loss: 0.7941\n",
      "Saving model at epoch 169 with loss 0.8558\n",
      "Epoch 170, Loss: 0.8530, Val Loss: 0.7914\n",
      "Saving model at epoch 170 with loss 0.8530\n",
      "Epoch 171, Loss: 0.8504, Val Loss: 0.7887\n",
      "Saving model at epoch 171 with loss 0.8504\n",
      "Epoch 172, Loss: 0.8477, Val Loss: 0.7860\n",
      "Saving model at epoch 172 with loss 0.8477\n",
      "Epoch 173, Loss: 0.8452, Val Loss: 0.7834\n",
      "Saving model at epoch 173 with loss 0.8452\n",
      "Epoch 174, Loss: 0.8425, Val Loss: 0.7808\n",
      "Saving model at epoch 174 with loss 0.8425\n",
      "Epoch 175, Loss: 0.8401, Val Loss: 0.7782\n",
      "Saving model at epoch 175 with loss 0.8401\n",
      "Epoch 176, Loss: 0.8375, Val Loss: 0.7757\n",
      "Saving model at epoch 176 with loss 0.8375\n",
      "Epoch 177, Loss: 0.8351, Val Loss: 0.7732\n",
      "Saving model at epoch 177 with loss 0.8351\n",
      "Epoch 178, Loss: 0.8325, Val Loss: 0.7707\n",
      "Saving model at epoch 178 with loss 0.8325\n",
      "Epoch 179, Loss: 0.8302, Val Loss: 0.7683\n",
      "Saving model at epoch 179 with loss 0.8302\n",
      "Epoch 180, Loss: 0.8279, Val Loss: 0.7659\n",
      "Saving model at epoch 180 with loss 0.8279\n",
      "Epoch 181, Loss: 0.8254, Val Loss: 0.7635\n",
      "Saving model at epoch 181 with loss 0.8254\n",
      "Epoch 182, Loss: 0.8232, Val Loss: 0.7611\n",
      "Saving model at epoch 182 with loss 0.8232\n",
      "Epoch 183, Loss: 0.8208, Val Loss: 0.7588\n",
      "Saving model at epoch 183 with loss 0.8208\n",
      "Epoch 184, Loss: 0.8186, Val Loss: 0.7564\n",
      "Saving model at epoch 184 with loss 0.8186\n",
      "Epoch 185, Loss: 0.8162, Val Loss: 0.7542\n",
      "Saving model at epoch 185 with loss 0.8162\n",
      "Epoch 186, Loss: 0.8141, Val Loss: 0.7519\n",
      "Saving model at epoch 186 with loss 0.8141\n",
      "Epoch 187, Loss: 0.8118, Val Loss: 0.7497\n",
      "Saving model at epoch 187 with loss 0.8118\n",
      "Epoch 188, Loss: 0.8097, Val Loss: 0.7475\n",
      "Saving model at epoch 188 with loss 0.8097\n",
      "Epoch 189, Loss: 0.8075, Val Loss: 0.7453\n",
      "Saving model at epoch 189 with loss 0.8075\n",
      "Epoch 190, Loss: 0.8054, Val Loss: 0.7431\n",
      "Saving model at epoch 190 with loss 0.8054\n",
      "Epoch 191, Loss: 0.8033, Val Loss: 0.7410\n",
      "Saving model at epoch 191 with loss 0.8033\n",
      "Epoch 192, Loss: 0.8012, Val Loss: 0.7389\n",
      "Saving model at epoch 192 with loss 0.8012\n",
      "Epoch 193, Loss: 0.7992, Val Loss: 0.7368\n",
      "Saving model at epoch 193 with loss 0.7992\n",
      "Epoch 194, Loss: 0.7971, Val Loss: 0.7347\n",
      "Saving model at epoch 194 with loss 0.7971\n",
      "Epoch 195, Loss: 0.7951, Val Loss: 0.7327\n",
      "Saving model at epoch 195 with loss 0.7951\n",
      "Epoch 196, Loss: 0.7931, Val Loss: 0.7307\n",
      "Saving model at epoch 196 with loss 0.7931\n",
      "Epoch 197, Loss: 0.7911, Val Loss: 0.7287\n",
      "Saving model at epoch 197 with loss 0.7911\n",
      "Epoch 198, Loss: 0.7891, Val Loss: 0.7267\n",
      "Saving model at epoch 198 with loss 0.7891\n",
      "Epoch 199, Loss: 0.7873, Val Loss: 0.7247\n",
      "Saving model at epoch 199 with loss 0.7873\n",
      "Epoch 200, Loss: 0.7854, Val Loss: 0.7228\n",
      "Saving model at epoch 200 with loss 0.7854\n",
      "Epoch 201, Loss: 0.7835, Val Loss: 0.7209\n",
      "Saving model at epoch 201 with loss 0.7835\n",
      "Epoch 202, Loss: 0.7816, Val Loss: 0.7190\n",
      "Saving model at epoch 202 with loss 0.7816\n",
      "Epoch 203, Loss: 0.7797, Val Loss: 0.7171\n",
      "Saving model at epoch 203 with loss 0.7797\n",
      "Epoch 204, Loss: 0.7780, Val Loss: 0.7152\n",
      "Saving model at epoch 204 with loss 0.7780\n",
      "Epoch 205, Loss: 0.7761, Val Loss: 0.7134\n",
      "Saving model at epoch 205 with loss 0.7761\n",
      "Epoch 206, Loss: 0.7744, Val Loss: 0.7116\n",
      "Saving model at epoch 206 with loss 0.7744\n",
      "Epoch 207, Loss: 0.7725, Val Loss: 0.7098\n",
      "Saving model at epoch 207 with loss 0.7725\n",
      "Epoch 208, Loss: 0.7708, Val Loss: 0.7080\n",
      "Saving model at epoch 208 with loss 0.7708\n",
      "Epoch 209, Loss: 0.7690, Val Loss: 0.7062\n",
      "Saving model at epoch 209 with loss 0.7690\n",
      "Epoch 210, Loss: 0.7674, Val Loss: 0.7045\n",
      "Saving model at epoch 210 with loss 0.7674\n",
      "Epoch 211, Loss: 0.7656, Val Loss: 0.7027\n",
      "Saving model at epoch 211 with loss 0.7656\n",
      "Epoch 212, Loss: 0.7640, Val Loss: 0.7010\n",
      "Saving model at epoch 212 with loss 0.7640\n",
      "Epoch 213, Loss: 0.7622, Val Loss: 0.6993\n",
      "Saving model at epoch 213 with loss 0.7622\n",
      "Epoch 214, Loss: 0.7606, Val Loss: 0.6976\n",
      "Saving model at epoch 214 with loss 0.7606\n",
      "Epoch 215, Loss: 0.7591, Val Loss: 0.6960\n",
      "Saving model at epoch 215 with loss 0.7591\n",
      "Epoch 216, Loss: 0.7574, Val Loss: 0.6943\n",
      "Saving model at epoch 216 with loss 0.7574\n",
      "Epoch 217, Loss: 0.7558, Val Loss: 0.6927\n",
      "Saving model at epoch 217 with loss 0.7558\n",
      "Epoch 218, Loss: 0.7542, Val Loss: 0.6911\n",
      "Saving model at epoch 218 with loss 0.7542\n",
      "Epoch 219, Loss: 0.7527, Val Loss: 0.6895\n",
      "Saving model at epoch 219 with loss 0.7527\n",
      "Epoch 220, Loss: 0.7510, Val Loss: 0.6879\n",
      "Saving model at epoch 220 with loss 0.7510\n",
      "Epoch 221, Loss: 0.7496, Val Loss: 0.6863\n",
      "Saving model at epoch 221 with loss 0.7496\n",
      "Epoch 222, Loss: 0.7480, Val Loss: 0.6847\n",
      "Saving model at epoch 222 with loss 0.7480\n",
      "Epoch 223, Loss: 0.7465, Val Loss: 0.6832\n",
      "Saving model at epoch 223 with loss 0.7465\n",
      "Epoch 224, Loss: 0.7450, Val Loss: 0.6817\n",
      "Saving model at epoch 224 with loss 0.7450\n",
      "Epoch 225, Loss: 0.7435, Val Loss: 0.6801\n",
      "Saving model at epoch 225 with loss 0.7435\n",
      "Epoch 226, Loss: 0.7420, Val Loss: 0.6787\n",
      "Saving model at epoch 226 with loss 0.7420\n",
      "Epoch 227, Loss: 0.7406, Val Loss: 0.6772\n",
      "Saving model at epoch 227 with loss 0.7406\n",
      "Epoch 228, Loss: 0.7392, Val Loss: 0.6757\n",
      "Saving model at epoch 228 with loss 0.7392\n",
      "Epoch 229, Loss: 0.7377, Val Loss: 0.6742\n",
      "Saving model at epoch 229 with loss 0.7377\n",
      "Epoch 230, Loss: 0.7364, Val Loss: 0.6728\n",
      "Saving model at epoch 230 with loss 0.7364\n",
      "Epoch 231, Loss: 0.7349, Val Loss: 0.6714\n",
      "Saving model at epoch 231 with loss 0.7349\n",
      "Epoch 232, Loss: 0.7336, Val Loss: 0.6699\n",
      "Saving model at epoch 232 with loss 0.7336\n",
      "Epoch 233, Loss: 0.7322, Val Loss: 0.6685\n",
      "Saving model at epoch 233 with loss 0.7322\n",
      "Epoch 234, Loss: 0.7309, Val Loss: 0.6671\n",
      "Saving model at epoch 234 with loss 0.7309\n",
      "Epoch 235, Loss: 0.7295, Val Loss: 0.6658\n",
      "Saving model at epoch 235 with loss 0.7295\n",
      "Epoch 236, Loss: 0.7282, Val Loss: 0.6644\n",
      "Saving model at epoch 236 with loss 0.7282\n",
      "Epoch 237, Loss: 0.7268, Val Loss: 0.6631\n",
      "Saving model at epoch 237 with loss 0.7268\n",
      "Epoch 238, Loss: 0.7255, Val Loss: 0.6617\n",
      "Saving model at epoch 238 with loss 0.7255\n",
      "Epoch 239, Loss: 0.7242, Val Loss: 0.6604\n",
      "Saving model at epoch 239 with loss 0.7242\n",
      "Epoch 240, Loss: 0.7230, Val Loss: 0.6591\n",
      "Saving model at epoch 240 with loss 0.7230\n",
      "Epoch 241, Loss: 0.7217, Val Loss: 0.6578\n",
      "Saving model at epoch 241 with loss 0.7217\n",
      "Epoch 242, Loss: 0.7204, Val Loss: 0.6565\n",
      "Saving model at epoch 242 with loss 0.7204\n",
      "Epoch 243, Loss: 0.7192, Val Loss: 0.6552\n",
      "Saving model at epoch 243 with loss 0.7192\n",
      "Epoch 244, Loss: 0.7180, Val Loss: 0.6540\n",
      "Saving model at epoch 244 with loss 0.7180\n",
      "Epoch 245, Loss: 0.7168, Val Loss: 0.6527\n",
      "Saving model at epoch 245 with loss 0.7168\n",
      "Epoch 246, Loss: 0.7155, Val Loss: 0.6515\n",
      "Saving model at epoch 246 with loss 0.7155\n",
      "Epoch 247, Loss: 0.7144, Val Loss: 0.6502\n",
      "Saving model at epoch 247 with loss 0.7144\n",
      "Epoch 248, Loss: 0.7131, Val Loss: 0.6490\n",
      "Saving model at epoch 248 with loss 0.7131\n",
      "Epoch 249, Loss: 0.7120, Val Loss: 0.6478\n",
      "Saving model at epoch 249 with loss 0.7120\n",
      "Epoch 250, Loss: 0.7108, Val Loss: 0.6466\n",
      "Saving model at epoch 250 with loss 0.7108\n",
      "Epoch 251, Loss: 0.7097, Val Loss: 0.6454\n",
      "Saving model at epoch 251 with loss 0.7097\n",
      "Epoch 252, Loss: 0.7086, Val Loss: 0.6443\n",
      "Saving model at epoch 252 with loss 0.7086\n",
      "Epoch 253, Loss: 0.7074, Val Loss: 0.6431\n",
      "Saving model at epoch 253 with loss 0.7074\n",
      "Epoch 254, Loss: 0.7063, Val Loss: 0.6420\n",
      "Saving model at epoch 254 with loss 0.7063\n",
      "Epoch 255, Loss: 0.7052, Val Loss: 0.6408\n",
      "Saving model at epoch 255 with loss 0.7052\n",
      "Epoch 256, Loss: 0.7041, Val Loss: 0.6397\n",
      "Saving model at epoch 256 with loss 0.7041\n",
      "Epoch 257, Loss: 0.7030, Val Loss: 0.6386\n",
      "Saving model at epoch 257 with loss 0.7030\n",
      "Epoch 258, Loss: 0.7019, Val Loss: 0.6375\n",
      "Saving model at epoch 258 with loss 0.7019\n",
      "Epoch 259, Loss: 0.7008, Val Loss: 0.6364\n",
      "Saving model at epoch 259 with loss 0.7008\n",
      "Epoch 260, Loss: 0.6998, Val Loss: 0.6353\n",
      "Saving model at epoch 260 with loss 0.6998\n",
      "Epoch 261, Loss: 0.6987, Val Loss: 0.6342\n",
      "Saving model at epoch 261 with loss 0.6987\n",
      "Epoch 262, Loss: 0.6977, Val Loss: 0.6331\n",
      "Saving model at epoch 262 with loss 0.6977\n",
      "Epoch 263, Loss: 0.6966, Val Loss: 0.6321\n",
      "Saving model at epoch 263 with loss 0.6966\n",
      "Epoch 264, Loss: 0.6956, Val Loss: 0.6310\n",
      "Saving model at epoch 264 with loss 0.6956\n",
      "Epoch 265, Loss: 0.6947, Val Loss: 0.6300\n",
      "Saving model at epoch 265 with loss 0.6947\n",
      "Epoch 266, Loss: 0.6936, Val Loss: 0.6289\n",
      "Saving model at epoch 266 with loss 0.6936\n",
      "Epoch 267, Loss: 0.6926, Val Loss: 0.6279\n",
      "Saving model at epoch 267 with loss 0.6926\n",
      "Epoch 268, Loss: 0.6916, Val Loss: 0.6269\n",
      "Saving model at epoch 268 with loss 0.6916\n",
      "Epoch 269, Loss: 0.6907, Val Loss: 0.6259\n",
      "Saving model at epoch 269 with loss 0.6907\n",
      "Epoch 270, Loss: 0.6897, Val Loss: 0.6249\n",
      "Saving model at epoch 270 with loss 0.6897\n",
      "Epoch 271, Loss: 0.6887, Val Loss: 0.6239\n",
      "Saving model at epoch 271 with loss 0.6887\n",
      "Epoch 272, Loss: 0.6877, Val Loss: 0.6229\n",
      "Saving model at epoch 272 with loss 0.6877\n",
      "Epoch 273, Loss: 0.6868, Val Loss: 0.6219\n",
      "Saving model at epoch 273 with loss 0.6868\n",
      "Epoch 274, Loss: 0.6859, Val Loss: 0.6210\n",
      "Saving model at epoch 274 with loss 0.6859\n",
      "Epoch 275, Loss: 0.6850, Val Loss: 0.6200\n",
      "Saving model at epoch 275 with loss 0.6850\n",
      "Epoch 276, Loss: 0.6841, Val Loss: 0.6191\n",
      "Saving model at epoch 276 with loss 0.6841\n",
      "Epoch 277, Loss: 0.6831, Val Loss: 0.6181\n",
      "Saving model at epoch 277 with loss 0.6831\n",
      "Epoch 278, Loss: 0.6823, Val Loss: 0.6172\n",
      "Saving model at epoch 278 with loss 0.6823\n",
      "Epoch 279, Loss: 0.6813, Val Loss: 0.6163\n",
      "Saving model at epoch 279 with loss 0.6813\n",
      "Epoch 280, Loss: 0.6805, Val Loss: 0.6154\n",
      "Saving model at epoch 280 with loss 0.6805\n",
      "Epoch 281, Loss: 0.6796, Val Loss: 0.6145\n",
      "Saving model at epoch 281 with loss 0.6796\n",
      "Epoch 282, Loss: 0.6787, Val Loss: 0.6136\n",
      "Saving model at epoch 282 with loss 0.6787\n",
      "Epoch 283, Loss: 0.6778, Val Loss: 0.6127\n",
      "Saving model at epoch 283 with loss 0.6778\n",
      "Epoch 284, Loss: 0.6770, Val Loss: 0.6118\n",
      "Saving model at epoch 284 with loss 0.6770\n",
      "Epoch 285, Loss: 0.6762, Val Loss: 0.6109\n",
      "Saving model at epoch 285 with loss 0.6762\n",
      "Epoch 286, Loss: 0.6753, Val Loss: 0.6100\n",
      "Saving model at epoch 286 with loss 0.6753\n",
      "Epoch 287, Loss: 0.6745, Val Loss: 0.6092\n",
      "Saving model at epoch 287 with loss 0.6745\n",
      "Epoch 288, Loss: 0.6737, Val Loss: 0.6083\n",
      "Saving model at epoch 288 with loss 0.6737\n",
      "Epoch 289, Loss: 0.6729, Val Loss: 0.6075\n",
      "Saving model at epoch 289 with loss 0.6729\n",
      "Epoch 290, Loss: 0.6720, Val Loss: 0.6066\n",
      "Saving model at epoch 290 with loss 0.6720\n",
      "Epoch 291, Loss: 0.6713, Val Loss: 0.6058\n",
      "Saving model at epoch 291 with loss 0.6713\n",
      "Epoch 292, Loss: 0.6704, Val Loss: 0.6050\n",
      "Saving model at epoch 292 with loss 0.6704\n",
      "Epoch 293, Loss: 0.6697, Val Loss: 0.6041\n",
      "Saving model at epoch 293 with loss 0.6697\n",
      "Epoch 294, Loss: 0.6688, Val Loss: 0.6033\n",
      "Saving model at epoch 294 with loss 0.6688\n",
      "Epoch 295, Loss: 0.6681, Val Loss: 0.6025\n",
      "Saving model at epoch 295 with loss 0.6681\n",
      "Epoch 296, Loss: 0.6674, Val Loss: 0.6017\n",
      "Saving model at epoch 296 with loss 0.6674\n",
      "Epoch 297, Loss: 0.6666, Val Loss: 0.6010\n",
      "Saving model at epoch 297 with loss 0.6666\n",
      "Epoch 298, Loss: 0.6659, Val Loss: 0.6002\n",
      "Saving model at epoch 298 with loss 0.6659\n",
      "Epoch 299, Loss: 0.6651, Val Loss: 0.5994\n",
      "Saving model at epoch 299 with loss 0.6651\n",
      "Epoch 300, Loss: 0.6644, Val Loss: 0.5986\n",
      "Saving model at epoch 300 with loss 0.6644\n",
      "Epoch 301, Loss: 0.6636, Val Loss: 0.5979\n",
      "Saving model at epoch 301 with loss 0.6636\n",
      "Epoch 302, Loss: 0.6629, Val Loss: 0.5971\n",
      "Saving model at epoch 302 with loss 0.6629\n",
      "Epoch 303, Loss: 0.6621, Val Loss: 0.5964\n",
      "Saving model at epoch 303 with loss 0.6621\n",
      "Epoch 304, Loss: 0.6614, Val Loss: 0.5956\n",
      "Saving model at epoch 304 with loss 0.6614\n",
      "Epoch 305, Loss: 0.6607, Val Loss: 0.5949\n",
      "Saving model at epoch 305 with loss 0.6607\n",
      "Epoch 306, Loss: 0.6600, Val Loss: 0.5941\n",
      "Saving model at epoch 306 with loss 0.6600\n",
      "Epoch 307, Loss: 0.6593, Val Loss: 0.5934\n",
      "Saving model at epoch 307 with loss 0.6593\n",
      "Epoch 308, Loss: 0.6586, Val Loss: 0.5927\n",
      "Saving model at epoch 308 with loss 0.6586\n",
      "Epoch 309, Loss: 0.6579, Val Loss: 0.5920\n",
      "Saving model at epoch 309 with loss 0.6579\n",
      "Epoch 310, Loss: 0.6572, Val Loss: 0.5912\n",
      "Saving model at epoch 310 with loss 0.6572\n",
      "Epoch 311, Loss: 0.6566, Val Loss: 0.5905\n",
      "Saving model at epoch 311 with loss 0.6566\n",
      "Epoch 312, Loss: 0.6558, Val Loss: 0.5898\n",
      "Saving model at epoch 312 with loss 0.6558\n",
      "Epoch 313, Loss: 0.6552, Val Loss: 0.5891\n",
      "Saving model at epoch 313 with loss 0.6552\n",
      "Epoch 314, Loss: 0.6546, Val Loss: 0.5885\n",
      "Saving model at epoch 314 with loss 0.6546\n",
      "Epoch 315, Loss: 0.6539, Val Loss: 0.5878\n",
      "Saving model at epoch 315 with loss 0.6539\n",
      "Epoch 316, Loss: 0.6533, Val Loss: 0.5871\n",
      "Saving model at epoch 316 with loss 0.6533\n",
      "Epoch 317, Loss: 0.6526, Val Loss: 0.5864\n",
      "Saving model at epoch 317 with loss 0.6526\n",
      "Epoch 318, Loss: 0.6520, Val Loss: 0.5857\n",
      "Saving model at epoch 318 with loss 0.6520\n",
      "Epoch 319, Loss: 0.6513, Val Loss: 0.5851\n",
      "Saving model at epoch 319 with loss 0.6513\n",
      "Epoch 320, Loss: 0.6507, Val Loss: 0.5844\n",
      "Saving model at epoch 320 with loss 0.6507\n",
      "Epoch 321, Loss: 0.6500, Val Loss: 0.5837\n",
      "Saving model at epoch 321 with loss 0.6500\n",
      "Epoch 322, Loss: 0.6494, Val Loss: 0.5831\n",
      "Saving model at epoch 322 with loss 0.6494\n",
      "Epoch 323, Loss: 0.6488, Val Loss: 0.5825\n",
      "Saving model at epoch 323 with loss 0.6488\n",
      "Epoch 324, Loss: 0.6482, Val Loss: 0.5818\n",
      "Saving model at epoch 324 with loss 0.6482\n",
      "Epoch 325, Loss: 0.6476, Val Loss: 0.5812\n",
      "Saving model at epoch 325 with loss 0.6476\n",
      "Epoch 326, Loss: 0.6470, Val Loss: 0.5805\n",
      "Saving model at epoch 326 with loss 0.6470\n",
      "Epoch 327, Loss: 0.6464, Val Loss: 0.5799\n",
      "Saving model at epoch 327 with loss 0.6464\n",
      "Epoch 328, Loss: 0.6458, Val Loss: 0.5793\n",
      "Saving model at epoch 328 with loss 0.6458\n",
      "Epoch 329, Loss: 0.6452, Val Loss: 0.5787\n",
      "Saving model at epoch 329 with loss 0.6452\n",
      "Epoch 330, Loss: 0.6446, Val Loss: 0.5780\n",
      "Saving model at epoch 330 with loss 0.6446\n",
      "Epoch 331, Loss: 0.6440, Val Loss: 0.5774\n",
      "Saving model at epoch 331 with loss 0.6440\n",
      "Epoch 332, Loss: 0.6434, Val Loss: 0.5768\n",
      "Saving model at epoch 332 with loss 0.6434\n",
      "Epoch 333, Loss: 0.6429, Val Loss: 0.5762\n",
      "Saving model at epoch 333 with loss 0.6429\n",
      "Epoch 334, Loss: 0.6423, Val Loss: 0.5756\n",
      "Saving model at epoch 334 with loss 0.6423\n",
      "Epoch 335, Loss: 0.6417, Val Loss: 0.5750\n",
      "Saving model at epoch 335 with loss 0.6417\n",
      "Epoch 336, Loss: 0.6412, Val Loss: 0.5745\n",
      "Saving model at epoch 336 with loss 0.6412\n",
      "Epoch 337, Loss: 0.6406, Val Loss: 0.5739\n",
      "Saving model at epoch 337 with loss 0.6406\n",
      "Epoch 338, Loss: 0.6401, Val Loss: 0.5733\n",
      "Saving model at epoch 338 with loss 0.6401\n",
      "Epoch 339, Loss: 0.6395, Val Loss: 0.5727\n",
      "Saving model at epoch 339 with loss 0.6395\n",
      "Epoch 340, Loss: 0.6390, Val Loss: 0.5722\n",
      "Saving model at epoch 340 with loss 0.6390\n",
      "Epoch 341, Loss: 0.6384, Val Loss: 0.5716\n",
      "Saving model at epoch 341 with loss 0.6384\n",
      "Epoch 342, Loss: 0.6379, Val Loss: 0.5710\n",
      "Saving model at epoch 342 with loss 0.6379\n",
      "Epoch 343, Loss: 0.6373, Val Loss: 0.5705\n",
      "Saving model at epoch 343 with loss 0.6373\n",
      "Epoch 344, Loss: 0.6368, Val Loss: 0.5699\n",
      "Saving model at epoch 344 with loss 0.6368\n",
      "Epoch 345, Loss: 0.6363, Val Loss: 0.5694\n",
      "Saving model at epoch 345 with loss 0.6363\n",
      "Epoch 346, Loss: 0.6358, Val Loss: 0.5688\n",
      "Saving model at epoch 346 with loss 0.6358\n",
      "Epoch 347, Loss: 0.6353, Val Loss: 0.5683\n",
      "Saving model at epoch 347 with loss 0.6353\n",
      "Epoch 348, Loss: 0.6347, Val Loss: 0.5678\n",
      "Saving model at epoch 348 with loss 0.6347\n",
      "Epoch 349, Loss: 0.6343, Val Loss: 0.5672\n",
      "Saving model at epoch 349 with loss 0.6343\n",
      "Epoch 350, Loss: 0.6337, Val Loss: 0.5667\n",
      "Saving model at epoch 350 with loss 0.6337\n",
      "Epoch 351, Loss: 0.6332, Val Loss: 0.5662\n",
      "Saving model at epoch 351 with loss 0.6332\n",
      "Epoch 352, Loss: 0.6327, Val Loss: 0.5657\n",
      "Saving model at epoch 352 with loss 0.6327\n",
      "Epoch 353, Loss: 0.6323, Val Loss: 0.5651\n",
      "Saving model at epoch 353 with loss 0.6323\n",
      "Epoch 354, Loss: 0.6317, Val Loss: 0.5646\n",
      "Saving model at epoch 354 with loss 0.6317\n",
      "Epoch 355, Loss: 0.6313, Val Loss: 0.5641\n",
      "Saving model at epoch 355 with loss 0.6313\n",
      "Epoch 356, Loss: 0.6308, Val Loss: 0.5636\n",
      "Saving model at epoch 356 with loss 0.6308\n",
      "Epoch 357, Loss: 0.6303, Val Loss: 0.5631\n",
      "Saving model at epoch 357 with loss 0.6303\n",
      "Epoch 358, Loss: 0.6298, Val Loss: 0.5626\n",
      "Saving model at epoch 358 with loss 0.6298\n",
      "Epoch 359, Loss: 0.6293, Val Loss: 0.5621\n",
      "Saving model at epoch 359 with loss 0.6293\n",
      "Epoch 360, Loss: 0.6289, Val Loss: 0.5616\n",
      "Saving model at epoch 360 with loss 0.6289\n",
      "Epoch 361, Loss: 0.6284, Val Loss: 0.5611\n",
      "Saving model at epoch 361 with loss 0.6284\n",
      "Epoch 362, Loss: 0.6280, Val Loss: 0.5606\n",
      "Saving model at epoch 362 with loss 0.6280\n",
      "Epoch 363, Loss: 0.6275, Val Loss: 0.5602\n",
      "Saving model at epoch 363 with loss 0.6275\n",
      "Epoch 364, Loss: 0.6270, Val Loss: 0.5597\n",
      "Saving model at epoch 364 with loss 0.6270\n",
      "Epoch 365, Loss: 0.6265, Val Loss: 0.5592\n",
      "Saving model at epoch 365 with loss 0.6265\n",
      "Epoch 366, Loss: 0.6261, Val Loss: 0.5587\n",
      "Saving model at epoch 366 with loss 0.6261\n",
      "Epoch 367, Loss: 0.6257, Val Loss: 0.5583\n",
      "Saving model at epoch 367 with loss 0.6257\n",
      "Epoch 368, Loss: 0.6252, Val Loss: 0.5578\n",
      "Saving model at epoch 368 with loss 0.6252\n",
      "Epoch 369, Loss: 0.6248, Val Loss: 0.5573\n",
      "Saving model at epoch 369 with loss 0.6248\n",
      "Epoch 370, Loss: 0.6243, Val Loss: 0.5569\n",
      "Saving model at epoch 370 with loss 0.6243\n",
      "Epoch 371, Loss: 0.6239, Val Loss: 0.5564\n",
      "Saving model at epoch 371 with loss 0.6239\n",
      "Epoch 372, Loss: 0.6235, Val Loss: 0.5559\n",
      "Saving model at epoch 372 with loss 0.6235\n",
      "Epoch 373, Loss: 0.6231, Val Loss: 0.5555\n",
      "Saving model at epoch 373 with loss 0.6231\n",
      "Epoch 374, Loss: 0.6226, Val Loss: 0.5550\n",
      "Saving model at epoch 374 with loss 0.6226\n",
      "Epoch 375, Loss: 0.6222, Val Loss: 0.5546\n",
      "Saving model at epoch 375 with loss 0.6222\n",
      "Epoch 376, Loss: 0.6217, Val Loss: 0.5541\n",
      "Saving model at epoch 376 with loss 0.6217\n",
      "Epoch 377, Loss: 0.6213, Val Loss: 0.5537\n",
      "Saving model at epoch 377 with loss 0.6213\n",
      "Epoch 378, Loss: 0.6209, Val Loss: 0.5533\n",
      "Saving model at epoch 378 with loss 0.6209\n",
      "Epoch 379, Loss: 0.6205, Val Loss: 0.5528\n",
      "Saving model at epoch 379 with loss 0.6205\n",
      "Epoch 380, Loss: 0.6201, Val Loss: 0.5524\n",
      "Saving model at epoch 380 with loss 0.6201\n",
      "Epoch 381, Loss: 0.6197, Val Loss: 0.5520\n",
      "Saving model at epoch 381 with loss 0.6197\n",
      "Epoch 382, Loss: 0.6193, Val Loss: 0.5515\n",
      "Saving model at epoch 382 with loss 0.6193\n",
      "Epoch 383, Loss: 0.6189, Val Loss: 0.5511\n",
      "Saving model at epoch 383 with loss 0.6189\n",
      "Epoch 384, Loss: 0.6185, Val Loss: 0.5507\n",
      "Saving model at epoch 384 with loss 0.6185\n",
      "Epoch 385, Loss: 0.6180, Val Loss: 0.5503\n",
      "Saving model at epoch 385 with loss 0.6180\n",
      "Epoch 386, Loss: 0.6177, Val Loss: 0.5498\n",
      "Saving model at epoch 386 with loss 0.6177\n",
      "Epoch 387, Loss: 0.6172, Val Loss: 0.5494\n",
      "Saving model at epoch 387 with loss 0.6172\n",
      "Epoch 388, Loss: 0.6169, Val Loss: 0.5490\n",
      "Saving model at epoch 388 with loss 0.6169\n",
      "Epoch 389, Loss: 0.6165, Val Loss: 0.5486\n",
      "Saving model at epoch 389 with loss 0.6165\n",
      "Epoch 390, Loss: 0.6161, Val Loss: 0.5482\n",
      "Saving model at epoch 390 with loss 0.6161\n",
      "Epoch 391, Loss: 0.6157, Val Loss: 0.5478\n",
      "Saving model at epoch 391 with loss 0.6157\n",
      "Epoch 392, Loss: 0.6153, Val Loss: 0.5474\n",
      "Saving model at epoch 392 with loss 0.6153\n",
      "Epoch 393, Loss: 0.6150, Val Loss: 0.5470\n",
      "Saving model at epoch 393 with loss 0.6150\n",
      "Epoch 394, Loss: 0.6146, Val Loss: 0.5466\n",
      "Saving model at epoch 394 with loss 0.6146\n",
      "Epoch 395, Loss: 0.6142, Val Loss: 0.5462\n",
      "Saving model at epoch 395 with loss 0.6142\n",
      "Epoch 396, Loss: 0.6138, Val Loss: 0.5458\n",
      "Saving model at epoch 396 with loss 0.6138\n",
      "Epoch 397, Loss: 0.6135, Val Loss: 0.5454\n",
      "Saving model at epoch 397 with loss 0.6135\n",
      "Epoch 398, Loss: 0.6131, Val Loss: 0.5450\n",
      "Saving model at epoch 398 with loss 0.6131\n",
      "Epoch 399, Loss: 0.6127, Val Loss: 0.5446\n",
      "Saving model at epoch 399 with loss 0.6127\n",
      "Epoch 400, Loss: 0.6124, Val Loss: 0.5442\n",
      "Saving model at epoch 400 with loss 0.6124\n",
      "Epoch 401, Loss: 0.6120, Val Loss: 0.5439\n",
      "Saving model at epoch 401 with loss 0.6120\n",
      "Epoch 402, Loss: 0.6116, Val Loss: 0.5435\n",
      "Saving model at epoch 402 with loss 0.6116\n",
      "Epoch 403, Loss: 0.6112, Val Loss: 0.5431\n",
      "Saving model at epoch 403 with loss 0.6112\n",
      "Epoch 404, Loss: 0.6109, Val Loss: 0.5427\n",
      "Saving model at epoch 404 with loss 0.6109\n",
      "Epoch 405, Loss: 0.6105, Val Loss: 0.5423\n",
      "Saving model at epoch 405 with loss 0.6105\n",
      "Epoch 406, Loss: 0.6102, Val Loss: 0.5420\n",
      "Saving model at epoch 406 with loss 0.6102\n",
      "Epoch 407, Loss: 0.6098, Val Loss: 0.5416\n",
      "Saving model at epoch 407 with loss 0.6098\n",
      "Epoch 408, Loss: 0.6095, Val Loss: 0.5412\n",
      "Saving model at epoch 408 with loss 0.6095\n",
      "Epoch 409, Loss: 0.6091, Val Loss: 0.5409\n",
      "Saving model at epoch 409 with loss 0.6091\n",
      "Epoch 410, Loss: 0.6088, Val Loss: 0.5405\n",
      "Saving model at epoch 410 with loss 0.6088\n",
      "Epoch 411, Loss: 0.6085, Val Loss: 0.5401\n",
      "Saving model at epoch 411 with loss 0.6085\n",
      "Epoch 412, Loss: 0.6081, Val Loss: 0.5398\n",
      "Saving model at epoch 412 with loss 0.6081\n",
      "Epoch 413, Loss: 0.6078, Val Loss: 0.5394\n",
      "Saving model at epoch 413 with loss 0.6078\n",
      "Epoch 414, Loss: 0.6074, Val Loss: 0.5391\n",
      "Saving model at epoch 414 with loss 0.6074\n",
      "Epoch 415, Loss: 0.6071, Val Loss: 0.5387\n",
      "Saving model at epoch 415 with loss 0.6071\n",
      "Epoch 416, Loss: 0.6067, Val Loss: 0.5383\n",
      "Saving model at epoch 416 with loss 0.6067\n",
      "Epoch 417, Loss: 0.6064, Val Loss: 0.5380\n",
      "Saving model at epoch 417 with loss 0.6064\n",
      "Epoch 418, Loss: 0.6061, Val Loss: 0.5376\n",
      "Saving model at epoch 418 with loss 0.6061\n",
      "Epoch 419, Loss: 0.6058, Val Loss: 0.5373\n",
      "Saving model at epoch 419 with loss 0.6058\n",
      "Epoch 420, Loss: 0.6054, Val Loss: 0.5370\n",
      "Saving model at epoch 420 with loss 0.6054\n",
      "Epoch 421, Loss: 0.6051, Val Loss: 0.5366\n",
      "Saving model at epoch 421 with loss 0.6051\n",
      "Epoch 422, Loss: 0.6048, Val Loss: 0.5363\n",
      "Saving model at epoch 422 with loss 0.6048\n",
      "Epoch 423, Loss: 0.6045, Val Loss: 0.5359\n",
      "Saving model at epoch 423 with loss 0.6045\n",
      "Epoch 424, Loss: 0.6042, Val Loss: 0.5356\n",
      "Saving model at epoch 424 with loss 0.6042\n",
      "Epoch 425, Loss: 0.6038, Val Loss: 0.5352\n",
      "Saving model at epoch 425 with loss 0.6038\n",
      "Epoch 426, Loss: 0.6035, Val Loss: 0.5349\n",
      "Saving model at epoch 426 with loss 0.6035\n",
      "Epoch 427, Loss: 0.6032, Val Loss: 0.5346\n",
      "Saving model at epoch 427 with loss 0.6032\n",
      "Epoch 428, Loss: 0.6029, Val Loss: 0.5343\n",
      "Saving model at epoch 428 with loss 0.6029\n",
      "Epoch 429, Loss: 0.6026, Val Loss: 0.5339\n",
      "Saving model at epoch 429 with loss 0.6026\n",
      "Epoch 430, Loss: 0.6023, Val Loss: 0.5336\n",
      "Saving model at epoch 430 with loss 0.6023\n",
      "Epoch 431, Loss: 0.6019, Val Loss: 0.5333\n",
      "Saving model at epoch 431 with loss 0.6019\n",
      "Epoch 432, Loss: 0.6017, Val Loss: 0.5329\n",
      "Saving model at epoch 432 with loss 0.6017\n",
      "Epoch 433, Loss: 0.6013, Val Loss: 0.5326\n",
      "Saving model at epoch 433 with loss 0.6013\n",
      "Epoch 434, Loss: 0.6010, Val Loss: 0.5323\n",
      "Saving model at epoch 434 with loss 0.6010\n",
      "Epoch 435, Loss: 0.6007, Val Loss: 0.5320\n",
      "Saving model at epoch 435 with loss 0.6007\n",
      "Epoch 436, Loss: 0.6004, Val Loss: 0.5316\n",
      "Saving model at epoch 436 with loss 0.6004\n",
      "Epoch 437, Loss: 0.6001, Val Loss: 0.5313\n",
      "Saving model at epoch 437 with loss 0.6001\n",
      "Epoch 438, Loss: 0.5998, Val Loss: 0.5310\n",
      "Saving model at epoch 438 with loss 0.5998\n",
      "Epoch 439, Loss: 0.5995, Val Loss: 0.5307\n",
      "Saving model at epoch 439 with loss 0.5995\n",
      "Epoch 440, Loss: 0.5992, Val Loss: 0.5304\n",
      "Saving model at epoch 440 with loss 0.5992\n",
      "Epoch 441, Loss: 0.5990, Val Loss: 0.5301\n",
      "Saving model at epoch 441 with loss 0.5990\n",
      "Epoch 442, Loss: 0.5987, Val Loss: 0.5298\n",
      "Saving model at epoch 442 with loss 0.5987\n",
      "Epoch 443, Loss: 0.5984, Val Loss: 0.5295\n",
      "Saving model at epoch 443 with loss 0.5984\n",
      "Epoch 444, Loss: 0.5981, Val Loss: 0.5292\n",
      "Saving model at epoch 444 with loss 0.5981\n",
      "Epoch 445, Loss: 0.5978, Val Loss: 0.5288\n",
      "Saving model at epoch 445 with loss 0.5978\n",
      "Epoch 446, Loss: 0.5975, Val Loss: 0.5285\n",
      "Saving model at epoch 446 with loss 0.5975\n",
      "Epoch 447, Loss: 0.5972, Val Loss: 0.5282\n",
      "Saving model at epoch 447 with loss 0.5972\n",
      "Epoch 448, Loss: 0.5969, Val Loss: 0.5279\n",
      "Saving model at epoch 448 with loss 0.5969\n",
      "Epoch 449, Loss: 0.5967, Val Loss: 0.5276\n",
      "Saving model at epoch 449 with loss 0.5967\n",
      "Epoch 450, Loss: 0.5964, Val Loss: 0.5273\n",
      "Saving model at epoch 450 with loss 0.5964\n",
      "Epoch 451, Loss: 0.5961, Val Loss: 0.5270\n",
      "Saving model at epoch 451 with loss 0.5961\n",
      "Epoch 452, Loss: 0.5958, Val Loss: 0.5268\n",
      "Saving model at epoch 452 with loss 0.5958\n",
      "Epoch 453, Loss: 0.5956, Val Loss: 0.5265\n",
      "Saving model at epoch 453 with loss 0.5956\n",
      "Epoch 454, Loss: 0.5953, Val Loss: 0.5262\n",
      "Saving model at epoch 454 with loss 0.5953\n",
      "Epoch 455, Loss: 0.5950, Val Loss: 0.5259\n",
      "Saving model at epoch 455 with loss 0.5950\n",
      "Epoch 456, Loss: 0.5947, Val Loss: 0.5256\n",
      "Saving model at epoch 456 with loss 0.5947\n",
      "Epoch 457, Loss: 0.5945, Val Loss: 0.5253\n",
      "Saving model at epoch 457 with loss 0.5945\n",
      "Epoch 458, Loss: 0.5942, Val Loss: 0.5250\n",
      "Saving model at epoch 458 with loss 0.5942\n",
      "Epoch 459, Loss: 0.5939, Val Loss: 0.5247\n",
      "Saving model at epoch 459 with loss 0.5939\n",
      "Epoch 460, Loss: 0.5937, Val Loss: 0.5244\n",
      "Saving model at epoch 460 with loss 0.5937\n",
      "Epoch 461, Loss: 0.5934, Val Loss: 0.5242\n",
      "Saving model at epoch 461 with loss 0.5934\n",
      "Epoch 462, Loss: 0.5931, Val Loss: 0.5239\n",
      "Saving model at epoch 462 with loss 0.5931\n",
      "Epoch 463, Loss: 0.5929, Val Loss: 0.5236\n",
      "Saving model at epoch 463 with loss 0.5929\n",
      "Epoch 464, Loss: 0.5926, Val Loss: 0.5233\n",
      "Saving model at epoch 464 with loss 0.5926\n",
      "Epoch 465, Loss: 0.5923, Val Loss: 0.5230\n",
      "Saving model at epoch 465 with loss 0.5923\n",
      "Epoch 466, Loss: 0.5921, Val Loss: 0.5228\n",
      "Saving model at epoch 466 with loss 0.5921\n",
      "Epoch 467, Loss: 0.5918, Val Loss: 0.5225\n",
      "Saving model at epoch 467 with loss 0.5918\n",
      "Epoch 468, Loss: 0.5916, Val Loss: 0.5222\n",
      "Saving model at epoch 468 with loss 0.5916\n",
      "Epoch 469, Loss: 0.5913, Val Loss: 0.5219\n",
      "Saving model at epoch 469 with loss 0.5913\n",
      "Epoch 470, Loss: 0.5911, Val Loss: 0.5217\n",
      "Saving model at epoch 470 with loss 0.5911\n",
      "Epoch 471, Loss: 0.5908, Val Loss: 0.5214\n",
      "Saving model at epoch 471 with loss 0.5908\n",
      "Epoch 472, Loss: 0.5906, Val Loss: 0.5211\n",
      "Saving model at epoch 472 with loss 0.5906\n",
      "Epoch 473, Loss: 0.5903, Val Loss: 0.5208\n",
      "Saving model at epoch 473 with loss 0.5903\n",
      "Epoch 474, Loss: 0.5900, Val Loss: 0.5206\n",
      "Saving model at epoch 474 with loss 0.5900\n",
      "Epoch 475, Loss: 0.5898, Val Loss: 0.5203\n",
      "Saving model at epoch 475 with loss 0.5898\n",
      "Epoch 476, Loss: 0.5895, Val Loss: 0.5200\n",
      "Saving model at epoch 476 with loss 0.5895\n",
      "Epoch 477, Loss: 0.5893, Val Loss: 0.5198\n",
      "Saving model at epoch 477 with loss 0.5893\n",
      "Epoch 478, Loss: 0.5890, Val Loss: 0.5195\n",
      "Saving model at epoch 478 with loss 0.5890\n",
      "Epoch 479, Loss: 0.5888, Val Loss: 0.5193\n",
      "Saving model at epoch 479 with loss 0.5888\n",
      "Epoch 480, Loss: 0.5886, Val Loss: 0.5190\n",
      "Saving model at epoch 480 with loss 0.5886\n",
      "Epoch 481, Loss: 0.5883, Val Loss: 0.5187\n",
      "Saving model at epoch 481 with loss 0.5883\n",
      "Epoch 482, Loss: 0.5881, Val Loss: 0.5185\n",
      "Saving model at epoch 482 with loss 0.5881\n",
      "Epoch 483, Loss: 0.5878, Val Loss: 0.5182\n",
      "Saving model at epoch 483 with loss 0.5878\n",
      "Epoch 484, Loss: 0.5876, Val Loss: 0.5180\n",
      "Saving model at epoch 484 with loss 0.5876\n",
      "Epoch 485, Loss: 0.5874, Val Loss: 0.5177\n",
      "Saving model at epoch 485 with loss 0.5874\n",
      "Epoch 486, Loss: 0.5871, Val Loss: 0.5174\n",
      "Saving model at epoch 486 with loss 0.5871\n",
      "Epoch 487, Loss: 0.5869, Val Loss: 0.5172\n",
      "Saving model at epoch 487 with loss 0.5869\n",
      "Epoch 488, Loss: 0.5867, Val Loss: 0.5169\n",
      "Saving model at epoch 488 with loss 0.5867\n",
      "Epoch 489, Loss: 0.5864, Val Loss: 0.5167\n",
      "Saving model at epoch 489 with loss 0.5864\n",
      "Epoch 490, Loss: 0.5862, Val Loss: 0.5164\n",
      "Saving model at epoch 490 with loss 0.5862\n",
      "Epoch 491, Loss: 0.5859, Val Loss: 0.5162\n",
      "Saving model at epoch 491 with loss 0.5859\n",
      "Epoch 492, Loss: 0.5857, Val Loss: 0.5159\n",
      "Saving model at epoch 492 with loss 0.5857\n",
      "Epoch 493, Loss: 0.5855, Val Loss: 0.5157\n",
      "Saving model at epoch 493 with loss 0.5855\n",
      "Epoch 494, Loss: 0.5852, Val Loss: 0.5154\n",
      "Saving model at epoch 494 with loss 0.5852\n",
      "Epoch 495, Loss: 0.5850, Val Loss: 0.5152\n",
      "Saving model at epoch 495 with loss 0.5850\n",
      "Epoch 496, Loss: 0.5848, Val Loss: 0.5149\n",
      "Saving model at epoch 496 with loss 0.5848\n",
      "Epoch 497, Loss: 0.5845, Val Loss: 0.5147\n",
      "Saving model at epoch 497 with loss 0.5845\n",
      "Epoch 498, Loss: 0.5843, Val Loss: 0.5144\n",
      "Saving model at epoch 498 with loss 0.5843\n",
      "Epoch 499, Loss: 0.5841, Val Loss: 0.5142\n",
      "Saving model at epoch 499 with loss 0.5841\n",
      "Epoch 500, Loss: 0.5839, Val Loss: 0.5140\n",
      "Saving model at epoch 500 with loss 0.5839\n",
      "Epoch 501, Loss: 0.5836, Val Loss: 0.5137\n",
      "Saving model at epoch 501 with loss 0.5836\n",
      "Epoch 502, Loss: 0.5834, Val Loss: 0.5135\n",
      "Saving model at epoch 502 with loss 0.5834\n",
      "Epoch 503, Loss: 0.5832, Val Loss: 0.5132\n",
      "Saving model at epoch 503 with loss 0.5832\n",
      "Epoch 504, Loss: 0.5830, Val Loss: 0.5130\n",
      "Saving model at epoch 504 with loss 0.5830\n",
      "Epoch 505, Loss: 0.5828, Val Loss: 0.5128\n",
      "Saving model at epoch 505 with loss 0.5828\n",
      "Epoch 506, Loss: 0.5825, Val Loss: 0.5125\n",
      "Saving model at epoch 506 with loss 0.5825\n",
      "Epoch 507, Loss: 0.5823, Val Loss: 0.5123\n",
      "Saving model at epoch 507 with loss 0.5823\n",
      "Epoch 508, Loss: 0.5821, Val Loss: 0.5121\n",
      "Saving model at epoch 508 with loss 0.5821\n",
      "Epoch 509, Loss: 0.5819, Val Loss: 0.5118\n",
      "Saving model at epoch 509 with loss 0.5819\n",
      "Epoch 510, Loss: 0.5817, Val Loss: 0.5116\n",
      "Saving model at epoch 510 with loss 0.5817\n",
      "Epoch 511, Loss: 0.5814, Val Loss: 0.5114\n",
      "Saving model at epoch 511 with loss 0.5814\n",
      "Epoch 512, Loss: 0.5812, Val Loss: 0.5111\n",
      "Saving model at epoch 512 with loss 0.5812\n",
      "Epoch 513, Loss: 0.5810, Val Loss: 0.5109\n",
      "Saving model at epoch 513 with loss 0.5810\n",
      "Epoch 514, Loss: 0.5808, Val Loss: 0.5107\n",
      "Saving model at epoch 514 with loss 0.5808\n",
      "Epoch 515, Loss: 0.5806, Val Loss: 0.5104\n",
      "Saving model at epoch 515 with loss 0.5806\n",
      "Epoch 516, Loss: 0.5804, Val Loss: 0.5102\n",
      "Saving model at epoch 516 with loss 0.5804\n",
      "Epoch 517, Loss: 0.5801, Val Loss: 0.5100\n",
      "Saving model at epoch 517 with loss 0.5801\n",
      "Epoch 518, Loss: 0.5799, Val Loss: 0.5098\n",
      "Saving model at epoch 518 with loss 0.5799\n",
      "Epoch 519, Loss: 0.5797, Val Loss: 0.5095\n",
      "Saving model at epoch 519 with loss 0.5797\n",
      "Epoch 520, Loss: 0.5795, Val Loss: 0.5093\n",
      "Saving model at epoch 520 with loss 0.5795\n",
      "Epoch 521, Loss: 0.5793, Val Loss: 0.5091\n",
      "Saving model at epoch 521 with loss 0.5793\n",
      "Epoch 522, Loss: 0.5791, Val Loss: 0.5089\n",
      "Saving model at epoch 522 with loss 0.5791\n",
      "Epoch 523, Loss: 0.5789, Val Loss: 0.5086\n",
      "Saving model at epoch 523 with loss 0.5789\n",
      "Epoch 524, Loss: 0.5787, Val Loss: 0.5084\n",
      "Saving model at epoch 524 with loss 0.5787\n",
      "Epoch 525, Loss: 0.5785, Val Loss: 0.5082\n",
      "Saving model at epoch 525 with loss 0.5785\n",
      "Epoch 526, Loss: 0.5782, Val Loss: 0.5080\n",
      "Saving model at epoch 526 with loss 0.5782\n",
      "Epoch 527, Loss: 0.5781, Val Loss: 0.5077\n",
      "Saving model at epoch 527 with loss 0.5781\n",
      "Epoch 528, Loss: 0.5778, Val Loss: 0.5075\n",
      "Saving model at epoch 528 with loss 0.5778\n",
      "Epoch 529, Loss: 0.5777, Val Loss: 0.5073\n",
      "Saving model at epoch 529 with loss 0.5777\n",
      "Epoch 530, Loss: 0.5774, Val Loss: 0.5071\n",
      "Saving model at epoch 530 with loss 0.5774\n",
      "Epoch 531, Loss: 0.5772, Val Loss: 0.5069\n",
      "Saving model at epoch 531 with loss 0.5772\n",
      "Epoch 532, Loss: 0.5770, Val Loss: 0.5067\n",
      "Saving model at epoch 532 with loss 0.5770\n",
      "Epoch 533, Loss: 0.5768, Val Loss: 0.5064\n",
      "Saving model at epoch 533 with loss 0.5768\n",
      "Epoch 534, Loss: 0.5766, Val Loss: 0.5062\n",
      "Saving model at epoch 534 with loss 0.5766\n",
      "Epoch 535, Loss: 0.5764, Val Loss: 0.5060\n",
      "Saving model at epoch 535 with loss 0.5764\n",
      "Epoch 536, Loss: 0.5762, Val Loss: 0.5058\n",
      "Saving model at epoch 536 with loss 0.5762\n",
      "Epoch 537, Loss: 0.5760, Val Loss: 0.5056\n",
      "Saving model at epoch 537 with loss 0.5760\n",
      "Epoch 538, Loss: 0.5758, Val Loss: 0.5054\n",
      "Saving model at epoch 538 with loss 0.5758\n",
      "Epoch 539, Loss: 0.5756, Val Loss: 0.5052\n",
      "Saving model at epoch 539 with loss 0.5756\n",
      "Epoch 540, Loss: 0.5755, Val Loss: 0.5050\n",
      "Saving model at epoch 540 with loss 0.5755\n",
      "Epoch 541, Loss: 0.5752, Val Loss: 0.5048\n",
      "Saving model at epoch 541 with loss 0.5752\n",
      "Epoch 542, Loss: 0.5751, Val Loss: 0.5045\n",
      "Saving model at epoch 542 with loss 0.5751\n",
      "Epoch 543, Loss: 0.5748, Val Loss: 0.5043\n",
      "Saving model at epoch 543 with loss 0.5748\n",
      "Epoch 544, Loss: 0.5747, Val Loss: 0.5041\n",
      "Saving model at epoch 544 with loss 0.5747\n",
      "Epoch 545, Loss: 0.5744, Val Loss: 0.5039\n",
      "Saving model at epoch 545 with loss 0.5744\n",
      "Epoch 546, Loss: 0.5743, Val Loss: 0.5037\n",
      "Saving model at epoch 546 with loss 0.5743\n",
      "Epoch 547, Loss: 0.5741, Val Loss: 0.5035\n",
      "Saving model at epoch 547 with loss 0.5741\n",
      "Epoch 548, Loss: 0.5739, Val Loss: 0.5033\n",
      "Saving model at epoch 548 with loss 0.5739\n",
      "Epoch 549, Loss: 0.5737, Val Loss: 0.5031\n",
      "Saving model at epoch 549 with loss 0.5737\n",
      "Epoch 550, Loss: 0.5735, Val Loss: 0.5029\n",
      "Saving model at epoch 550 with loss 0.5735\n",
      "Epoch 551, Loss: 0.5733, Val Loss: 0.5027\n",
      "Saving model at epoch 551 with loss 0.5733\n",
      "Epoch 552, Loss: 0.5731, Val Loss: 0.5025\n",
      "Saving model at epoch 552 with loss 0.5731\n",
      "Epoch 553, Loss: 0.5729, Val Loss: 0.5023\n",
      "Saving model at epoch 553 with loss 0.5729\n",
      "Epoch 554, Loss: 0.5727, Val Loss: 0.5021\n",
      "Saving model at epoch 554 with loss 0.5727\n",
      "Epoch 555, Loss: 0.5726, Val Loss: 0.5019\n",
      "Saving model at epoch 555 with loss 0.5726\n",
      "Epoch 556, Loss: 0.5723, Val Loss: 0.5017\n",
      "Saving model at epoch 556 with loss 0.5723\n",
      "Epoch 557, Loss: 0.5722, Val Loss: 0.5015\n",
      "Saving model at epoch 557 with loss 0.5722\n",
      "Epoch 558, Loss: 0.5720, Val Loss: 0.5013\n",
      "Saving model at epoch 558 with loss 0.5720\n",
      "Epoch 559, Loss: 0.5718, Val Loss: 0.5011\n",
      "Saving model at epoch 559 with loss 0.5718\n",
      "Epoch 560, Loss: 0.5716, Val Loss: 0.5009\n",
      "Saving model at epoch 560 with loss 0.5716\n",
      "Epoch 561, Loss: 0.5714, Val Loss: 0.5007\n",
      "Saving model at epoch 561 with loss 0.5714\n",
      "Epoch 562, Loss: 0.5713, Val Loss: 0.5005\n",
      "Saving model at epoch 562 with loss 0.5713\n",
      "Epoch 563, Loss: 0.5711, Val Loss: 0.5003\n",
      "Saving model at epoch 563 with loss 0.5711\n",
      "Epoch 564, Loss: 0.5709, Val Loss: 0.5001\n",
      "Saving model at epoch 564 with loss 0.5709\n",
      "Epoch 565, Loss: 0.5707, Val Loss: 0.4999\n",
      "Saving model at epoch 565 with loss 0.5707\n",
      "Epoch 566, Loss: 0.5705, Val Loss: 0.4997\n",
      "Saving model at epoch 566 with loss 0.5705\n",
      "Epoch 567, Loss: 0.5704, Val Loss: 0.4995\n",
      "Saving model at epoch 567 with loss 0.5704\n",
      "Epoch 568, Loss: 0.5702, Val Loss: 0.4993\n",
      "Saving model at epoch 568 with loss 0.5702\n",
      "Epoch 569, Loss: 0.5700, Val Loss: 0.4992\n",
      "Saving model at epoch 569 with loss 0.5700\n",
      "Epoch 570, Loss: 0.5698, Val Loss: 0.4990\n",
      "Saving model at epoch 570 with loss 0.5698\n",
      "Epoch 571, Loss: 0.5696, Val Loss: 0.4988\n",
      "Saving model at epoch 571 with loss 0.5696\n",
      "Epoch 572, Loss: 0.5695, Val Loss: 0.4986\n",
      "Saving model at epoch 572 with loss 0.5695\n",
      "Epoch 573, Loss: 0.5693, Val Loss: 0.4984\n",
      "Saving model at epoch 573 with loss 0.5693\n",
      "Epoch 574, Loss: 0.5691, Val Loss: 0.4982\n",
      "Saving model at epoch 574 with loss 0.5691\n",
      "Epoch 575, Loss: 0.5689, Val Loss: 0.4980\n",
      "Saving model at epoch 575 with loss 0.5689\n",
      "Epoch 576, Loss: 0.5687, Val Loss: 0.4978\n",
      "Saving model at epoch 576 with loss 0.5687\n",
      "Epoch 577, Loss: 0.5685, Val Loss: 0.4976\n",
      "Saving model at epoch 577 with loss 0.5685\n",
      "Epoch 578, Loss: 0.5684, Val Loss: 0.4975\n",
      "Saving model at epoch 578 with loss 0.5684\n",
      "Epoch 579, Loss: 0.5682, Val Loss: 0.4973\n",
      "Saving model at epoch 579 with loss 0.5682\n",
      "Epoch 580, Loss: 0.5680, Val Loss: 0.4971\n",
      "Saving model at epoch 580 with loss 0.5680\n",
      "Epoch 581, Loss: 0.5679, Val Loss: 0.4969\n",
      "Saving model at epoch 581 with loss 0.5679\n",
      "Epoch 582, Loss: 0.5677, Val Loss: 0.4967\n",
      "Saving model at epoch 582 with loss 0.5677\n",
      "Epoch 583, Loss: 0.5675, Val Loss: 0.4965\n",
      "Saving model at epoch 583 with loss 0.5675\n",
      "Epoch 584, Loss: 0.5673, Val Loss: 0.4964\n",
      "Saving model at epoch 584 with loss 0.5673\n",
      "Epoch 585, Loss: 0.5672, Val Loss: 0.4962\n",
      "Saving model at epoch 585 with loss 0.5672\n",
      "Epoch 586, Loss: 0.5670, Val Loss: 0.4960\n",
      "Saving model at epoch 586 with loss 0.5670\n",
      "Epoch 587, Loss: 0.5668, Val Loss: 0.4958\n",
      "Saving model at epoch 587 with loss 0.5668\n",
      "Epoch 588, Loss: 0.5667, Val Loss: 0.4956\n",
      "Saving model at epoch 588 with loss 0.5667\n",
      "Epoch 589, Loss: 0.5665, Val Loss: 0.4954\n",
      "Saving model at epoch 589 with loss 0.5665\n",
      "Epoch 590, Loss: 0.5663, Val Loss: 0.4953\n",
      "Saving model at epoch 590 with loss 0.5663\n",
      "Epoch 591, Loss: 0.5661, Val Loss: 0.4951\n",
      "Saving model at epoch 591 with loss 0.5661\n",
      "Epoch 592, Loss: 0.5660, Val Loss: 0.4949\n",
      "Saving model at epoch 592 with loss 0.5660\n",
      "Epoch 593, Loss: 0.5658, Val Loss: 0.4947\n",
      "Saving model at epoch 593 with loss 0.5658\n",
      "Epoch 594, Loss: 0.5656, Val Loss: 0.4946\n",
      "Saving model at epoch 594 with loss 0.5656\n",
      "Epoch 595, Loss: 0.5655, Val Loss: 0.4944\n",
      "Saving model at epoch 595 with loss 0.5655\n",
      "Epoch 596, Loss: 0.5653, Val Loss: 0.4942\n",
      "Saving model at epoch 596 with loss 0.5653\n",
      "Epoch 597, Loss: 0.5651, Val Loss: 0.4940\n",
      "Saving model at epoch 597 with loss 0.5651\n",
      "Epoch 598, Loss: 0.5650, Val Loss: 0.4938\n",
      "Saving model at epoch 598 with loss 0.5650\n",
      "Epoch 599, Loss: 0.5648, Val Loss: 0.4937\n",
      "Saving model at epoch 599 with loss 0.5648\n",
      "Epoch 600, Loss: 0.5646, Val Loss: 0.4935\n",
      "Saving model at epoch 600 with loss 0.5646\n",
      "Epoch 601, Loss: 0.5645, Val Loss: 0.4933\n",
      "Saving model at epoch 601 with loss 0.5645\n",
      "Epoch 602, Loss: 0.5643, Val Loss: 0.4931\n",
      "Saving model at epoch 602 with loss 0.5643\n",
      "Epoch 603, Loss: 0.5642, Val Loss: 0.4930\n",
      "Saving model at epoch 603 with loss 0.5642\n",
      "Epoch 604, Loss: 0.5640, Val Loss: 0.4928\n",
      "Saving model at epoch 604 with loss 0.5640\n",
      "Epoch 605, Loss: 0.5638, Val Loss: 0.4926\n",
      "Saving model at epoch 605 with loss 0.5638\n",
      "Epoch 606, Loss: 0.5636, Val Loss: 0.4924\n",
      "Saving model at epoch 606 with loss 0.5636\n",
      "Epoch 607, Loss: 0.5635, Val Loss: 0.4923\n",
      "Saving model at epoch 607 with loss 0.5635\n",
      "Epoch 608, Loss: 0.5633, Val Loss: 0.4921\n",
      "Saving model at epoch 608 with loss 0.5633\n",
      "Epoch 609, Loss: 0.5632, Val Loss: 0.4919\n",
      "Saving model at epoch 609 with loss 0.5632\n",
      "Epoch 610, Loss: 0.5630, Val Loss: 0.4918\n",
      "Saving model at epoch 610 with loss 0.5630\n",
      "Epoch 611, Loss: 0.5628, Val Loss: 0.4916\n",
      "Saving model at epoch 611 with loss 0.5628\n",
      "Epoch 612, Loss: 0.5627, Val Loss: 0.4914\n",
      "Saving model at epoch 612 with loss 0.5627\n",
      "Epoch 613, Loss: 0.5625, Val Loss: 0.4912\n",
      "Saving model at epoch 613 with loss 0.5625\n",
      "Epoch 614, Loss: 0.5624, Val Loss: 0.4911\n",
      "Saving model at epoch 614 with loss 0.5624\n",
      "Epoch 615, Loss: 0.5622, Val Loss: 0.4909\n",
      "Saving model at epoch 615 with loss 0.5622\n",
      "Epoch 616, Loss: 0.5620, Val Loss: 0.4907\n",
      "Saving model at epoch 616 with loss 0.5620\n",
      "Epoch 617, Loss: 0.5619, Val Loss: 0.4906\n",
      "Saving model at epoch 617 with loss 0.5619\n",
      "Epoch 618, Loss: 0.5617, Val Loss: 0.4904\n",
      "Saving model at epoch 618 with loss 0.5617\n",
      "Epoch 619, Loss: 0.5616, Val Loss: 0.4902\n",
      "Saving model at epoch 619 with loss 0.5616\n",
      "Epoch 620, Loss: 0.5614, Val Loss: 0.4901\n",
      "Saving model at epoch 620 with loss 0.5614\n",
      "Epoch 621, Loss: 0.5612, Val Loss: 0.4899\n",
      "Saving model at epoch 621 with loss 0.5612\n",
      "Epoch 622, Loss: 0.5611, Val Loss: 0.4897\n",
      "Saving model at epoch 622 with loss 0.5611\n",
      "Epoch 623, Loss: 0.5610, Val Loss: 0.4896\n",
      "Saving model at epoch 623 with loss 0.5610\n",
      "Epoch 624, Loss: 0.5608, Val Loss: 0.4894\n",
      "Saving model at epoch 624 with loss 0.5608\n",
      "Epoch 625, Loss: 0.5606, Val Loss: 0.4892\n",
      "Saving model at epoch 625 with loss 0.5606\n",
      "Epoch 626, Loss: 0.5605, Val Loss: 0.4891\n",
      "Saving model at epoch 626 with loss 0.5605\n",
      "Epoch 627, Loss: 0.5603, Val Loss: 0.4889\n",
      "Saving model at epoch 627 with loss 0.5603\n",
      "Epoch 628, Loss: 0.5602, Val Loss: 0.4887\n",
      "Saving model at epoch 628 with loss 0.5602\n",
      "Epoch 629, Loss: 0.5600, Val Loss: 0.4886\n",
      "Saving model at epoch 629 with loss 0.5600\n",
      "Epoch 630, Loss: 0.5599, Val Loss: 0.4884\n",
      "Saving model at epoch 630 with loss 0.5599\n",
      "Epoch 631, Loss: 0.5597, Val Loss: 0.4883\n",
      "Saving model at epoch 631 with loss 0.5597\n",
      "Epoch 632, Loss: 0.5596, Val Loss: 0.4881\n",
      "Saving model at epoch 632 with loss 0.5596\n",
      "Epoch 633, Loss: 0.5594, Val Loss: 0.4879\n",
      "Saving model at epoch 633 with loss 0.5594\n",
      "Epoch 634, Loss: 0.5593, Val Loss: 0.4878\n",
      "Saving model at epoch 634 with loss 0.5593\n",
      "Epoch 635, Loss: 0.5591, Val Loss: 0.4876\n",
      "Saving model at epoch 635 with loss 0.5591\n",
      "Epoch 636, Loss: 0.5590, Val Loss: 0.4875\n",
      "Saving model at epoch 636 with loss 0.5590\n",
      "Epoch 637, Loss: 0.5588, Val Loss: 0.4873\n",
      "Saving model at epoch 637 with loss 0.5588\n",
      "Epoch 638, Loss: 0.5587, Val Loss: 0.4871\n",
      "Saving model at epoch 638 with loss 0.5587\n",
      "Epoch 639, Loss: 0.5585, Val Loss: 0.4870\n",
      "Saving model at epoch 639 with loss 0.5585\n",
      "Epoch 640, Loss: 0.5584, Val Loss: 0.4868\n",
      "Saving model at epoch 640 with loss 0.5584\n",
      "Epoch 641, Loss: 0.5582, Val Loss: 0.4867\n",
      "Saving model at epoch 641 with loss 0.5582\n",
      "Epoch 642, Loss: 0.5581, Val Loss: 0.4865\n",
      "Saving model at epoch 642 with loss 0.5581\n",
      "Epoch 643, Loss: 0.5579, Val Loss: 0.4863\n",
      "Saving model at epoch 643 with loss 0.5579\n",
      "Epoch 644, Loss: 0.5578, Val Loss: 0.4862\n",
      "Saving model at epoch 644 with loss 0.5578\n",
      "Epoch 645, Loss: 0.5576, Val Loss: 0.4860\n",
      "Saving model at epoch 645 with loss 0.5576\n",
      "Epoch 646, Loss: 0.5575, Val Loss: 0.4859\n",
      "Saving model at epoch 646 with loss 0.5575\n",
      "Epoch 647, Loss: 0.5573, Val Loss: 0.4857\n",
      "Saving model at epoch 647 with loss 0.5573\n",
      "Epoch 648, Loss: 0.5572, Val Loss: 0.4856\n",
      "Saving model at epoch 648 with loss 0.5572\n",
      "Epoch 649, Loss: 0.5570, Val Loss: 0.4854\n",
      "Saving model at epoch 649 with loss 0.5570\n",
      "Epoch 650, Loss: 0.5569, Val Loss: 0.4852\n",
      "Saving model at epoch 650 with loss 0.5569\n",
      "Epoch 651, Loss: 0.5567, Val Loss: 0.4851\n",
      "Saving model at epoch 651 with loss 0.5567\n",
      "Epoch 652, Loss: 0.5566, Val Loss: 0.4849\n",
      "Saving model at epoch 652 with loss 0.5566\n",
      "Epoch 653, Loss: 0.5565, Val Loss: 0.4848\n",
      "Saving model at epoch 653 with loss 0.5565\n",
      "Epoch 654, Loss: 0.5563, Val Loss: 0.4846\n",
      "Saving model at epoch 654 with loss 0.5563\n",
      "Epoch 655, Loss: 0.5562, Val Loss: 0.4845\n",
      "Saving model at epoch 655 with loss 0.5562\n",
      "Epoch 656, Loss: 0.5560, Val Loss: 0.4843\n",
      "Saving model at epoch 656 with loss 0.5560\n",
      "Epoch 657, Loss: 0.5559, Val Loss: 0.4842\n",
      "Saving model at epoch 657 with loss 0.5559\n",
      "Epoch 658, Loss: 0.5558, Val Loss: 0.4840\n",
      "Saving model at epoch 658 with loss 0.5558\n",
      "Epoch 659, Loss: 0.5556, Val Loss: 0.4839\n",
      "Saving model at epoch 659 with loss 0.5556\n",
      "Epoch 660, Loss: 0.5555, Val Loss: 0.4837\n",
      "Saving model at epoch 660 with loss 0.5555\n",
      "Epoch 661, Loss: 0.5553, Val Loss: 0.4836\n",
      "Saving model at epoch 661 with loss 0.5553\n",
      "Epoch 662, Loss: 0.5552, Val Loss: 0.4834\n",
      "Saving model at epoch 662 with loss 0.5552\n",
      "Epoch 663, Loss: 0.5551, Val Loss: 0.4833\n",
      "Saving model at epoch 663 with loss 0.5551\n",
      "Epoch 664, Loss: 0.5549, Val Loss: 0.4831\n",
      "Saving model at epoch 664 with loss 0.5549\n",
      "Epoch 665, Loss: 0.5548, Val Loss: 0.4830\n",
      "Saving model at epoch 665 with loss 0.5548\n",
      "Epoch 666, Loss: 0.5546, Val Loss: 0.4828\n",
      "Saving model at epoch 666 with loss 0.5546\n",
      "Epoch 667, Loss: 0.5545, Val Loss: 0.4827\n",
      "Saving model at epoch 667 with loss 0.5545\n",
      "Epoch 668, Loss: 0.5544, Val Loss: 0.4825\n",
      "Saving model at epoch 668 with loss 0.5544\n",
      "Epoch 669, Loss: 0.5542, Val Loss: 0.4824\n",
      "Saving model at epoch 669 with loss 0.5542\n",
      "Epoch 670, Loss: 0.5541, Val Loss: 0.4822\n",
      "Saving model at epoch 670 with loss 0.5541\n",
      "Epoch 671, Loss: 0.5539, Val Loss: 0.4821\n",
      "Saving model at epoch 671 with loss 0.5539\n",
      "Epoch 672, Loss: 0.5538, Val Loss: 0.4819\n",
      "Saving model at epoch 672 with loss 0.5538\n",
      "Epoch 673, Loss: 0.5537, Val Loss: 0.4818\n",
      "Saving model at epoch 673 with loss 0.5537\n",
      "Epoch 674, Loss: 0.5535, Val Loss: 0.4816\n",
      "Saving model at epoch 674 with loss 0.5535\n",
      "Epoch 675, Loss: 0.5534, Val Loss: 0.4815\n",
      "Saving model at epoch 675 with loss 0.5534\n",
      "Epoch 676, Loss: 0.5533, Val Loss: 0.4813\n",
      "Saving model at epoch 676 with loss 0.5533\n",
      "Epoch 677, Loss: 0.5532, Val Loss: 0.4812\n",
      "Saving model at epoch 677 with loss 0.5532\n",
      "Epoch 678, Loss: 0.5530, Val Loss: 0.4810\n",
      "Saving model at epoch 678 with loss 0.5530\n",
      "Epoch 679, Loss: 0.5529, Val Loss: 0.4809\n",
      "Saving model at epoch 679 with loss 0.5529\n",
      "Epoch 680, Loss: 0.5528, Val Loss: 0.4807\n",
      "Saving model at epoch 680 with loss 0.5528\n",
      "Epoch 681, Loss: 0.5527, Val Loss: 0.4806\n",
      "Saving model at epoch 681 with loss 0.5527\n",
      "Epoch 682, Loss: 0.5525, Val Loss: 0.4805\n",
      "Saving model at epoch 682 with loss 0.5525\n",
      "Epoch 683, Loss: 0.5524, Val Loss: 0.4803\n",
      "Saving model at epoch 683 with loss 0.5524\n",
      "Epoch 684, Loss: 0.5523, Val Loss: 0.4802\n",
      "Saving model at epoch 684 with loss 0.5523\n",
      "Epoch 685, Loss: 0.5522, Val Loss: 0.4800\n",
      "Saving model at epoch 685 with loss 0.5522\n",
      "Epoch 686, Loss: 0.5521, Val Loss: 0.4799\n",
      "Saving model at epoch 686 with loss 0.5521\n",
      "Epoch 687, Loss: 0.5520, Val Loss: 0.4797\n",
      "Saving model at epoch 687 with loss 0.5520\n",
      "Epoch 688, Loss: 0.5518, Val Loss: 0.4796\n",
      "Saving model at epoch 688 with loss 0.5518\n",
      "Epoch 689, Loss: 0.5517, Val Loss: 0.4795\n",
      "Saving model at epoch 689 with loss 0.5517\n",
      "Epoch 690, Loss: 0.5516, Val Loss: 0.4793\n",
      "Saving model at epoch 690 with loss 0.5516\n",
      "Epoch 691, Loss: 0.5515, Val Loss: 0.4792\n",
      "Saving model at epoch 691 with loss 0.5515\n",
      "Epoch 692, Loss: 0.5514, Val Loss: 0.4790\n",
      "Saving model at epoch 692 with loss 0.5514\n",
      "Epoch 693, Loss: 0.5513, Val Loss: 0.4789\n",
      "Saving model at epoch 693 with loss 0.5513\n",
      "Epoch 694, Loss: 0.5511, Val Loss: 0.4788\n",
      "Saving model at epoch 694 with loss 0.5511\n",
      "Epoch 695, Loss: 0.5510, Val Loss: 0.4786\n",
      "Saving model at epoch 695 with loss 0.5510\n",
      "Epoch 696, Loss: 0.5509, Val Loss: 0.4785\n",
      "Saving model at epoch 696 with loss 0.5509\n",
      "Epoch 697, Loss: 0.5508, Val Loss: 0.4783\n",
      "Saving model at epoch 697 with loss 0.5508\n",
      "Epoch 698, Loss: 0.5507, Val Loss: 0.4782\n",
      "Saving model at epoch 698 with loss 0.5507\n",
      "Epoch 699, Loss: 0.5506, Val Loss: 0.4781\n",
      "Saving model at epoch 699 with loss 0.5506\n",
      "Epoch 700, Loss: 0.5504, Val Loss: 0.4779\n",
      "Saving model at epoch 700 with loss 0.5504\n",
      "Epoch 701, Loss: 0.5503, Val Loss: 0.4778\n",
      "Saving model at epoch 701 with loss 0.5503\n",
      "Epoch 702, Loss: 0.5502, Val Loss: 0.4776\n",
      "Saving model at epoch 702 with loss 0.5502\n",
      "Epoch 703, Loss: 0.5501, Val Loss: 0.4775\n",
      "Saving model at epoch 703 with loss 0.5501\n",
      "Epoch 704, Loss: 0.5500, Val Loss: 0.4774\n",
      "Saving model at epoch 704 with loss 0.5500\n",
      "Epoch 705, Loss: 0.5499, Val Loss: 0.4772\n",
      "Saving model at epoch 705 with loss 0.5499\n",
      "Epoch 706, Loss: 0.5498, Val Loss: 0.4771\n",
      "Saving model at epoch 706 with loss 0.5498\n",
      "Epoch 707, Loss: 0.5497, Val Loss: 0.4769\n",
      "Saving model at epoch 707 with loss 0.5497\n",
      "Epoch 708, Loss: 0.5495, Val Loss: 0.4768\n",
      "Saving model at epoch 708 with loss 0.5495\n",
      "Epoch 709, Loss: 0.5494, Val Loss: 0.4767\n",
      "Saving model at epoch 709 with loss 0.5494\n",
      "Epoch 710, Loss: 0.5493, Val Loss: 0.4765\n",
      "Saving model at epoch 710 with loss 0.5493\n",
      "Epoch 711, Loss: 0.5492, Val Loss: 0.4764\n",
      "Saving model at epoch 711 with loss 0.5492\n",
      "Epoch 712, Loss: 0.5491, Val Loss: 0.4763\n",
      "Saving model at epoch 712 with loss 0.5491\n",
      "Epoch 713, Loss: 0.5490, Val Loss: 0.4761\n",
      "Saving model at epoch 713 with loss 0.5490\n",
      "Epoch 714, Loss: 0.5489, Val Loss: 0.4760\n",
      "Saving model at epoch 714 with loss 0.5489\n",
      "Epoch 715, Loss: 0.5488, Val Loss: 0.4759\n",
      "Saving model at epoch 715 with loss 0.5488\n",
      "Epoch 716, Loss: 0.5486, Val Loss: 0.4757\n",
      "Saving model at epoch 716 with loss 0.5486\n",
      "Epoch 717, Loss: 0.5485, Val Loss: 0.4756\n",
      "Saving model at epoch 717 with loss 0.5485\n",
      "Epoch 718, Loss: 0.5484, Val Loss: 0.4755\n",
      "Saving model at epoch 718 with loss 0.5484\n",
      "Epoch 719, Loss: 0.5483, Val Loss: 0.4753\n",
      "Saving model at epoch 719 with loss 0.5483\n",
      "Epoch 720, Loss: 0.5482, Val Loss: 0.4752\n",
      "Saving model at epoch 720 with loss 0.5482\n",
      "Epoch 721, Loss: 0.5481, Val Loss: 0.4751\n",
      "Saving model at epoch 721 with loss 0.5481\n",
      "Epoch 722, Loss: 0.5480, Val Loss: 0.4749\n",
      "Saving model at epoch 722 with loss 0.5480\n",
      "Epoch 723, Loss: 0.5479, Val Loss: 0.4748\n",
      "Saving model at epoch 723 with loss 0.5479\n",
      "Epoch 724, Loss: 0.5478, Val Loss: 0.4747\n",
      "Saving model at epoch 724 with loss 0.5478\n",
      "Epoch 725, Loss: 0.5477, Val Loss: 0.4745\n",
      "Saving model at epoch 725 with loss 0.5477\n",
      "Epoch 726, Loss: 0.5476, Val Loss: 0.4744\n",
      "Saving model at epoch 726 with loss 0.5476\n",
      "Epoch 727, Loss: 0.5475, Val Loss: 0.4743\n",
      "Saving model at epoch 727 with loss 0.5475\n",
      "Epoch 728, Loss: 0.5474, Val Loss: 0.4741\n",
      "Saving model at epoch 728 with loss 0.5474\n",
      "Epoch 729, Loss: 0.5473, Val Loss: 0.4740\n",
      "Saving model at epoch 729 with loss 0.5473\n",
      "Epoch 730, Loss: 0.5471, Val Loss: 0.4739\n",
      "Saving model at epoch 730 with loss 0.5471\n",
      "Epoch 731, Loss: 0.5471, Val Loss: 0.4737\n",
      "Saving model at epoch 731 with loss 0.5471\n",
      "Epoch 732, Loss: 0.5469, Val Loss: 0.4736\n",
      "Saving model at epoch 732 with loss 0.5469\n",
      "Epoch 733, Loss: 0.5468, Val Loss: 0.4735\n",
      "Saving model at epoch 733 with loss 0.5468\n",
      "Epoch 734, Loss: 0.5467, Val Loss: 0.4733\n",
      "Saving model at epoch 734 with loss 0.5467\n",
      "Epoch 735, Loss: 0.5466, Val Loss: 0.4732\n",
      "Saving model at epoch 735 with loss 0.5466\n",
      "Epoch 736, Loss: 0.5465, Val Loss: 0.4731\n",
      "Saving model at epoch 736 with loss 0.5465\n",
      "Epoch 737, Loss: 0.5464, Val Loss: 0.4729\n",
      "Saving model at epoch 737 with loss 0.5464\n",
      "Epoch 738, Loss: 0.5463, Val Loss: 0.4728\n",
      "Saving model at epoch 738 with loss 0.5463\n",
      "Epoch 739, Loss: 0.5462, Val Loss: 0.4727\n",
      "Saving model at epoch 739 with loss 0.5462\n",
      "Epoch 740, Loss: 0.5461, Val Loss: 0.4725\n",
      "Saving model at epoch 740 with loss 0.5461\n",
      "Epoch 741, Loss: 0.5460, Val Loss: 0.4724\n",
      "Saving model at epoch 741 with loss 0.5460\n",
      "Epoch 742, Loss: 0.5459, Val Loss: 0.4723\n",
      "Saving model at epoch 742 with loss 0.5459\n",
      "Epoch 743, Loss: 0.5458, Val Loss: 0.4722\n",
      "Saving model at epoch 743 with loss 0.5458\n",
      "Epoch 744, Loss: 0.5457, Val Loss: 0.4720\n",
      "Saving model at epoch 744 with loss 0.5457\n",
      "Epoch 745, Loss: 0.5456, Val Loss: 0.4719\n",
      "Saving model at epoch 745 with loss 0.5456\n",
      "Epoch 746, Loss: 0.5455, Val Loss: 0.4718\n",
      "Saving model at epoch 746 with loss 0.5455\n",
      "Epoch 747, Loss: 0.5454, Val Loss: 0.4716\n",
      "Saving model at epoch 747 with loss 0.5454\n",
      "Epoch 748, Loss: 0.5453, Val Loss: 0.4715\n",
      "Saving model at epoch 748 with loss 0.5453\n",
      "Epoch 749, Loss: 0.5452, Val Loss: 0.4714\n",
      "Saving model at epoch 749 with loss 0.5452\n",
      "Epoch 750, Loss: 0.5451, Val Loss: 0.4713\n",
      "Saving model at epoch 750 with loss 0.5451\n",
      "Epoch 751, Loss: 0.5450, Val Loss: 0.4711\n",
      "Saving model at epoch 751 with loss 0.5450\n",
      "Epoch 752, Loss: 0.5449, Val Loss: 0.4710\n",
      "Saving model at epoch 752 with loss 0.5449\n",
      "Epoch 753, Loss: 0.5448, Val Loss: 0.4709\n",
      "Saving model at epoch 753 with loss 0.5448\n",
      "Epoch 754, Loss: 0.5447, Val Loss: 0.4708\n",
      "Saving model at epoch 754 with loss 0.5447\n",
      "Epoch 755, Loss: 0.5446, Val Loss: 0.4706\n",
      "Saving model at epoch 755 with loss 0.5446\n",
      "Epoch 756, Loss: 0.5445, Val Loss: 0.4705\n",
      "Saving model at epoch 756 with loss 0.5445\n",
      "Epoch 757, Loss: 0.5444, Val Loss: 0.4704\n",
      "Saving model at epoch 757 with loss 0.5444\n",
      "Epoch 758, Loss: 0.5443, Val Loss: 0.4703\n",
      "Saving model at epoch 758 with loss 0.5443\n",
      "Epoch 759, Loss: 0.5442, Val Loss: 0.4701\n",
      "Saving model at epoch 759 with loss 0.5442\n",
      "Epoch 760, Loss: 0.5441, Val Loss: 0.4700\n",
      "Saving model at epoch 760 with loss 0.5441\n",
      "Epoch 761, Loss: 0.5440, Val Loss: 0.4699\n",
      "Saving model at epoch 761 with loss 0.5440\n",
      "Epoch 762, Loss: 0.5439, Val Loss: 0.4698\n",
      "Saving model at epoch 762 with loss 0.5439\n",
      "Epoch 763, Loss: 0.5438, Val Loss: 0.4696\n",
      "Saving model at epoch 763 with loss 0.5438\n",
      "Epoch 764, Loss: 0.5437, Val Loss: 0.4695\n",
      "Saving model at epoch 764 with loss 0.5437\n",
      "Epoch 765, Loss: 0.5436, Val Loss: 0.4694\n",
      "Saving model at epoch 765 with loss 0.5436\n",
      "Epoch 766, Loss: 0.5435, Val Loss: 0.4693\n",
      "Saving model at epoch 766 with loss 0.5435\n",
      "Epoch 767, Loss: 0.5434, Val Loss: 0.4691\n",
      "Saving model at epoch 767 with loss 0.5434\n",
      "Epoch 768, Loss: 0.5433, Val Loss: 0.4690\n",
      "Saving model at epoch 768 with loss 0.5433\n",
      "Epoch 769, Loss: 0.5432, Val Loss: 0.4689\n",
      "Saving model at epoch 769 with loss 0.5432\n",
      "Epoch 770, Loss: 0.5431, Val Loss: 0.4688\n",
      "Saving model at epoch 770 with loss 0.5431\n",
      "Epoch 771, Loss: 0.5430, Val Loss: 0.4686\n",
      "Saving model at epoch 771 with loss 0.5430\n",
      "Epoch 772, Loss: 0.5429, Val Loss: 0.4685\n",
      "Saving model at epoch 772 with loss 0.5429\n",
      "Epoch 773, Loss: 0.5428, Val Loss: 0.4684\n",
      "Saving model at epoch 773 with loss 0.5428\n",
      "Epoch 774, Loss: 0.5427, Val Loss: 0.4683\n",
      "Saving model at epoch 774 with loss 0.5427\n",
      "Epoch 775, Loss: 0.5426, Val Loss: 0.4681\n",
      "Saving model at epoch 775 with loss 0.5426\n",
      "Epoch 776, Loss: 0.5425, Val Loss: 0.4680\n",
      "Saving model at epoch 776 with loss 0.5425\n",
      "Epoch 777, Loss: 0.5424, Val Loss: 0.4679\n",
      "Saving model at epoch 777 with loss 0.5424\n",
      "Epoch 778, Loss: 0.5423, Val Loss: 0.4678\n",
      "Saving model at epoch 778 with loss 0.5423\n",
      "Epoch 779, Loss: 0.5422, Val Loss: 0.4677\n",
      "Saving model at epoch 779 with loss 0.5422\n",
      "Epoch 780, Loss: 0.5421, Val Loss: 0.4675\n",
      "Saving model at epoch 780 with loss 0.5421\n",
      "Epoch 781, Loss: 0.5420, Val Loss: 0.4674\n",
      "Saving model at epoch 781 with loss 0.5420\n",
      "Epoch 782, Loss: 0.5419, Val Loss: 0.4673\n",
      "Saving model at epoch 782 with loss 0.5419\n",
      "Epoch 783, Loss: 0.5418, Val Loss: 0.4672\n",
      "Saving model at epoch 783 with loss 0.5418\n",
      "Epoch 784, Loss: 0.5418, Val Loss: 0.4671\n",
      "Saving model at epoch 784 with loss 0.5418\n",
      "Epoch 785, Loss: 0.5417, Val Loss: 0.4669\n",
      "Saving model at epoch 785 with loss 0.5417\n",
      "Epoch 786, Loss: 0.5416, Val Loss: 0.4668\n",
      "Saving model at epoch 786 with loss 0.5416\n",
      "Epoch 787, Loss: 0.5415, Val Loss: 0.4667\n",
      "Saving model at epoch 787 with loss 0.5415\n",
      "Epoch 788, Loss: 0.5414, Val Loss: 0.4666\n",
      "Saving model at epoch 788 with loss 0.5414\n",
      "Epoch 789, Loss: 0.5413, Val Loss: 0.4664\n",
      "Saving model at epoch 789 with loss 0.5413\n",
      "Epoch 790, Loss: 0.5412, Val Loss: 0.4663\n",
      "Saving model at epoch 790 with loss 0.5412\n",
      "Epoch 791, Loss: 0.5411, Val Loss: 0.4662\n",
      "Saving model at epoch 791 with loss 0.5411\n",
      "Epoch 792, Loss: 0.5410, Val Loss: 0.4661\n",
      "Saving model at epoch 792 with loss 0.5410\n",
      "Epoch 793, Loss: 0.5409, Val Loss: 0.4660\n",
      "Saving model at epoch 793 with loss 0.5409\n",
      "Epoch 794, Loss: 0.5408, Val Loss: 0.4659\n",
      "Saving model at epoch 794 with loss 0.5408\n",
      "Epoch 795, Loss: 0.5407, Val Loss: 0.4657\n",
      "Saving model at epoch 795 with loss 0.5407\n",
      "Epoch 796, Loss: 0.5406, Val Loss: 0.4656\n",
      "Saving model at epoch 796 with loss 0.5406\n",
      "Epoch 797, Loss: 0.5405, Val Loss: 0.4655\n",
      "Saving model at epoch 797 with loss 0.5405\n",
      "Epoch 798, Loss: 0.5404, Val Loss: 0.4654\n",
      "Saving model at epoch 798 with loss 0.5404\n",
      "Epoch 799, Loss: 0.5404, Val Loss: 0.4653\n",
      "Saving model at epoch 799 with loss 0.5404\n",
      "Epoch 800, Loss: 0.5403, Val Loss: 0.4651\n",
      "Saving model at epoch 800 with loss 0.5403\n",
      "Epoch 801, Loss: 0.5402, Val Loss: 0.4650\n",
      "Saving model at epoch 801 with loss 0.5402\n",
      "Epoch 802, Loss: 0.5401, Val Loss: 0.4649\n",
      "Saving model at epoch 802 with loss 0.5401\n",
      "Epoch 803, Loss: 0.5400, Val Loss: 0.4648\n",
      "Saving model at epoch 803 with loss 0.5400\n",
      "Epoch 804, Loss: 0.5399, Val Loss: 0.4647\n",
      "Saving model at epoch 804 with loss 0.5399\n",
      "Epoch 805, Loss: 0.5398, Val Loss: 0.4646\n",
      "Saving model at epoch 805 with loss 0.5398\n",
      "Epoch 806, Loss: 0.5397, Val Loss: 0.4644\n",
      "Saving model at epoch 806 with loss 0.5397\n",
      "Epoch 807, Loss: 0.5396, Val Loss: 0.4643\n",
      "Saving model at epoch 807 with loss 0.5396\n",
      "Epoch 808, Loss: 0.5395, Val Loss: 0.4642\n",
      "Saving model at epoch 808 with loss 0.5395\n",
      "Epoch 809, Loss: 0.5394, Val Loss: 0.4641\n",
      "Saving model at epoch 809 with loss 0.5394\n",
      "Epoch 810, Loss: 0.5394, Val Loss: 0.4640\n",
      "Saving model at epoch 810 with loss 0.5394\n",
      "Epoch 811, Loss: 0.5393, Val Loss: 0.4639\n",
      "Saving model at epoch 811 with loss 0.5393\n",
      "Epoch 812, Loss: 0.5392, Val Loss: 0.4637\n",
      "Saving model at epoch 812 with loss 0.5392\n",
      "Epoch 813, Loss: 0.5391, Val Loss: 0.4636\n",
      "Saving model at epoch 813 with loss 0.5391\n",
      "Epoch 814, Loss: 0.5390, Val Loss: 0.4635\n",
      "Saving model at epoch 814 with loss 0.5390\n",
      "Epoch 815, Loss: 0.5389, Val Loss: 0.4634\n",
      "Saving model at epoch 815 with loss 0.5389\n",
      "Epoch 816, Loss: 0.5388, Val Loss: 0.4633\n",
      "Saving model at epoch 816 with loss 0.5388\n",
      "Epoch 817, Loss: 0.5387, Val Loss: 0.4632\n",
      "Saving model at epoch 817 with loss 0.5387\n",
      "Epoch 818, Loss: 0.5386, Val Loss: 0.4631\n",
      "Saving model at epoch 818 with loss 0.5386\n",
      "Epoch 819, Loss: 0.5386, Val Loss: 0.4629\n",
      "Saving model at epoch 819 with loss 0.5386\n",
      "Epoch 820, Loss: 0.5385, Val Loss: 0.4628\n",
      "Saving model at epoch 820 with loss 0.5385\n",
      "Epoch 821, Loss: 0.5384, Val Loss: 0.4627\n",
      "Saving model at epoch 821 with loss 0.5384\n",
      "Epoch 822, Loss: 0.5383, Val Loss: 0.4626\n",
      "Saving model at epoch 822 with loss 0.5383\n",
      "Epoch 823, Loss: 0.5382, Val Loss: 0.4625\n",
      "Saving model at epoch 823 with loss 0.5382\n",
      "Epoch 824, Loss: 0.5381, Val Loss: 0.4624\n",
      "Saving model at epoch 824 with loss 0.5381\n",
      "Epoch 825, Loss: 0.5380, Val Loss: 0.4623\n",
      "Saving model at epoch 825 with loss 0.5380\n",
      "Epoch 826, Loss: 0.5379, Val Loss: 0.4621\n",
      "Saving model at epoch 826 with loss 0.5379\n",
      "Epoch 827, Loss: 0.5379, Val Loss: 0.4620\n",
      "Saving model at epoch 827 with loss 0.5379\n",
      "Epoch 828, Loss: 0.5378, Val Loss: 0.4619\n",
      "Saving model at epoch 828 with loss 0.5378\n",
      "Epoch 829, Loss: 0.5377, Val Loss: 0.4618\n",
      "Saving model at epoch 829 with loss 0.5377\n",
      "Epoch 830, Loss: 0.5376, Val Loss: 0.4617\n",
      "Saving model at epoch 830 with loss 0.5376\n",
      "Epoch 831, Loss: 0.5375, Val Loss: 0.4616\n",
      "Saving model at epoch 831 with loss 0.5375\n",
      "Epoch 832, Loss: 0.5374, Val Loss: 0.4615\n",
      "Saving model at epoch 832 with loss 0.5374\n",
      "Epoch 833, Loss: 0.5374, Val Loss: 0.4613\n",
      "Saving model at epoch 833 with loss 0.5374\n",
      "Epoch 834, Loss: 0.5373, Val Loss: 0.4612\n",
      "Saving model at epoch 834 with loss 0.5373\n",
      "Epoch 835, Loss: 0.5372, Val Loss: 0.4611\n",
      "Saving model at epoch 835 with loss 0.5372\n",
      "Epoch 836, Loss: 0.5371, Val Loss: 0.4610\n",
      "Saving model at epoch 836 with loss 0.5371\n",
      "Epoch 837, Loss: 0.5370, Val Loss: 0.4609\n",
      "Saving model at epoch 837 with loss 0.5370\n",
      "Epoch 838, Loss: 0.5369, Val Loss: 0.4608\n",
      "Saving model at epoch 838 with loss 0.5369\n",
      "Epoch 839, Loss: 0.5369, Val Loss: 0.4607\n",
      "Saving model at epoch 839 with loss 0.5369\n",
      "Epoch 840, Loss: 0.5368, Val Loss: 0.4606\n",
      "Saving model at epoch 840 with loss 0.5368\n",
      "Epoch 841, Loss: 0.5367, Val Loss: 0.4605\n",
      "Saving model at epoch 841 with loss 0.5367\n",
      "Epoch 842, Loss: 0.5366, Val Loss: 0.4603\n",
      "Saving model at epoch 842 with loss 0.5366\n",
      "Epoch 843, Loss: 0.5365, Val Loss: 0.4602\n",
      "Saving model at epoch 843 with loss 0.5365\n",
      "Epoch 844, Loss: 0.5365, Val Loss: 0.4601\n",
      "Saving model at epoch 844 with loss 0.5365\n",
      "Epoch 845, Loss: 0.5364, Val Loss: 0.4600\n",
      "Saving model at epoch 845 with loss 0.5364\n",
      "Epoch 846, Loss: 0.5363, Val Loss: 0.4599\n",
      "Saving model at epoch 846 with loss 0.5363\n",
      "Epoch 847, Loss: 0.5362, Val Loss: 0.4598\n",
      "Saving model at epoch 847 with loss 0.5362\n",
      "Epoch 848, Loss: 0.5362, Val Loss: 0.4597\n",
      "Saving model at epoch 848 with loss 0.5362\n",
      "Epoch 849, Loss: 0.5361, Val Loss: 0.4596\n",
      "Saving model at epoch 849 with loss 0.5361\n",
      "Epoch 850, Loss: 0.5360, Val Loss: 0.4595\n",
      "Saving model at epoch 850 with loss 0.5360\n",
      "Epoch 851, Loss: 0.5359, Val Loss: 0.4594\n",
      "Saving model at epoch 851 with loss 0.5359\n",
      "Epoch 852, Loss: 0.5358, Val Loss: 0.4592\n",
      "Saving model at epoch 852 with loss 0.5358\n",
      "Epoch 853, Loss: 0.5358, Val Loss: 0.4591\n",
      "Saving model at epoch 853 with loss 0.5358\n",
      "Epoch 854, Loss: 0.5357, Val Loss: 0.4590\n",
      "Saving model at epoch 854 with loss 0.5357\n",
      "Epoch 855, Loss: 0.5356, Val Loss: 0.4589\n",
      "Saving model at epoch 855 with loss 0.5356\n",
      "Epoch 856, Loss: 0.5355, Val Loss: 0.4588\n",
      "Saving model at epoch 856 with loss 0.5355\n",
      "Epoch 857, Loss: 0.5355, Val Loss: 0.4587\n",
      "Saving model at epoch 857 with loss 0.5355\n",
      "Epoch 858, Loss: 0.5354, Val Loss: 0.4586\n",
      "Saving model at epoch 858 with loss 0.5354\n",
      "Epoch 859, Loss: 0.5353, Val Loss: 0.4585\n",
      "Saving model at epoch 859 with loss 0.5353\n",
      "Epoch 860, Loss: 0.5352, Val Loss: 0.4584\n",
      "Saving model at epoch 860 with loss 0.5352\n",
      "Epoch 861, Loss: 0.5352, Val Loss: 0.4583\n",
      "Saving model at epoch 861 with loss 0.5352\n",
      "Epoch 862, Loss: 0.5351, Val Loss: 0.4582\n",
      "Saving model at epoch 862 with loss 0.5351\n",
      "Epoch 863, Loss: 0.5350, Val Loss: 0.4580\n",
      "Saving model at epoch 863 with loss 0.5350\n",
      "Epoch 864, Loss: 0.5349, Val Loss: 0.4579\n",
      "Saving model at epoch 864 with loss 0.5349\n",
      "Epoch 865, Loss: 0.5349, Val Loss: 0.4578\n",
      "Saving model at epoch 865 with loss 0.5349\n",
      "Epoch 866, Loss: 0.5348, Val Loss: 0.4577\n",
      "Saving model at epoch 866 with loss 0.5348\n",
      "Epoch 867, Loss: 0.5347, Val Loss: 0.4576\n",
      "Saving model at epoch 867 with loss 0.5347\n",
      "Epoch 868, Loss: 0.5346, Val Loss: 0.4575\n",
      "Saving model at epoch 868 with loss 0.5346\n",
      "Epoch 869, Loss: 0.5346, Val Loss: 0.4574\n",
      "Saving model at epoch 869 with loss 0.5346\n",
      "Epoch 870, Loss: 0.5345, Val Loss: 0.4573\n",
      "Saving model at epoch 870 with loss 0.5345\n",
      "Epoch 871, Loss: 0.5344, Val Loss: 0.4572\n",
      "Saving model at epoch 871 with loss 0.5344\n",
      "Epoch 872, Loss: 0.5343, Val Loss: 0.4571\n",
      "Saving model at epoch 872 with loss 0.5343\n",
      "Epoch 873, Loss: 0.5343, Val Loss: 0.4570\n",
      "Saving model at epoch 873 with loss 0.5343\n",
      "Epoch 874, Loss: 0.5342, Val Loss: 0.4569\n",
      "Saving model at epoch 874 with loss 0.5342\n",
      "Epoch 875, Loss: 0.5341, Val Loss: 0.4568\n",
      "Saving model at epoch 875 with loss 0.5341\n",
      "Epoch 876, Loss: 0.5340, Val Loss: 0.4566\n",
      "Saving model at epoch 876 with loss 0.5340\n",
      "Epoch 877, Loss: 0.5340, Val Loss: 0.4565\n",
      "Saving model at epoch 877 with loss 0.5340\n",
      "Epoch 878, Loss: 0.5339, Val Loss: 0.4564\n",
      "Saving model at epoch 878 with loss 0.5339\n",
      "Epoch 879, Loss: 0.5338, Val Loss: 0.4563\n",
      "Saving model at epoch 879 with loss 0.5338\n",
      "Epoch 880, Loss: 0.5338, Val Loss: 0.4562\n",
      "Saving model at epoch 880 with loss 0.5338\n",
      "Epoch 881, Loss: 0.5337, Val Loss: 0.4561\n",
      "Saving model at epoch 881 with loss 0.5337\n",
      "Epoch 882, Loss: 0.5336, Val Loss: 0.4560\n",
      "Saving model at epoch 882 with loss 0.5336\n",
      "Epoch 883, Loss: 0.5335, Val Loss: 0.4559\n",
      "Saving model at epoch 883 with loss 0.5335\n",
      "Epoch 884, Loss: 0.5335, Val Loss: 0.4558\n",
      "Saving model at epoch 884 with loss 0.5335\n",
      "Epoch 885, Loss: 0.5334, Val Loss: 0.4557\n",
      "Saving model at epoch 885 with loss 0.5334\n",
      "Epoch 886, Loss: 0.5333, Val Loss: 0.4556\n",
      "Saving model at epoch 886 with loss 0.5333\n",
      "Epoch 887, Loss: 0.5333, Val Loss: 0.4555\n",
      "Saving model at epoch 887 with loss 0.5333\n",
      "Epoch 888, Loss: 0.5332, Val Loss: 0.4554\n",
      "Saving model at epoch 888 with loss 0.5332\n",
      "Epoch 889, Loss: 0.5331, Val Loss: 0.4553\n",
      "Saving model at epoch 889 with loss 0.5331\n",
      "Epoch 890, Loss: 0.5330, Val Loss: 0.4552\n",
      "Saving model at epoch 890 with loss 0.5330\n",
      "Epoch 891, Loss: 0.5330, Val Loss: 0.4551\n",
      "Saving model at epoch 891 with loss 0.5330\n",
      "Epoch 892, Loss: 0.5329, Val Loss: 0.4550\n",
      "Saving model at epoch 892 with loss 0.5329\n",
      "Epoch 893, Loss: 0.5328, Val Loss: 0.4549\n",
      "Saving model at epoch 893 with loss 0.5328\n",
      "Epoch 894, Loss: 0.5328, Val Loss: 0.4548\n",
      "Saving model at epoch 894 with loss 0.5328\n",
      "Epoch 895, Loss: 0.5327, Val Loss: 0.4547\n",
      "Saving model at epoch 895 with loss 0.5327\n",
      "Epoch 896, Loss: 0.5326, Val Loss: 0.4546\n",
      "Saving model at epoch 896 with loss 0.5326\n",
      "Epoch 897, Loss: 0.5326, Val Loss: 0.4545\n",
      "Saving model at epoch 897 with loss 0.5326\n",
      "Epoch 898, Loss: 0.5325, Val Loss: 0.4544\n",
      "Saving model at epoch 898 with loss 0.5325\n",
      "Epoch 899, Loss: 0.5324, Val Loss: 0.4542\n",
      "Saving model at epoch 899 with loss 0.5324\n",
      "Epoch 900, Loss: 0.5324, Val Loss: 0.4541\n",
      "Saving model at epoch 900 with loss 0.5324\n",
      "Epoch 901, Loss: 0.5323, Val Loss: 0.4540\n",
      "Saving model at epoch 901 with loss 0.5323\n",
      "Epoch 902, Loss: 0.5322, Val Loss: 0.4539\n",
      "Saving model at epoch 902 with loss 0.5322\n",
      "Epoch 903, Loss: 0.5322, Val Loss: 0.4538\n",
      "Saving model at epoch 903 with loss 0.5322\n",
      "Epoch 904, Loss: 0.5321, Val Loss: 0.4537\n",
      "Saving model at epoch 904 with loss 0.5321\n",
      "Epoch 905, Loss: 0.5320, Val Loss: 0.4536\n",
      "Saving model at epoch 905 with loss 0.5320\n",
      "Epoch 906, Loss: 0.5319, Val Loss: 0.4535\n",
      "Saving model at epoch 906 with loss 0.5319\n",
      "Epoch 907, Loss: 0.5319, Val Loss: 0.4534\n",
      "Saving model at epoch 907 with loss 0.5319\n",
      "Epoch 908, Loss: 0.5318, Val Loss: 0.4533\n",
      "Saving model at epoch 908 with loss 0.5318\n",
      "Epoch 909, Loss: 0.5317, Val Loss: 0.4532\n",
      "Saving model at epoch 909 with loss 0.5317\n",
      "Epoch 910, Loss: 0.5317, Val Loss: 0.4531\n",
      "Saving model at epoch 910 with loss 0.5317\n",
      "Epoch 911, Loss: 0.5316, Val Loss: 0.4530\n",
      "Saving model at epoch 911 with loss 0.5316\n",
      "Epoch 912, Loss: 0.5315, Val Loss: 0.4529\n",
      "Saving model at epoch 912 with loss 0.5315\n",
      "Epoch 913, Loss: 0.5315, Val Loss: 0.4528\n",
      "Saving model at epoch 913 with loss 0.5315\n",
      "Epoch 914, Loss: 0.5314, Val Loss: 0.4527\n",
      "Saving model at epoch 914 with loss 0.5314\n",
      "Epoch 915, Loss: 0.5313, Val Loss: 0.4526\n",
      "Saving model at epoch 915 with loss 0.5313\n",
      "Epoch 916, Loss: 0.5313, Val Loss: 0.4525\n",
      "Saving model at epoch 916 with loss 0.5313\n",
      "Epoch 917, Loss: 0.5312, Val Loss: 0.4524\n",
      "Saving model at epoch 917 with loss 0.5312\n",
      "Epoch 918, Loss: 0.5311, Val Loss: 0.4523\n",
      "Saving model at epoch 918 with loss 0.5311\n",
      "Epoch 919, Loss: 0.5311, Val Loss: 0.4522\n",
      "Saving model at epoch 919 with loss 0.5311\n",
      "Epoch 920, Loss: 0.5310, Val Loss: 0.4521\n",
      "Saving model at epoch 920 with loss 0.5310\n",
      "Epoch 921, Loss: 0.5309, Val Loss: 0.4520\n",
      "Saving model at epoch 921 with loss 0.5309\n",
      "Epoch 922, Loss: 0.5309, Val Loss: 0.4519\n",
      "Saving model at epoch 922 with loss 0.5309\n",
      "Epoch 923, Loss: 0.5308, Val Loss: 0.4518\n",
      "Saving model at epoch 923 with loss 0.5308\n",
      "Epoch 924, Loss: 0.5307, Val Loss: 0.4517\n",
      "Saving model at epoch 924 with loss 0.5307\n",
      "Epoch 925, Loss: 0.5307, Val Loss: 0.4516\n",
      "Saving model at epoch 925 with loss 0.5307\n",
      "Epoch 926, Loss: 0.5306, Val Loss: 0.4515\n",
      "Saving model at epoch 926 with loss 0.5306\n",
      "Epoch 927, Loss: 0.5305, Val Loss: 0.4514\n",
      "Saving model at epoch 927 with loss 0.5305\n",
      "Epoch 928, Loss: 0.5305, Val Loss: 0.4513\n",
      "Saving model at epoch 928 with loss 0.5305\n",
      "Epoch 929, Loss: 0.5304, Val Loss: 0.4512\n",
      "Saving model at epoch 929 with loss 0.5304\n",
      "Epoch 930, Loss: 0.5303, Val Loss: 0.4511\n",
      "Saving model at epoch 930 with loss 0.5303\n",
      "Epoch 931, Loss: 0.5303, Val Loss: 0.4510\n",
      "Saving model at epoch 931 with loss 0.5303\n",
      "Epoch 932, Loss: 0.5302, Val Loss: 0.4509\n",
      "Saving model at epoch 932 with loss 0.5302\n",
      "Epoch 933, Loss: 0.5302, Val Loss: 0.4508\n",
      "Saving model at epoch 933 with loss 0.5302\n",
      "Epoch 934, Loss: 0.5301, Val Loss: 0.4507\n",
      "Saving model at epoch 934 with loss 0.5301\n",
      "Epoch 935, Loss: 0.5300, Val Loss: 0.4506\n",
      "Saving model at epoch 935 with loss 0.5300\n",
      "Epoch 936, Loss: 0.5300, Val Loss: 0.4505\n",
      "Saving model at epoch 936 with loss 0.5300\n",
      "Epoch 937, Loss: 0.5299, Val Loss: 0.4504\n",
      "Saving model at epoch 937 with loss 0.5299\n",
      "Epoch 938, Loss: 0.5298, Val Loss: 0.4503\n",
      "Saving model at epoch 938 with loss 0.5298\n",
      "Epoch 939, Loss: 0.5298, Val Loss: 0.4502\n",
      "Saving model at epoch 939 with loss 0.5298\n",
      "Epoch 940, Loss: 0.5297, Val Loss: 0.4501\n",
      "Saving model at epoch 940 with loss 0.5297\n",
      "Epoch 941, Loss: 0.5296, Val Loss: 0.4500\n",
      "Saving model at epoch 941 with loss 0.5296\n",
      "Epoch 942, Loss: 0.5296, Val Loss: 0.4499\n",
      "Saving model at epoch 942 with loss 0.5296\n",
      "Epoch 943, Loss: 0.5295, Val Loss: 0.4498\n",
      "Saving model at epoch 943 with loss 0.5295\n",
      "Epoch 944, Loss: 0.5294, Val Loss: 0.4497\n",
      "Saving model at epoch 944 with loss 0.5294\n",
      "Epoch 945, Loss: 0.5294, Val Loss: 0.4496\n",
      "Saving model at epoch 945 with loss 0.5294\n",
      "Epoch 946, Loss: 0.5293, Val Loss: 0.4495\n",
      "Saving model at epoch 946 with loss 0.5293\n",
      "Epoch 947, Loss: 0.5292, Val Loss: 0.4494\n",
      "Saving model at epoch 947 with loss 0.5292\n",
      "Epoch 948, Loss: 0.5292, Val Loss: 0.4493\n",
      "Saving model at epoch 948 with loss 0.5292\n",
      "Epoch 949, Loss: 0.5291, Val Loss: 0.4492\n",
      "Saving model at epoch 949 with loss 0.5291\n",
      "Epoch 950, Loss: 0.5291, Val Loss: 0.4491\n",
      "Saving model at epoch 950 with loss 0.5291\n",
      "Epoch 951, Loss: 0.5290, Val Loss: 0.4490\n",
      "Saving model at epoch 951 with loss 0.5290\n",
      "Epoch 952, Loss: 0.5289, Val Loss: 0.4489\n",
      "Saving model at epoch 952 with loss 0.5289\n",
      "Epoch 953, Loss: 0.5289, Val Loss: 0.4488\n",
      "Saving model at epoch 953 with loss 0.5289\n",
      "Epoch 954, Loss: 0.5288, Val Loss: 0.4487\n",
      "Saving model at epoch 954 with loss 0.5288\n",
      "Epoch 955, Loss: 0.5287, Val Loss: 0.4487\n",
      "Saving model at epoch 955 with loss 0.5287\n",
      "Epoch 956, Loss: 0.5287, Val Loss: 0.4486\n",
      "Saving model at epoch 956 with loss 0.5287\n",
      "Epoch 957, Loss: 0.5286, Val Loss: 0.4485\n",
      "Saving model at epoch 957 with loss 0.5286\n",
      "Epoch 958, Loss: 0.5285, Val Loss: 0.4484\n",
      "Saving model at epoch 958 with loss 0.5285\n",
      "Epoch 959, Loss: 0.5285, Val Loss: 0.4483\n",
      "Saving model at epoch 959 with loss 0.5285\n",
      "Epoch 960, Loss: 0.5284, Val Loss: 0.4482\n",
      "Saving model at epoch 960 with loss 0.5284\n",
      "Epoch 961, Loss: 0.5284, Val Loss: 0.4481\n",
      "Saving model at epoch 961 with loss 0.5284\n",
      "Epoch 962, Loss: 0.5283, Val Loss: 0.4480\n",
      "Saving model at epoch 962 with loss 0.5283\n",
      "Epoch 963, Loss: 0.5282, Val Loss: 0.4479\n",
      "Saving model at epoch 963 with loss 0.5282\n",
      "Epoch 964, Loss: 0.5282, Val Loss: 0.4478\n",
      "Saving model at epoch 964 with loss 0.5282\n",
      "Epoch 965, Loss: 0.5281, Val Loss: 0.4477\n",
      "Saving model at epoch 965 with loss 0.5281\n",
      "Epoch 966, Loss: 0.5280, Val Loss: 0.4476\n",
      "Saving model at epoch 966 with loss 0.5280\n",
      "Epoch 967, Loss: 0.5280, Val Loss: 0.4475\n",
      "Saving model at epoch 967 with loss 0.5280\n",
      "Epoch 968, Loss: 0.5279, Val Loss: 0.4474\n",
      "Saving model at epoch 968 with loss 0.5279\n",
      "Epoch 969, Loss: 0.5279, Val Loss: 0.4473\n",
      "Saving model at epoch 969 with loss 0.5279\n",
      "Epoch 970, Loss: 0.5278, Val Loss: 0.4472\n",
      "Saving model at epoch 970 with loss 0.5278\n",
      "Epoch 971, Loss: 0.5277, Val Loss: 0.4471\n",
      "Saving model at epoch 971 with loss 0.5277\n",
      "Epoch 972, Loss: 0.5277, Val Loss: 0.4470\n",
      "Saving model at epoch 972 with loss 0.5277\n",
      "Epoch 973, Loss: 0.5276, Val Loss: 0.4469\n",
      "Saving model at epoch 973 with loss 0.5276\n",
      "Epoch 974, Loss: 0.5275, Val Loss: 0.4468\n",
      "Saving model at epoch 974 with loss 0.5275\n",
      "Epoch 975, Loss: 0.5275, Val Loss: 0.4467\n",
      "Saving model at epoch 975 with loss 0.5275\n",
      "Epoch 976, Loss: 0.5274, Val Loss: 0.4466\n",
      "Saving model at epoch 976 with loss 0.5274\n",
      "Epoch 977, Loss: 0.5274, Val Loss: 0.4466\n",
      "Saving model at epoch 977 with loss 0.5274\n",
      "Epoch 978, Loss: 0.5273, Val Loss: 0.4465\n",
      "Saving model at epoch 978 with loss 0.5273\n",
      "Epoch 979, Loss: 0.5272, Val Loss: 0.4464\n",
      "Saving model at epoch 979 with loss 0.5272\n",
      "Epoch 980, Loss: 0.5272, Val Loss: 0.4463\n",
      "Saving model at epoch 980 with loss 0.5272\n",
      "Epoch 981, Loss: 0.5271, Val Loss: 0.4462\n",
      "Saving model at epoch 981 with loss 0.5271\n",
      "Epoch 982, Loss: 0.5271, Val Loss: 0.4461\n",
      "Saving model at epoch 982 with loss 0.5271\n",
      "Epoch 983, Loss: 0.5270, Val Loss: 0.4460\n",
      "Saving model at epoch 983 with loss 0.5270\n",
      "Epoch 984, Loss: 0.5269, Val Loss: 0.4459\n",
      "Saving model at epoch 984 with loss 0.5269\n",
      "Epoch 985, Loss: 0.5269, Val Loss: 0.4458\n",
      "Saving model at epoch 985 with loss 0.5269\n",
      "Epoch 986, Loss: 0.5268, Val Loss: 0.4457\n",
      "Saving model at epoch 986 with loss 0.5268\n",
      "Epoch 987, Loss: 0.5268, Val Loss: 0.4456\n",
      "Saving model at epoch 987 with loss 0.5268\n",
      "Epoch 988, Loss: 0.5267, Val Loss: 0.4455\n",
      "Saving model at epoch 988 with loss 0.5267\n",
      "Epoch 989, Loss: 0.5266, Val Loss: 0.4454\n",
      "Saving model at epoch 989 with loss 0.5266\n",
      "Epoch 990, Loss: 0.5266, Val Loss: 0.4453\n",
      "Saving model at epoch 990 with loss 0.5266\n",
      "Epoch 991, Loss: 0.5265, Val Loss: 0.4452\n",
      "Saving model at epoch 991 with loss 0.5265\n",
      "Epoch 992, Loss: 0.5265, Val Loss: 0.4451\n",
      "Saving model at epoch 992 with loss 0.5265\n",
      "Epoch 993, Loss: 0.5264, Val Loss: 0.4451\n",
      "Saving model at epoch 993 with loss 0.5264\n",
      "Epoch 994, Loss: 0.5263, Val Loss: 0.4450\n",
      "Saving model at epoch 994 with loss 0.5263\n",
      "Epoch 995, Loss: 0.5263, Val Loss: 0.4449\n",
      "Saving model at epoch 995 with loss 0.5263\n",
      "Epoch 996, Loss: 0.5262, Val Loss: 0.4448\n",
      "Saving model at epoch 996 with loss 0.5262\n",
      "Epoch 997, Loss: 0.5262, Val Loss: 0.4447\n",
      "Saving model at epoch 997 with loss 0.5262\n",
      "Epoch 998, Loss: 0.5261, Val Loss: 0.4446\n",
      "Saving model at epoch 998 with loss 0.5261\n",
      "Epoch 999, Loss: 0.5260, Val Loss: 0.4445\n",
      "Saving model at epoch 999 with loss 0.5260\n",
      "Test Loss (MSE): 0.7539\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the features we want to keep\n",
    "# Keep only pickup and drop off positions, and add pickup and dropoff month, day, hour, and day of week\n",
    "features_model3 = [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\",\n",
    "                   \"pickup_month\", \"pickup_day\", \"pickup_hour\", \"dropoff_month\", \"dropoff_day\", \"dropoff_hour\", \n",
    "                   \"pickup_day_of_week\", \"dropoff_day_of_week\"]\n",
    "\n",
    "# Split 10% of the training data as validation set\n",
    "model3_x = select_features(X_train, features_model3)\n",
    "model3_y = y_train\n",
    "model3_x_train, model3_x_val, model3_y_train, model3_y_val = train_test_split(model3_x, model3_y, test_size=0.1, random_state=0)\n",
    "model3_x_test = select_features(X_test, features_model3)\n",
    "\n",
    "# Define Model\n",
    "model3 = Sequential()\n",
    "model3.add(LinearLayer(model3_x_train.shape[1], 12))\n",
    "model3.add(ReLU())\n",
    "model3.add(LinearLayer(12, 12))\n",
    "model3.add(ReLU())\n",
    "model3.add(LinearLayer(12, 12))\n",
    "model3.add(ReLU())\n",
    "model3.add(LinearLayer(12, 1))\n",
    "\n",
    "# Training Setup\n",
    "epochs_model3 = 1000\n",
    "learning_rate_model3 = 0.01\n",
    "loss_function_model3 = MeanSquaredErrorLoss()\n",
    "\n",
    "best_loss_model3 = float(\"inf\")\n",
    "patience_model3 = 3  # Early stopping patience\n",
    "stagnation_model3 = 0\n",
    "losses_model3 = []\n",
    "val_losses_model3 = []\n",
    "\n",
    "for epoch_model3 in range(epochs_model3):\n",
    "    predictions_model3 = model3.forward(model3_x_train)\n",
    "    loss_model3 = loss_function_model3.forward(predictions_model3, model3_y_train)\n",
    "    losses_model3.append(loss_model3)\n",
    "    \n",
    "    val_predictions_model3 = model3.forward(model3_x_val)\n",
    "    val_loss_model3 = loss_function_model3.forward(val_predictions_model3, model3_y_val)\n",
    "    val_losses_model3.append(val_loss_model3)\n",
    "    \n",
    "    print(f\"Epoch {epoch_model3}, Loss: {loss_model3:.4f}, Val Loss: {val_loss_model3:.4f}\")\n",
    "    \n",
    "    grad_output_model3 = loss_function_model3.backward()\n",
    "    model3.backward(grad_output_model3, learning_rate_model3)\n",
    "\n",
    "    # Early stopping\n",
    "    if loss_model3 < best_loss_model3:\n",
    "        best_loss_model3 = loss_model3\n",
    "        stagnation_model3 = 0\n",
    "        print(f\"Saving model at epoch {epoch_model3} with loss {loss_model3:.4f}\")\n",
    "        model3.save_weights(\"best_model3\")\n",
    "    else:\n",
    "        stagnation_model3 += 1\n",
    "        if stagnation_model3 >= patience_model3:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load best model and evaluate on test set\n",
    "test_predictions_model3 = model3.forward(model3_x_test)\n",
    "test_loss_model3 = loss_function_model3.forward(test_predictions_model3, y_test)\n",
    "print(f\"Test Loss (MSE): {test_loss_model3:.4f}\")\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses_model3, label='Training Loss')\n",
    "plt.plot(val_losses_model3, label='Validation Loss', linestyle='dashed')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
