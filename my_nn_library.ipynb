{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Neural Network Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"Base class for all layers in the neural network.\"\"\"\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Computes the forward pass.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"Computes the backward pass.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class LinearLayer(Layer):\n",
    "    \"\"\"Fully connected linear layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.weights = np.random.randn(output_dim, input_dim) * np.sqrt(2 / input_dim)\n",
    "        self.bias = np.zeros((output_dim, 1))\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Performs forward pass: output = input * weights^T + bias\"\"\"\n",
    "        self.input_data = input_data  # Store for use in backward pass\n",
    "        return np.dot(self.weights, input_data.T).T + self.bias.T\n",
    "\n",
    "    def backward(self, grad_output, learning_rate=0.01):\n",
    "        \"\"\"Computes gradients and updates parameters.\"\"\"\n",
    "        grad_input = np.dot(grad_output, self.weights)  # dL/dX\n",
    "        grad_weights = np.dot(grad_output.T, self.input_data)  # dL/dW\n",
    "        grad_bias = np.sum(grad_output, axis=0, keepdims=True).T  # dL/db\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weights -= learning_rate * grad_weights\n",
    "        self.bias -= learning_rate * grad_bias\n",
    "        \n",
    "        return grad_input\n",
    "    \n",
    "class Sigmoid(Layer):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    def forward(self, input_data):\n",
    "        self.output = 1 / (1 + np.exp(-input_data))\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * self.output * (1 - self.output)\n",
    "    \n",
    "class Tanh(Layer):\n",
    "    \"\"\"Tanh activation function.\"\"\"\n",
    "    def forward(self, input_data):\n",
    "        self.output = np.tanh(input_data)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * (1 - self.output ** 2)\n",
    "    \n",
    "class ReLU(Layer):\n",
    "    \"\"\"ReLU activation function.\"\"\"\n",
    "    def forward(self, input_data):\n",
    "        self.input_data = input_data\n",
    "        return np.maximum(0, input_data)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * (self.input_data > 0)\n",
    "\n",
    "class BinaryCrossEntropyLoss(Layer):\n",
    "    \"\"\"Binary Cross-Entropy Loss Function.\"\"\"\n",
    "    def forward(self, predictions, targets):\n",
    "        self.predictions = np.clip(predictions, 1e-12, 1 - 1e-12)  # Avoid log(0)\n",
    "        self.targets = targets\n",
    "        return -np.mean(targets * np.log(self.predictions) + (1 - targets) * np.log(1 - self.predictions))\n",
    "    \n",
    "    def backward(self):\n",
    "        return (self.predictions - self.targets) / (self.targets.shape[0])\n",
    "\n",
    "class Sequential(Layer):\n",
    "    \"\"\"Sequential model to stack multiple layers.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def add(self, layer):\n",
    "        \"\"\"Adds a new layer to the model.\"\"\"\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Performs a forward pass through all layers.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            input_data = layer.forward(input_data)\n",
    "        return input_data\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate=0.01):\n",
    "        \"\"\"Performs a backward pass through all layers.\"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_output = layer.backward(grad_output, learning_rate) if isinstance(layer, LinearLayer) else layer.backward(grad_output)\n",
    "    \n",
    "    def save_weights(self, filename):\n",
    "        \"\"\"Saves model weights to a file.\"\"\"\n",
    "        weights = [layer.weights for layer in self.layers if isinstance(layer, LinearLayer)]\n",
    "        biases = [layer.bias for layer in self.layers if isinstance(layer, LinearLayer)]\n",
    "        np.savez(filename, *weights, *biases)\n",
    "    \n",
    "    def load_weights(self, filename):\n",
    "        \"\"\"Loads model weights from a file.\"\"\"\n",
    "        data = np.load(filename)\n",
    "        num_layers = len(self.layers) // 2  # Since every LinearLayer has a pair (weights, bias)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, LinearLayer):\n",
    "                layer.weights = data[f\"arr_{i}\"]\n",
    "                layer.bias = data[f\"arr_{i + num_layers}\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Library Against XOR Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 7.2639\n",
      "Epoch 1000, Loss: 0.3836\n",
      "Epoch 2000, Loss: 0.0517\n",
      "Epoch 3000, Loss: 0.0263\n",
      "Epoch 4000, Loss: 0.0193\n",
      "Epoch 5000, Loss: 0.0158\n",
      "Epoch 6000, Loss: 0.0136\n",
      "Epoch 7000, Loss: 0.0121\n",
      "Epoch 8000, Loss: 0.0110\n",
      "Epoch 9000, Loss: 0.0101\n",
      "Final Predictions:\n",
      "[[4.67735818e-04]\n",
      " [9.82337554e-01]\n",
      " [9.81713307e-01]\n",
      " [9.25608756e-04]]\n"
     ]
    }
   ],
   "source": [
    "# XOR Problem Setup\n",
    "np.random.seed(0)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(LinearLayer(2, 2))  # Hidden layer with 2 neurons\n",
    "model.add(Tanh())\n",
    "model.add(LinearLayer(2, 1))  # Output layer\n",
    "model.add(Tanh())\n",
    "\n",
    "# Training\n",
    "epochs = 10000\n",
    "learning_rate = 0.1\n",
    "loss_function = BinaryCrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    predictions = model.forward(X)\n",
    "    loss = loss_function.forward(predictions, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    grad_output = loss_function.backward()\n",
    "    model.backward(grad_output, learning_rate)\n",
    "    \n",
    "    # Print loss every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Save trained model weights\n",
    "model.save_weights(\"XOR_solved.w\")\n",
    "\n",
    "# Test the model\n",
    "predictions = model.forward(X)\n",
    "print(\"Final Predictions:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR Problem Results\n",
    "\n",
    "Looks like using the tanh activations got the right predictions, couldn't get it to work well with sigmoid activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
