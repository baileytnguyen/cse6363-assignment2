{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Neural Network Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"Base class for all layers in the neural network.\"\"\"\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Computes the forward pass.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"Computes the backward pass.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class LinearLayer(Layer):\n",
    "    \"\"\"Fully connected linear layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.weights = np.random.randn(output_dim, input_dim) * np.sqrt(2 / input_dim)\n",
    "        self.bias = np.zeros((output_dim, 1))\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Performs forward pass: output = input * weights^T + bias\"\"\"\n",
    "        self.input_data = input_data  # Store for use in backward pass\n",
    "        return np.dot(self.weights, input_data.T).T + self.bias.T\n",
    "\n",
    "    def backward(self, grad_output, learning_rate=0.01):\n",
    "        \"\"\"Computes gradients and updates parameters.\"\"\"\n",
    "        grad_input = np.dot(grad_output, self.weights)  # dL/dX\n",
    "        grad_weights = np.dot(grad_output.T, self.input_data)  # dL/dW\n",
    "        grad_bias = np.sum(grad_output, axis=0, keepdims=True).T  # dL/db\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weights -= learning_rate * grad_weights\n",
    "        self.bias -= learning_rate * grad_bias\n",
    "        \n",
    "        return grad_input\n",
    "    \n",
    "class Sigmoid(Layer):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    def forward(self, input_data):\n",
    "        self.output = 1 / (1 + np.exp(-input_data))\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * self.output * (1 - self.output)\n",
    "    \n",
    "class Tanh(Layer):\n",
    "    \"\"\"Tanh activation function.\"\"\"\n",
    "    def forward(self, input_data):\n",
    "        self.output = np.tanh(input_data)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * (1 - self.output ** 2)\n",
    "    \n",
    "class ReLU(Layer):\n",
    "    \"\"\"ReLU activation function.\"\"\"\n",
    "    def forward(self, input_data):\n",
    "        self.input_data = input_data\n",
    "        return np.maximum(0, input_data)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * (self.input_data > 0)\n",
    "\n",
    "class BinaryCrossEntropyLoss(Layer):\n",
    "    \"\"\"Binary Cross-Entropy Loss Function.\"\"\"\n",
    "    def forward(self, predictions, targets):\n",
    "        self.predictions = np.clip(predictions, 1e-12, 1 - 1e-12)  # Avoid log(0)\n",
    "        self.targets = targets\n",
    "        return -np.mean(targets * np.log(self.predictions) + (1 - targets) * np.log(1 - self.predictions))\n",
    "    \n",
    "    def backward(self):\n",
    "        return (self.predictions - self.targets) / (self.targets.shape[0])\n",
    "\n",
    "class MeanSquaredErrorLoss(Layer):\n",
    "    \"\"\"Mean Squared Error Loss Function.\"\"\"\n",
    "    def forward(self, predictions, targets):\n",
    "        self.predictions = predictions\n",
    "        self.targets = targets\n",
    "        # Ensure both predictions and targets have the same shape\n",
    "        assert predictions.shape == targets.shape, \"Shapes of predictions and targets do not match\"\n",
    "        return np.mean((predictions - targets) ** 2)  # Element-wise squared error\n",
    "\n",
    "    def backward(self):\n",
    "        # Compute the gradient of the loss with respect to predictions\n",
    "        return 2 * (self.predictions - self.targets) / self.predictions.size\n",
    "\n",
    "class Sequential(Layer):\n",
    "    \"\"\"Sequential model to stack multiple layers.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def add(self, layer):\n",
    "        \"\"\"Adds a new layer to the model.\"\"\"\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Performs a forward pass through all layers.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            input_data = layer.forward(input_data)\n",
    "        return input_data\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate=0.01):\n",
    "        \"\"\"Performs a backward pass through all layers.\"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_output = layer.backward(grad_output, learning_rate) if isinstance(layer, LinearLayer) else layer.backward(grad_output)\n",
    "    \n",
    "    def save_weights(self, filename):\n",
    "        \"\"\"Saves model weights to a file.\"\"\"\n",
    "        weights = [layer.weights for layer in self.layers if isinstance(layer, LinearLayer)]\n",
    "        biases = [layer.bias for layer in self.layers if isinstance(layer, LinearLayer)]\n",
    "        np.savez(filename, *weights, *biases)\n",
    "    \n",
    "    def load_weights(self, filename):\n",
    "        \"\"\"Loads model weights from a file while ensuring correct indexing.\"\"\"\n",
    "        data = np.load(filename)\n",
    "        keys = sorted(data.files)  # Ensure we get them in the right order\n",
    "        num_linear_layers = sum(1 for layer in self.layers if isinstance(layer, LinearLayer))\n",
    "\n",
    "        weight_keys = keys[:num_linear_layers]\n",
    "        bias_keys = keys[num_linear_layers:]\n",
    "\n",
    "        linear_layers = [layer for layer in self.layers if isinstance(layer, LinearLayer)]\n",
    "\n",
    "        for layer, w_key, b_key in zip(linear_layers, weight_keys, bias_keys):\n",
    "            layer.weights = data[w_key]\n",
    "            layer.bias = data[b_key]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Library Against XOR Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 7.2639\n",
      "Epoch 1000, Loss: 0.3836\n",
      "Epoch 2000, Loss: 0.0517\n",
      "Epoch 3000, Loss: 0.0263\n",
      "Epoch 4000, Loss: 0.0193\n",
      "Epoch 5000, Loss: 0.0158\n",
      "Epoch 6000, Loss: 0.0136\n",
      "Epoch 7000, Loss: 0.0121\n",
      "Epoch 8000, Loss: 0.0110\n",
      "Epoch 9000, Loss: 0.0101\n",
      "Final Predictions:\n",
      "[[4.67735818e-04]\n",
      " [9.82337554e-01]\n",
      " [9.81713307e-01]\n",
      " [9.25608756e-04]]\n"
     ]
    }
   ],
   "source": [
    "# XOR Problem Setup\n",
    "np.random.seed(0)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(LinearLayer(2, 2))  # Hidden layer with 2 neurons\n",
    "model.add(Tanh())\n",
    "model.add(LinearLayer(2, 1))  # Output layer\n",
    "model.add(Tanh())\n",
    "\n",
    "# Training\n",
    "epochs = 10000\n",
    "learning_rate = 0.1\n",
    "loss_function = BinaryCrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    predictions = model.forward(X)\n",
    "    loss = loss_function.forward(predictions, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    grad_output = loss_function.backward()\n",
    "    model.backward(grad_output, learning_rate)\n",
    "    \n",
    "    # Print loss every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Save trained model weights\n",
    "model.save_weights(\"XOR_solved\")\n",
    "\n",
    "# Test the model\n",
    "predictions = model.forward(X)\n",
    "print(\"Final Predictions:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR Problem Results\n",
    "\n",
    "Looks like using the tanh activations got the right predictions, couldn't get it to work well with sigmoid activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 38.2597\n",
      "Epoch 0, Loss: 38.2597\n",
      "Saving model at epoch 0 with loss 38.2597\n",
      "Epoch 1, Loss: 26.3598\n",
      "Saving model at epoch 1 with loss 26.3598\n",
      "Epoch 2, Loss: 16.5700\n",
      "Saving model at epoch 2 with loss 16.5700\n",
      "Epoch 3, Loss: 12.3845\n",
      "Saving model at epoch 3 with loss 12.3845\n",
      "Epoch 4, Loss: 10.3907\n",
      "Saving model at epoch 4 with loss 10.3907\n",
      "Epoch 5, Loss: 9.0443\n",
      "Saving model at epoch 5 with loss 9.0443\n",
      "Epoch 6, Loss: 8.0610\n",
      "Saving model at epoch 6 with loss 8.0610\n",
      "Epoch 7, Loss: 7.2957\n",
      "Saving model at epoch 7 with loss 7.2957\n",
      "Epoch 8, Loss: 6.6849\n",
      "Saving model at epoch 8 with loss 6.6849\n",
      "Epoch 9, Loss: 6.1887\n",
      "Saving model at epoch 9 with loss 6.1887\n",
      "Epoch 10, Loss: 5.7828\n",
      "Epoch 10, Loss: 5.7828\n",
      "Saving model at epoch 10 with loss 5.7828\n",
      "Epoch 11, Loss: 5.4440\n",
      "Saving model at epoch 11 with loss 5.4440\n",
      "Epoch 12, Loss: 5.1527\n",
      "Saving model at epoch 12 with loss 5.1527\n",
      "Epoch 13, Loss: 4.9003\n",
      "Saving model at epoch 13 with loss 4.9003\n",
      "Epoch 14, Loss: 4.6774\n",
      "Saving model at epoch 14 with loss 4.6774\n",
      "Layer 0 - Weights: (64, 9), Bias: (64, 1)\n",
      "Layer 2 - Weights: (32, 64), Bias: (32, 1)\n",
      "Layer 4 - Weights: (1, 32), Bias: (1, 1)\n",
      "Test Loss (MSE): 4.3123\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from geopy.distance import great_circle\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    return great_circle((lat1, lon1), (lat2, lon2)).km\n",
    "\n",
    "# Load dataset\n",
    "np.random.seed(0)\n",
    "dataset = np.load(\"nyc_taxi_data.npy\", allow_pickle=True).item()\n",
    "X_train, y_train = dataset[\"X_train\"], dataset[\"y_train\"]\n",
    "X_test, y_test = dataset[\"X_test\"], dataset[\"y_test\"]\n",
    "\n",
    "df_train = pd.DataFrame(X_train, columns=[\"vendor_id\", \"pickup_datetime\", \"passenger_count\",\n",
    "                                          \"pickup_longitude\", \"pickup_latitude\",\n",
    "                                          \"dropoff_longitude\", \"dropoff_latitude\", \"store_and_fwd_flag\"])\n",
    "df_train[\"trip_duration\"] = y_train\n",
    "df_train[\"pickup_datetime\"] = pd.to_datetime(df_train[\"pickup_datetime\"])\n",
    "df_train[\"hour\"] = df_train[\"pickup_datetime\"].dt.hour\n",
    "df_train[\"day_of_week\"] = df_train[\"pickup_datetime\"].dt.dayofweek\n",
    "df_train[\"month\"] = df_train[\"pickup_datetime\"].dt.month\n",
    "df_train[\"haversine_distance\"] = df_train.apply(lambda row: \n",
    "    haversine_distance(row[\"pickup_latitude\"], row[\"pickup_longitude\"], \n",
    "                       row[\"dropoff_latitude\"], row[\"dropoff_longitude\"]), axis=1)\n",
    "df_train[\"store_and_fwd_flag\"] = df_train[\"store_and_fwd_flag\"].map({\"N\": 0, \"Y\": 1})\n",
    "df_train.drop(columns=[\"vendor_id\", \"pickup_datetime\", \"passenger_count\", \"store_and_fwd_flag\"], inplace=True)\n",
    "\n",
    "X_train_mean, X_train_std = df_train.mean(), df_train.std()\n",
    "X_train = (df_train - X_train_mean) / X_train_std\n",
    "\n",
    "df_test = pd.DataFrame(X_test, columns=[\"vendor_id\", \"pickup_datetime\", \"passenger_count\",\n",
    "                                        \"pickup_longitude\", \"pickup_latitude\",\n",
    "                                        \"dropoff_longitude\", \"dropoff_latitude\", \"store_and_fwd_flag\"])\n",
    "df_test[\"trip_duration\"] = y_test\n",
    "df_test[\"pickup_datetime\"] = pd.to_datetime(df_test[\"pickup_datetime\"])\n",
    "df_test[\"hour\"] = df_test[\"pickup_datetime\"].dt.hour\n",
    "df_test[\"day_of_week\"] = df_test[\"pickup_datetime\"].dt.dayofweek\n",
    "df_test[\"month\"] = df_test[\"pickup_datetime\"].dt.month\n",
    "df_test[\"haversine_distance\"] = df_test.apply(lambda row: \n",
    "    haversine_distance(row[\"pickup_latitude\"], row[\"pickup_longitude\"], \n",
    "                       row[\"dropoff_latitude\"], row[\"dropoff_longitude\"]), axis=1)\n",
    "df_test[\"store_and_fwd_flag\"] = df_test[\"store_and_fwd_flag\"].map({\"N\": 0, \"Y\": 1})\n",
    "df_test.drop(columns=[\"vendor_id\", \"pickup_datetime\", \"passenger_count\", \"store_and_fwd_flag\"], inplace=True)\n",
    "\n",
    "X_test_mean, X_test_std = df_test.mean(), df_test.std()\n",
    "X_test = (df_test - X_test_mean) / X_test_std\n",
    "\n",
    "y_train = np.log1p(y_train.to_numpy().reshape(-1, 1))  # Convert to NumPy and reshape\n",
    "y_test = np.log1p(y_test.to_numpy().reshape(-1, 1))\n",
    "\n",
    "# Define Model\n",
    "model = Sequential()\n",
    "model.add(LinearLayer(X_train.shape[1], 64))  # Input layer\n",
    "model.add(ReLU())\n",
    "model.add(LinearLayer(64, 32))  # Hidden layer\n",
    "model.add(ReLU())\n",
    "model.add(LinearLayer(32, 1))  # Output layer\n",
    "model.add(ReLU())\n",
    "\n",
    "# Training Setup\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "loss_function = MeanSquaredErrorLoss()\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "patience = 3  # Early stopping patience\n",
    "stagnation = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    predictions = model.forward(X_train)\n",
    "    loss = loss_function.forward(predictions, y_train)\n",
    "\n",
    "    # print loss for debugging\n",
    "    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    grad_output = loss_function.backward()\n",
    "    model.backward(grad_output, learning_rate)\n",
    "\n",
    "    # Early stopping\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        stagnation = 0\n",
    "        print(f\"Saving model at epoch {epoch} with loss {loss:.4f}\")\n",
    "        model.save_weights(\"best_model\")  # Ensure this runs\n",
    "    else:\n",
    "        stagnation += 1\n",
    "        if stagnation >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "for i, layer in enumerate(model.layers):\n",
    "    if isinstance(layer, LinearLayer):\n",
    "        print(f\"Layer {i} - Weights: {layer.weights.shape}, Bias: {layer.bias.shape}\")\n",
    "\n",
    "# Load best model and evaluate on test set\n",
    "test_predictions = model.forward(X_test)\n",
    "test_loss = loss_function.forward(test_predictions, y_test)\n",
    "print(f\"Test Loss (MSE): {test_loss:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
